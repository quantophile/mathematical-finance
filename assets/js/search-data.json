{
  
    
        "post0": {
            "title": "Gaussian Distribution.",
            "content": "Taylor&#39;s theorem in one variable : A review. . Suppose you have a function $f:X subset mathbf{R} to mathbf{R}$ that is differentiable at a point $a in X$. Then, the equation of the tangent line gives the best linear approximation for $f$ near $a$. The tangent-line to the curve $y=f(x)$ at the point $(a,f(a))$ is given by: . $$ begin{aligned} y - y_1 &amp;= m(x - x_1) quad { text{ Slope-point form } } y - f(a)&amp;= f&#39;(a)(x - a) y &amp;= f(a) + f&#39;(a)(x-a) end{aligned} $$ . Thus, . $$p_1(x) = f(a) + f&#39;(a)(x-a)$$ . The phrase best linear approximation means that if we take $R_1(x)$ to be the error term : . $$R_1(x) = f(x) - p_1(x)$$ . then, . $$ lim_{x to a} frac{R_1(x)}{x - a} = 0$$ . This is intuitively true, because the difference between the $y$-values of the graph of $f$ and its tangent line approach zero much faster than $x$ approaches $a$. This is what we meant by $p_1(x)$ is a good linear approximation to $f$ near $a$. . . Generally, tangent lines approximate graphs of functions over very small neighborhoods containing the point of tangency. For a better approximation, we might try to fit a parabola that hugs a function&#39;s graph more closely as in the figure below. In this case, we want $p_2$ to be a quadratic function such that: . $$p_2(a) = f(a), quad p_2&#39;(a) = f&#39;(a), quad p_2&#39;&#39;(a) = f&#39;&#39;(a)$$ . Let the unknown quadratic polynomial be of the functional form: . $$p_2(x) = c_0 + c_1(x-a) + c_2(x-a)^2 tag{1}$$ . Substituting $x = a$, we have: . $$c_0 = p_2(a)$$ . Differentiating (1) with respect to $x$, we get: . $$p_2&#39;(x) = c_1 + 2c_2(x-a) tag{2}$$ . Substituting $x = a$, we get: . $$p_2&#39;(a) = c_1$$ . Differentiating (2) with respect to $x$, we get: . $$p_2&#39;&#39;(x) = 2c_2$$ . Substituting $x = a$, we get: . $$c_2 = frac{p_2&#39;&#39;(a)}{2}$$ . So the only quadratic curve that satisfies the three conditions is given by: . $$p_2(x) = p_2(a) + p_2&#39;(a)(x-a) + frac{p_2&#39;&#39;(a)}{2!}(x-a)^2$$ . It can be proved that if $f$ is of class $C^2$, then . $$f(x) = p_2(x) + R_2(x,a)$$ . where . $$ lim_{x to a} frac{R_2(x,a)}{(x-a)^2} = 0$$ . There is no reason to stop with quadratic polynomials. Suppose we want to approximate $f$ by a polynomial $p_k$ of degree $k$, where $k$ is a positive integer. Analogous to the work above, we require that $p_k$ and its first $k$ derivatives agree with $f$ at the point $a$. Thus, we demand that: . $$ begin{aligned} p_k(a) &amp;= f(a) p_k&#39;(a) &amp;= f&#39;(a) p_k&#39;&#39;(a) &amp;= f&#39;&#39;(a) &amp; vdots p_k^{(k)}(a) &amp;= f^{(k)}(a) end{aligned} $$ . . Theorem 1. (Taylor&#39;s theorem in one variable) Let $X$ be open in $ mathbf{R}$ and suppose $f:X subseteq mathbf{R} to mathbf{R}$ is differentiable upto (atleast) order $k$. . Given $a in X$, let . $$p_k(x) = f(a) + f&#39;(a)(x-a) + frac{f&#39;&#39;(a)}{2!}(x-a)^2 + ldots + frac{f^{k}(a)}{k!}(x-a)^k tag{3}$$ . Then, . $$f(x) = p_k(x) + R_k(x,a)$$ . where the remainder term $R_k$ is such that $R_k(x,a)/(x-a)^k to 0$ as $x to a$. . . The polynomial defined by the formula (1) is called the $k$th-order Taylor polynomial of $f$ at $a$. The essence of the Taylor&#39;s theorem is this : For $x$ near $a$, the Taylor polynomial $p_k$ approximates $f$ in the sense that the error $R_k$ involved in making this approximation tends to zero even faster than $(x-a)^k$ does. When $k$ is fast this is very fast indeed, as we see graphically. . %matplotlib inline import matplotlib.pyplot as plt import numpy as np x = np.linspace(0,2,1000) y1= [(x-1) for x in x] y2= [(x-1)**2 for x in x] y3= [(x-1)**3 for x in x] y4= [(x-1)**4 for x in x] plt.plot(x,y1) plt.plot(x,y2) plt.plot(x,y3) plt.plot(x,y4) plt.xlim((0,2)) plt.grid(True) plt.show() . Proof . We settle for an intuitive calculus-based proof. We shall prove this more rigorously in the posts on Real Analysis. . By the fundamental theorem of Calculus, . $$f(x) - f(a) = int_{a}^{x}f&#39;(t)dt tag{4}$$ . We evaluate the integral on the right side of (4) by means of integration by parts. Recall that the relevant formula is: . $$ int u dv = uv - int v du$$ . We use this formula with $u = f&#39;(t)$ and $v = x - t$, so that $dv = -dt$. Note that in the right hand side of (4), $x$ plays the role of constant. We obtain : . $$ begin{aligned} int_{a}^{x} f&#39;(t)dt &amp;= -f&#39;(t)(x - t)|_{a}^{x} + int_{a}^{x}(x-t)f&#39;&#39;(t)dt &amp;=f&#39;(a)(x-a)+ int_{a}^{x}(x-t)f&#39;&#39;(t)dt end{aligned} $$ . Let&#39;s refer to the above equation as (5). Combining (4) and (5), we get : . $$f(x) = f(a) + f&#39;(a)(x - a) + int_{a}^{x}(x-t)f&#39;&#39;(t)dt tag{6}$$ . Thus, we have shown that when $f$ is differentiable atleast upto the second order, that: . $$R_1(x,a) = int_{a}^{x}(x-t)f&#39;&#39;(t)dt$$ . This provides an integral formula for the remainder of formula . $$f(x) = p_1(x) + R_1(x,a)$$ . We are yet to establish that: . $$ lim_{x to a} frac{R_1(x,a)}{(x-a)} = 0$$ . To obtain the second-order formula, the case of $k=2$, we focus on $R_1(x,a) = int_{a}^{x}(x-t)f&#39;&#39;(t)dt$ and integrate by parts again, this time with $u = f&#39;&#39;(t)$, $dv = (x-t)dt$, so $v = - frac{(x-t)^2}{2}$. We obtain: . $$ begin{aligned} int_{a}^{x} f&#39;(t)dt &amp;=f&#39;(a)(x-a)+ int_{a}^{x}(x-t)f&#39;&#39;(t)dt &amp;=f&#39;(a)(x-a) -f&#39;&#39;(t) frac{(x-t)^2}{2}|_{a}^{x} + int_{a}^{x} frac{(x-t)^2}{2}f&#39;&#39;&#39;(t)dt &amp;= f&#39;(a)(x-a) + f&#39;&#39;(a) frac{(x-a)^2}{2!} + int_{a}^{x} frac{(x-t)^2}{2!}f&#39;&#39;&#39;(t)dt end{aligned} $$ . Thus, we have an integral formula for the remainder of the formula: . $$f(x) = p_2(x) + R_2(x,a)$$ . We can continue to argue in this manner or use mathematical induction to show that formula (3) holds in general with . $$R_k(x,a) = int_{a}^{x} frac{(x-t)^k}{k!}f^{(k)}(t)dt tag{7}$$ . It remains to be seen that $R_k(x,a)/(x-a)^k to 0$ as $x to a$. In formula (7), we are only considering $t$ to fall between $a$ and $x$, so that the distance $|x - t| leq |x - a|$. Moreover, since we are assuming that $f$ is of class $C^{k+1}$, we have that $f^{k+1}(t)$ is continuous and, therefore, bounded for $t$ between $a$ and $x$. . This follows from an important result in Analysis. Let $K$ is a compact set, and $f$ be continuous over $K$. Then, $f$ preserves compact sets. $f(K)$ is compact. Thus, $f(K)$ is bounded. . Consequently, there exists $M in mathbf{N}$ for all $t in [a,x]$ such that $|f^{k+1}(t)| leq M$. . Thus, . $$ begin{aligned} |R_k(x,a)| &amp; leq left | int_{a}^{x} frac{(x-t)^k}{k!}f^{k}(t)dt right | &amp; leq pm int_{a}^{x} left | frac{(x-t)^k}{k!}f^{k}(t)dt right | quad { text{ Triangle Inequality } } &amp; leq pm int_{a}^{x} frac{M}{k!}|x-t|^k dt &amp; leq pm int_{a}^{x} frac{M}{k!}|x-a|^k dt &amp;= frac{M}{k!}|x-a|^{k+1} end{aligned} $$ . Thus, . $$0 leq | frac{R_k(x,a)}{(x-a)^k}| leq frac{M}{k!}|x - a|$$ . As $x to a$, the quantity $ frac{M}{k!}|x-a| to 0$. Hence, by the squeeze theorem, . $$ frac{R_k(x,a)}{(x-a)^k} to 0$$ . Stirling&#39;s Formula. . An important tool of analytical probability theory is contained in the classical theorem known as Stirling&#39;s formula. . $$n! sim sqrt{2 pi n} left( frac{n}{e} right)^n$$ . where the sign $ sim$ is used to indicate that the two sides are asymptotically equal. In other words, the ratio of the two sides tends to unity, as $n to infty$. . $x_n sim y_n$ . if and only if . $$ lim_{n to infty} frac{x_n}{y_n} = 1$$ . This formula is invaluable for many theoretical purposes and can be used also to obtain excellent numerical approximations. It is true that, the difference of the two sides increases over all bounds, but it is the percentage error that really matters. It decreases steadily, and Stirling&#39;s approximation is remarkably accurate even for small $n$. . We consider: . $$a_n = log 2 + log 3 + ldots + log (n-1) + frac{1}{2} log n$$ . which differs from $ log n!$ only by the factor $ frac{1}{2}$ in the last term to the right. We shall show that $a_n$ represents the areas of two polygons, and this remark will lead to two bounds for $ log n!$. . . On writing . $$a_n = frac{1}{2}( log 1 + log 2) + frac{1}{2}( log 2 + log 3) + ldots + frac{1}{2}( log (n-1) + log n)$$ . it becomes apparent that $a_n$ equals the area of the green trapezoids whose vertices are $A_1,A_2, ldots,A_n$ of the curve $y = log x$ with abscissas $1,2, ldots,n$. This polygon being inside the curve, its area is smaller than the area of the domain bounded by the curve, the $x$ -axis and the line $x = n$. . Thus, . $$a_n &lt; int_{1}^{n} log x dx$$ . On the other hand, $ log k$ equals the area of the red trapezoid with basis $k- frac{1}{2} &lt; x &lt; k + frac{1}{2}$. . Note: Observe that, geometrically, the slope of the red tangent at $A_2$ is $1/2$, at $A_3$ is $1/3$ and at $A_n$ is $1/n$. Thus, the endpoints of each red segment are $(1/2, log2 - 1/4)$ and $(3/2, log 2 + 1/4)$, $( log 3 - 1/6, log 3 + 1/6)$, etc. So, the areas of these trapezoids are $ log 2$, $ log 3$, $ ldots$. . It follows that $ log (n-1)!$ is greater than the area of the domain bounded by the $x$-axis, the curve $y= log x$ and the vertical lines $x = frac{3}{2}$ and $x=n- frac{1}{2}$. Now, the last rectangle, $ frac{1}{2} log n$ quite obviously exceeds the area of the strip $n- frac{1}{2} &lt; x &lt; n$ under the curve. . But, . $$a_n = log (n-1)! + frac{1}{2} log n$$ . Thus, . $$a_n &gt; int_{3/2}^{n} log x dx$$ . Consequently, we have found lower and upper bounds for $a_n$: . $$ int_{3/2}^{n} log x dx &lt; a_n &lt; int_{1}^{n} log x dx$$ . The indefinite integral of $ log x$ is given by $x log x - x$, so the inequality reduces to: . $$(n log n - n) - (3/2 log 3/2 - 3/2) &lt; a_n &lt; (n log n - n) - (1 log 1 - 1)$$ . Or equivalently, . $$(n log n - n) - (3/2 log 3/2 - 3/2) &lt; log (n-1)! + frac{1}{2} log n &lt; (n log n - n) +1$$ . Adding $ frac{1}{2} log n$ we have: . $$ left(n + frac{1}{2} right) log n - n + frac{3}{2} left(1- log frac{3}{2} right) &lt; log n! &lt; left(n + frac{1}{2} right) log n - n + 1$$ . $$ frac{3}{2} left(1- log frac{3}{2} right) &lt; log n! - left(n + frac{1}{2} right) log n + n &lt; 1$$ . Put for abbreviation : . $$ delta_n = log n! - left(n + frac{1}{2} right) log n + n$$ . Then, . $$ frac{3}{2} left(1 - log frac{3}{2} right) &lt; delta_n &lt; 1$$ . Geometrically, . $$1- delta_n = left[1 + left(n + frac{1}{2} right) log n - n right]- log n! $$ . equals the area of the domain between the curve . Gaussian Distribution. . The Gaussian distribution is a very famous continuous distribution with a bell-shaped PDF. It is extremely widely used in finance and statistics, because of a theorem, the central limit theorem, which says that under very weak assumptions, the sum of a large number of i.i.d. random variables has an approximately Gaussian distribution, regardless of the distribution of the individual random variables. This means we can start with independent random variables from almost any distribution, discrete or continuous, but once we add up a bunch of them, the distribution of the resulting random variable looks like a Gaussian distribution. . The central limit theorem is a topic for later. But in the meantime, we look at the properties of the Gaussian PDF and CDF and derive the expectation and variance of the Gaussian distribution. To do this, we will again use the strategy of location-scale transformation by starting with the simplest normal distribution, the standard Normal, which is centered at $0$ and has variance $1$. After deriving the properties of the standard Normal, we&#39;ll be able to get to any Normal distribution we want by shifting and scaling. . . Definition 1. (Standard Normal Distribution) A continuous random variable $Z$ is said to have the standard normal distribution if its PDF $ phi$ is given by: . $$ phi(z) = frac{1}{ sqrt{2 pi}}e^{-z^2/2}$$ . We write this as $Z sim mathcal{N}(0,1)$ since, as we will see, $Z$ has mean $0$ and variance $1$. . . The constant $ frac{1}{ sqrt{2 pi}}$ in front of the PDF may look surprising (why is something with $ pi$ needed in front of something with $e$, when there are no circles in sight?), but it&#39;s exactly what is needed to make the PDF integrate to $1$. Such constants are called normalizing constants because they normalize the total area under the PDF to $1$. We&#39;ll verify soon that this is a valid PDF. . The standard normal CDF $ Phi$ is the accumulated area under the PDF: . $$ Phi(z) = int_{- infty}^{z} phi(t)dt = int_{- infty}^{z} frac{1}{ sqrt{2 pi}}e^{- frac{t^2}{2}}$$ . Some people, upon seeing the function $ Phi$ for the first time, express dismay that it is left in terms of an integral. Unfortunately, we have little choice in the mattter: it turns out to be mathematically impossible to find a closed-form express for the anti-derivative of $ phi$, meaning that we cannot express $ Phi$ as a finite sum of elementary functions such as polynomials or exponentials. But closed-form or not, it&#39;s still a well-defined function: if we give $ Phi$ an input $z$, it returns the accumulated area under the PDF from $- infty$ up to $z$. That is to say, the integral converges and has a definite value for all $z in mathbf{R}$. . . Important: We can tell that the normal distribution must be special because the standard normal PDF and CDF get their own greek letters. By convention, we use $ phi$ for the standard normal PDF and $ Phi$ for the CDF. We will often use $Z$ to denote a standard normal random variable. . The standard normal PDF and the standard normal CDF are plotted in the figure below. The PDF is bell-shaped and symmetric about $0$ and the CDF is $S$ shaped. . import numpy as np import matplotlib.pyplot as plt z = np.linspace(-5.0,5.0,10001) def phi(z): return 1/np.sqrt(2 * np.pi) * np.exp((-z**2)/2) def Phi(z_upper): # Approximate the integral by a Riemann sum z_lower = -5.0 dz = 0.0001 z = np.arange(z_lower,z_upper+dz,dz) prob_elmnt = phi(z) * dz return np.sum(prob_elmnt) . pdf = list(map(phi,z)) cdf = list(map(Phi,z)) . plt.xlabel(r&#39;$z$&#39;) plt.ylabel(r&#39;$ phi(z)$&#39;) plt.title(&#39;PDF of the standard normal&#39;) plt.grid(True) plt.plot(z,pdf) . . [&lt;matplotlib.lines.Line2D at 0x1d79067e5e0&gt;] . plt.xlabel(r&#39;$z$&#39;) plt.ylabel(r&#39;$ Phi(z)$&#39;) plt.title(&#39;CDF of the standard normal&#39;) plt.grid(True) plt.plot(z,cdf) . . [&lt;matplotlib.lines.Line2D at 0x1d7906e0c70&gt;] . There are several important symmetry propertoes that can be deduced from the standard normal PDF and CDF. . Symmetry of the PDF: $ phi$ satisfies $ phi(z) = phi(-z)$. Notice, that $ phi(z)$ is an even function. . | Symmetry of the tail areas: The area under the PDF curve to the left of $-2$, which is $P(Z leq -2) = Phi(-2)$ by definition equals the area to the right of $2$, which is $P(Z leq 2) = 1 - Phi(2)$. In general, we have for the cumulative distribution function : . | $$ Phi(z) = 1 - Phi(-z)$$ . for all $z$. This can be seen visually by looking at the PDF curve, and mathematically by substituting $u = -t$ below and using the fact that PDFs integrate to $1$: . $$ Phi(-z) = int_{- infty}^{-z} phi(t)dt = int_{z}^{ infty} phi(u)du = 1 - int_{- infty}^{z} phi(u)du = 1 - Phi(z)$$ . Symmetry of $Z$ and $-Z$: If $Z sim mathcal{N}(0,1)$, then $-Z sim mathcal{N}(0,1)$ as well. To see this, note that the CDF of $-Z$ is: | $$P(-Z leq z) = P(Z geq -z) = 1 - Phi(-z)$$ . but that is $ Phi(z)$ according to what we just argued. So, $-Z$ has the CDF $ Phi$. . We need to prove three key facts about the standard normal, and then we&#39;ll be ready to handle general normal distributions: we need to show that $ phi$ is a valid PDF, that $E(Z) = 0$ and that $Var(Z) = 1$. . To verify the validity of $ phi$, we&#39;ll show that the total area under $e^{-z^2/2}$ is $ sqrt{2 pi}$. However, we can&#39;t find the antiderivative of $e^{-z^2/2}$ directly, again because of the annoying fact that the antiderivative isn&#39;t expressible in closed form. But this doesn&#39;t mean we can&#39;t do definite integrals with some ingenuity. . An amazing trick saves the day here: write down the integral twice. Uusually, writing down the same problem repeatedly is more a sign of frustration than a problem solving strategy. But in this case, it allows a neat conversion to polar coordinates: . $$ begin{aligned} left( int_{- infty}^{ infty}e^{-z^2/2}dz right) left( int_{- infty}^{ infty}e^{-z^2/2}dz right) &amp;= left( int_{- infty}^{ infty}e^{-x^2/2}dx right) left( int_{- infty}^{ infty}e^{-y^2/2}dy right) &amp;= int_{- infty}^{ infty} int_{- infty}^{ infty}e^{- frac{x^2 + y^2}{2}}dxdy end{aligned} $$ . To effect the simplication of the integrand, we perform a coordinate transformtion. Let the transformation be . $$ mathbf{T}(r, theta) = (x(r, theta),y(r, theta)) = (r cos theta, r sin theta)$$ . That is, we convert rectangular to polar coordinates. The region of integration $D$ is the entire $xy$-plane, so the domain $D^*$ in the polar coordinates should be all of the $r theta$-plane given by $[0, infty] times[0,2 pi]$. . Thus, the double integral . $$ int int_{D}f(x,y)dx dy = int int_{D*}f(x(u,v),y(u,v)) left| frac{ partial(x,y)}{ partial(u,v)} right|du dv$$ . Consequently, we get: . $$ begin{aligned} left( int_{- infty}^{ infty}e^{-z^2/2}dz right) left( int_{- infty}^{ infty}e^{-z^2/2}dz right) &amp;= int_{- infty}^{ infty} int_{- infty}^{ infty}e^{- frac{x^2 + y^2}{2}}dxdy &amp;= int_{0}^{2 pi} int_{0}^{ infty}e^{-r^2/2}r dr d theta end{aligned} $$ . We can now use the substitution $u = r^2 / 2$ and thus $du = r dr$ to evaluate the inner integral. We get: . $$ begin{aligned} int_{0}^{2 pi} int_{0}^{ infty}e^{-r^2/2}r dr d theta &amp;= int_{0}^{2 pi} int_{0}^{ infty}e^{-u}du d theta &amp;= int_{0}^{2 pi}[-e^{-u}]_{0}^{ infty}d theta &amp;= int_{0}^{2 pi}d theta &amp;= 2 pi end{aligned} $$ . Therefore, . $$ int_{ infty}^{ infty}e^{-z^2/2}dz = sqrt{2 pi}$$ . as we wanted to show. . The expectation of the standard Normal has to be $0$, by the symmetry of the PDF; no other balancing point would make sense. We can also see this symmetry by looking at the definition of $E(Z)$: . $$E(Z) = frac{1}{ sqrt{2 pi}} int_{- infty}^{ infty}ze^{-z^2/2}dz$$ . and since $g(z) = ze^{-z^2/2}$ is an odd function, the area under $g$ from $- infty$ to $0$ cancels the area under $g$ from $0$ to $ infty$. Therefore, $E(Z)=0$. In fact, the same arument show that $E(Z^n)=0$ for any odd positive number $n$. . Getting the mean is easy, but the variance calculation is a bit more invovled. . $$Var(Z) = E(Z^2) - (E(Z))^2 = E(Z^2) - 0^2 = E(Z^2)$$ . Now, by LOTUS: . $$ begin{aligned} E(Z^2) &amp;= frac{1}{ sqrt{2 pi}} int_{- infty}^{ infty}z^2e^{-z^2/2}dz &amp;= frac{1}{ sqrt{2 pi}} left[-ze^{-z^2/2}|_{- infty}^{ infty}+ int_{- infty}^{ infty}e^{-z^2/2}dz right] &amp;= frac{1}{ sqrt{2 pi}} left[0 + sqrt{2 pi} right] &amp;= 1 end{aligned} $$ . In the first step we performed integration by parts by choosing $u = z$ and $dv = ze^{-z^2/2}dz$ so, $du = 1$ and $v=-e^{-z^2/2}$. . DeMoivre-Laplace Limit Theorem. . Consider a coin-tossing game consisting of a sequence $n$ coin flips. Each coin toss lands heads with probability $p$ and tails with probability $q = 1 - p$. . Let $X_1,X_2, ldots,X_n$ be $n$ independent random variables representing our gain or loss on each coin toss. If a coin lands heads, it results in a gain and $X_i = +1$ dollar. If a coin lands tails, it results in a loss, $X_i = -1$ dollar. So, $X_i$ is a $Bernoulli(p)$ random variable with the PMF: . $$ begin{aligned} P(X_i=+1) &amp;= p P(X_i=-1) &amp;= 1 - p end{aligned} $$ . The first thing we want to know, is what to expect for our gain or loss in the long term. We can think of this as the cumulative sum $S_n = X_1 + X_2 + ldots + X_n$ for some large determinate $n in mathbf{N}$. . We have: . $$E(X_j) = 1 cdot p + (-1) cdot(1-p) = 2p - 1$$ . By linearity of expectations, . $$ begin{aligned} E(S_n) &amp;= E(X_1 + X_2 + ldots + X_n) &amp;= E(X_1) + E(X_2) + ldots + E(X_n) &amp;= n(2p - 1) end{aligned} $$ . Suppose that we are a spectator watching the game and we are offered the chance to bet on how far away the final cumulative gains will be from what everyone in the crowd expects. What should we guess? . We know that, if $X$ and $Y$ are independent random variables, then . $$Var(X + Y) = Var(X) + Var(Y)$$ . Now, . $$E(X_j^2) = 1^2 cdot p + (-1)^2 cdot(1-p) = 1$$ . So, . $$ begin{aligned} Var(X_j) &amp;= E(X_j^2) - (E(X_j))^2 &amp;= 1 - (2p - 1)^2 &amp;= 1 - (4p^2 -4p + 1) &amp;= 4p - 4p^2 &amp;= 4p(1-p) end{aligned} $$ . So, . $$ begin{aligned} Var(S_n) &amp;= Var(X_1 + X_2 + ldots + X_n) &amp;= Var(X_1) + ldots + Var(X_n) &amp;= 4np(1-p) end{aligned} $$ . Our guess should be $SD(S_n) = sqrt{Var(S_n)} = 2 sqrt{np(1-p)}$. . We have determined the two basic parameters of the game: . The expected long term result : . $$E(S_n) = n(2p - 1)$$ . The guess for the deviation from the expected long term result: . $$SD(S_n) = 2 sqrt{np(1-p)}$$ .",
            "url": "https://quantophile.github.io/mathematical-finance/probability-theory/2022/03/07/The-Gaussian-Distribution.html",
            "relUrl": "/probability-theory/2022/03/07/The-Gaussian-Distribution.html",
            "date": " • Mar 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "The Binomial and Poisson Distribution.",
            "content": "Bernoulli Trials . Repeated independent trials are called Bernoulli trials if there are only two possible outcomes for each trial and their probabilities remain the same throughout the trials. It is usual to denote the two probabilities by $p$ and $q$, and to refer to the outcome with probability $p$ as success, $S$, and to the other as failure, $F$. Clearly, $p$ and $q$ must be non-negative and . begin{align*} p + q = 1 end{align*} The sample space of each individual trial is formed by the two points $S$ and $F$. The sample space of $n$ Bernoulli trials contains $2^n$ points or successions of $n$ symbols $S$ and $F$, each point representing one possible outcome of the compound experiment. Since the trials are independent, the probabilities multiply. In other words, the probability of any specified sequence is the product obtained on replacing the symbols $S$ and $F$ by $p$ and $q$ respectively. Thus, $P {SSFSF ldots FFS } = ppqpq ldots qqp$. . The Binomial Distribution . Frequently, we are interested only in the total number of successes produced in a succession of $n$ Bernoulli trials but not in their order. The number of successes can be $0,1,2 ldots,n$ and our first problem is to determine the corresponding probabilities. Now, the event $n$ trials result in $k$ successes and $n-k$ failures can happen in as many ways as $k$ letters $S$ can be distributed among $n$ places. In other words, our event contains ${n choose k}$ points, and, by definition, each point has the probability $p^k q^{n-k}$. This proves the . . Theorem. Let $b(k;n,p)$ be the probability that $n$ Bernoulli trails with probabilities $p$ for success and $q = 1-p$ for failure result in $k$ successes and $n-k$ failures. Then, . begin{equation*} b(k;n,p) = {n choose k} p^k q^{n - k} tag{1} end{equation*} . . In particular, the probability of no success is $q^n$ and the probability of atleast one success is $1-q^n$. . We shall treat $p$ as a constant and denote the number of successes in $n$ trilas by $S_n$; then $b(k;n,p) = P {S_n = k }$. In the general terminology, $S_n$ is a random variable, and the function $b(l;n,p)$ is the PMF of this random variable; we shall refer to it as the binomial PMF. The attribute binomial refers to the fact that equation (1) represents the $k$th term of the binomial expansion of $(q+p)^n$. This remark also shows that . begin{align*} b(0;n,p) + b(1;n,p) + ldots + b(n;n,p) = (q+p)^n = 1 end{align*}as is required by the notion of probability. . The Central Term and the tails . From equation (1), we see that . begin{align*} frac{b(k;n,p)}{b(k-1;n,p)} &amp;= frac{{n choose k}p^k q^{n-k}}{{n choose {k-1}}p^{k-1} q^{n-k+1}} = frac{ frac{n!}{k!(n-k)!} cdot p}{ frac{n!}{(k-1)!(n-k+1)!} cdot q} &amp;= frac{p}{q} left( frac{n - k + 1}{k} right) = frac{(n+1)p - pk + qk - qk}{qk} &amp;= 1 + frac{(n+1)p - k}{qk} end{align*} . Accordingly, the term $b(k;n,p)$ is greater than the preceding one for $(n+1)p &gt; k$, that is $k &lt; (n+1)p$ and is small for $k &gt; (n+1)p$. If $(n+1)p = m$ happens to be an integer, then $b(m;n,p) = b(m-1;n,p)$. Thus, there exists exactly one integer $m$ such that, . begin{equation*} (n+1)p - 1 &lt; m le (n+1)p tag{2} end{equation*} . Theorem. As $k$ goes from $0$ to $n$, the terms $b(k;n,p)$ first inrease monotonically, then decrease monotonically, reaching their greatest value when $k = m$, except that $b(m-1;n,p) = b(m;n,p)$ when $m = (n+1)p$. . . We shall call $b(m;n,p)$ the central term. Often $m$ is called the most probable number of successes, but it must be understood that for large values of $n$, all terms $b(k;n,p)$ are small. In 100 tossings of a true coin, the most probable number of heads is 50, but its probability is less than $0.09$. In the next chapter we shall find that $b(m;n,p)$ is approximately $1/ sqrt{2 pi npq}$. . Consider the sequence $(a_k)$ whose terms are ratio of the binomial coefficients $ frac{b(k;n,p)}{b(k-1;n,p)}$, we saw above. We have, $a_k = frac{(n+1)p}{kq}- frac{p}{q}$. Clearly therefore, as $k$ increases, the ratio $a_k$ decreases monotonically. . If $k geq (r+1)$, then, $(n+1-k)p leq (n+1-(r+1))p = (n-r)p$. And $qk geq q(r+1)$. So, $ frac{1}{qk} leq frac{1}{(r+1)q}$. So, we have an upper bound on $ frac{b(k;n,p)}{b(k-1;n,p)}$. . When $k geq (r+1)$, . begin{equation*} frac{b(k;n,p)}{b(k-1;n,p)} leq frac{(n-r)p}{(r+1)q} tag{3} end{equation*} Set herein $k=r+1, ldots,r+ nu$ and multiply the $ nu$ inequalities to obtain . begin{align*} frac{b(r+ nu;n,p)}{b(r+ nu-1;n,p)} cdot frac{b(r+ nu-1;n,p)}{b(r+ nu-2;n,p)} cdots frac{b(r+1;n,p)}{b(r;n,p)} le left { frac{(n-r)p}{(r+1)q} right }^{ nu} end{align*} . Consequently, . begin{equation*} frac{b(r+ nu;n,p)}{b(r;n,p)} leq left { frac{(n-r)p}{(r+1)q} right }^{ nu} tag{4} end{equation*} . For $r geq np$, let&#39;s find an upper bound for the fraction $ frac{(n-r)p}{(r+1)q}$. . Clearly, if $r geq np$, $(n-r)p leq (n-np)p = npq$. So, . begin{align*} frac{(n-r)p}{(r+1)q} leq frac{npq}{(r+1)q} = frac{np}{r+1} end{align*} Moreover, we want a lower bound on the denominator. $(r+1) geq (np + 1)$. So, . begin{align*} frac{(n-r)p}{(r+1)q} leq frac{np}{np+1} = 1 - frac{1}{np + 1} end{align*} So, for $r geq np$, the fraction $ frac{(n-r)p}{(r+1)q}$ is strictly less than unity, and the summation over $ nu$ leads to a finite geometric series with the ratio $ frac{(n-r)p}{(r+1)q}$. . begin{align*} sum_{ nu = 0}^{(n-r)} left( frac{(n-r)p}{(r+1)q} right)^{ nu} &amp; leq sum_{ nu = 0}^{ infty} left( frac{(n-r)p}{(r+1)q} right)^{ nu} = frac{1}{1- frac{(n-r)p}{(r+1)q}} &amp;= frac{(r+1)q}{(r+1)q-(n-r)p} &amp;= frac{(r+1)q}{(r+1)q-((n+1)-(r+1))p} = frac{(r+1)q}{(r+1)q-(n+1)p+(r+1)p} &amp;= frac{(r+1)q}{(r+1)-(n+1)p} end{align*} . We conclude that for $r geq np$, . begin{align*} sum_{ nu=0}^{(n-r)} frac{b(r+ nu;n,p)}{b(r;n,p)} leq frac{(r+1)q}{(r+1)-(n+1)p} end{align*} . Thus, . begin{equation*} sum_{ nu=0}^{(n-r)}b(r+ nu;n,p) leq b(r;n,p) cdot frac{(r+1)q}{(r+1)-(n+1)p} tag{5} end{equation*} . On the left, we have the right tail of the binomial distribution, namely the probability of atleast $r$ successes. The same calculation applied to the left tail shows that for $s leq np$ . begin{equation*} sum_{ rho=0}^{s}b( rho;n,p) leq b(s;n,p) cdot frac{(n-s+1)p}{(n+1)p - s} tag{6} end{equation*} . . Theorem. If $r geq np$, the probability of atleast $r$ successes satisfies the inequality (5); if $s leq np$, the probability of at most $s$ successes satisfies inequality (6). . . The Law Of Large Numbers . On several occasions, we have mentioned that our intuitive notion of probability is based on the following assumption. If in $n$ identical trails, $A$ occurs $ nu$ times, and if $n$ is very large, then $ nu/n$ should be near the probability of $A$. Clearly, a formal mathematical theory can never refer directly to real life, but it should atleast provie theoretical counterparts to the phenomena, which it tries to explain. Accordingly, we require that the vague introductory remark be made precise in the form a theorem. For this purpose we translate identical trials as Bernoulli trials with probability $p$ for success. If $S_n$ is the number of successes in $n$ trials, then $S_n/n$ is the average number of successes and should be near $p$. It is now easy to give a precise meaning to this. Pick an arbitrary $ epsilon &gt; 0$. Consider for example, the probability that $ frac{S_n}{n} - p$ exceeds $ epsilon$. . This probability is the same as $P {S_n &gt; n(p + epsilon) }$ and equals the left side of the inequality (5), when $r geq n(p + epsilon)$. Then, we can find an upper bound for the fraction $ frac{(r+1)q}{(r+1)-(n+1)p}$. Since $r leq n$, we have: . begin{align*} frac{(r+1)q}{(r+1)-(n+1)p} &amp; leq frac{(n+1)q}{(n(p+ epsilon)+1)-(n+1)p} &amp;= frac{(n+1)q}{np + n epsilon + 1 - np - p} &amp;= frac{(n+1)q}{n epsilon + q} end{align*} . So, we proved that the $n$th term of the sequence $(p_n)$ has an upper bound: . begin{align*} P {S_n &gt; n(p + epsilon) } leq b(r;n,p) cdot frac{(n+1)q}{n epsilon + q} end{align*}Since probabilities are non-negative, . begin{align*} 0 leq P {S_n &gt; n(p + epsilon) } leq b(r;n,p) cdot frac{(n+1)q}{n epsilon + q} end{align*} Passing to the limit, as $n to infty$, we have: . begin{align*} lim 0 leq lim P {S_n &gt; n(p + epsilon) } leq lim left[ b(r;n,p) cdot frac{(n+1)q}{n epsilon + q} right] end{align*} Now, . begin{align*} lim left[b(r;n,p) cdot frac{(n+1)q}{n epsilon + q} right] &amp;= lim b(r;n,p) cdot lim frac{ left(1+ frac{1}{n} right)q}{ epsilon + frac{q}{n}} &amp;= lim b(r;n,p) cdot frac{q}{ epsilon} end{align*} It&#39;s easy to see, that as $n to infty$, $b(r;n,p) to 0$. Note that, as $n to infty$, $r to infty$, because $r geq n(p + epsilon)$. . begin{align*} lim b(r;n,p) &amp;= lim frac{n!}{(n-r)!r!} cdot lim p^r q^{n-r} &amp; leq lim frac{n!}{n!0!} cdot lim p^n q^n quad { r ge 0 text{ and } r leq n } &amp;= lim p^n q^n { text{ since } (q^n) to 0, text{ if } |q|&lt;1 } &amp;= 0 end{align*} By the Squeeze theorem, . begin{align*} lim P {S_n &gt; n(p + epsilon) } = 0 end{align*} Using the inequality (6), we see in the same way that $P {S_n &lt; n(p - epsilon) } to 0$, and we have thus . begin{align*} P left { left lvert frac{S_n}{n} - p right rvert &lt; epsilon right } to 1 tag{8} end{align*} In words: As $n$ increases, the probability that the average number of successes deviates from $p$ by more than any preassigned $ epsilon$ tends to zero. This is one form of the law of large numbers and serves as a basis for the intuitive notion of probability as a measure of relative frequencies. For practical applications it must be supplemented by a more precise estimate of the probability on the left side in (8); such an estimate is provided by the normal approximation to the binomial distribution. . The assertion (8) is the classical law of large numbers. It is of very limited interest and should be replaced by the more previse and more useful strong law of large numbers. . Warning. It is usual to read into the law of large numbers things which it definitely does not imply. If Peter and Paul toss a perfect coin 10,000 times, it is customary to expect that Peter will lead roughly half the time. This is not true. The arc sine law states that such an equalization is least probable. The probability that Peter leads in less than $20$ trials is very much larger than the probability that the number of trials in which he leads lies between 4990 and 5010. There does not exist any tendency for the periods of lead to equalize. The law of large numbers asserts only that in a a large number of different coin tossing games, the frequency of those in which heads lead is, at any given moment, close to $ frac{1}{2}$. Nothing is said about the fluctuations of the lead within a fixed game. . The Poisson Approximation . In many applications, we deal with Bernoulli trials where, comparatively speaking $n$ is large and $p$ is small, whereas the product . begin{align*} lambda = np tag{9} end{align*}is of modertate magnitude. In such cases, it is convenient to use an approximation formula to $b(k;n,p)$ which is due to Poisson and which we proceed to derive. We have $b(0;n,p) = (1 - p)^n$ or substituting from (9), . begin{equation*} b(0;n,p) = left(1 - frac{ lambda}{n} right)^n tag{10} end{equation*}Taking the logarithm on both sides: . begin{equation*} log b(0;n,p) = n log left(1 - frac{ lambda}{n} right) end{equation*} The Taylor&#39;s series for $ log (1 + x)$ at $x = 0$ is given by: . begin{align*} log (1 + x) &amp;= f(0) + xf&#39;(0) + frac{x^2}{2!}f&#39;&#39;(0) + frac{x^3}{3!}f&#39;&#39;&#39;(0) + ldots &amp;= x + frac{x^2}{2!} cdot (-1) + frac{x^3}{3!} cdot 2 + frac{x^4}{4!} cdot (-3 cdot 2 cdot 1) &amp;= x - frac{x^2}{2} + frac{x^3}{3} - frac{x^4}{4} + ldots end{align*} So, . begin{align*} log b(0;n,p) &amp;= n log left(1- frac{ lambda}{n} right) = n left(- frac{ lambda}{n}- frac{ lambda^2}{2n^2}- ldots right) &amp;= - lambda - frac{ lambda^2}{2n} - frac{ lambda^3}{3n^2} - ldots tag{11} end{align*} As $n to infty$, $ lim log b(0;n,p) = - lambda$, so that for large $n$, . begin{align*} b(0;n,p) approx e^{- lambda} tag{12} end{align*} where the sign $ approx$ is used to indicate approximate equality. Furthermore, from the expression for $b(k;n,p)/b(k-1;n,p)$, it is seen that for any fixed $k$ and sufficiently large $n$ we have, . begin{align*} frac{b(k;n,p)}{b(k-1;n,p)} &amp;= frac{(n-k+1)p}{kq} &amp;= frac{np-(k-1)p}{kq} &amp;= frac{ lambda - (k-1)p}{kq} end{align*} . For very small values of $p$, we can write $p approx 0$ and $q approx 1$. Thus, . begin{align*} frac{b(k;n,p)}{b(k-1;n,p)} approx frac{ lambda}{k} tag{13} end{align*} . For $k=1$, we get from this and (12), $b(1;n,p) approx lambda e^{- lambda}$. For $k=2$, we get $b(2;n,p) approx frac{ lambda^2}{2}e^{- lambda}$. Generally, we see by induction that, . begin{equation*} b(k;n,p) approx frac{ lambda^k}{k!} e^{- lambda} tag{14} end{equation*} This is the famous Poisson approximation to the binomial distribution. (See problems 30-34 for an estimate of the error and a proof that the approximation in (14) is uniform when $n to infty$ and $p to 0$ in such a way that $ lambda = np$ remains bounded.) It is convenient to have a symbol for the right-hand member in (14) and we shall put . begin{align*} p(k; lambda) = frac{ lambda^k}{k!}e^{- lambda} tag{15} end{align*}With this notation $p(k; lambda)$ should be an approximation to $b(k;n, lambda/n)$ when $n$ is sufficiently large. . function p(k,λ) ((λ^k)/factorial(k)) * ℯ^(-λ) end . p (generic function with 1 method) . Birthdays. What is the probability, $p_k$, that in a company of $500$ people exactly $k$ will have birthdays on New Year&#39;s day? . If the $500$ people are cosen at random, we may apply the scheme of 500 Bernoulli trials with the probability of success $p = frac{1}{365}$. Then, $p_0 = left( frac{364}{365} right)^{500} approx 0.2537$. For the Poisson approximation, we put $ lambda = frac{500}{365}$. Then, . using Printf λ = 500/365 for k = 0:6 @printf(&quot;p(%d,%f) = %f n&quot;,k,λ,p(k,λ)) end . p(0,1.369863) = 0.254142 p(1,1.369863) = 0.348139 p(2,1.369863) = 0.238452 p(3,1.369863) = 0.108882 p(4,1.369863) = 0.037288 p(5,1.369863) = 0.010216 p(6,1.369863) = 0.002332 . Centenarians. At birth any particular person has a small chance of living 100 years, and in a large community the number of yearly births are large. Owing to wars, epidemics etc. different lives are not stochastically independent, but as a first approximation we may compare $n$ births to $n$ Bernoulli trials with death after $100$ years as success. In a stable community, where neither size nor mortality rate appreciably, it is reasonable to expect that the frequency of years in which exactly $k$ centenarians die is approximately $p(k; lambda)$, with $ lambda$ depending on the size and health of the community. . Intuitively, there will be years in which exactly $1$ centenarian dies, there will be years in which $2$ centenarians die and there will be years in which exactly $k$ centenarians die. $p(k, lambda)$ is on an average the count(number) of 365-day years in which $k$ centenarians die. . Misprints, Raisins. If in printing a book there is a constant probability of any letter&#39;s being misprinted, and if the conditions of printing remain unchanged, then we have as many Bernoulli trials as there are letters. The frequency of the pages containing exactly $k$ misprints will then be approximately $p(k; lambda)$ where $ lambda$ is the characteristic of the printer. Thus, the Poisson formula may be used to discover radical departures from uniformity or from the state of statistical control. A similar agrument applies in many cases. For example, if many raisins are distributed in the dough, we should expect that thorough mixing will result in the frequency of loaves with exactly $k$ raisin to be approximately $p(k; lambda)$ with $ lambda$ a measure of the density of raisins in the dough. . The Poisson Distribution . In the preceding section, we have used the Poisson expression merely as a convenient approximation to the binomial distribution in the case of large $n$ and small $p$. In connection with the matching and occupany problems of chapter IV, we have studied different probability distributions, which have also led to the Poisson expressions $p(k; lambda)$ as a limiting form. We have here a special case of the remarkable fact that there exist a few distributions of great universaility that occur in a surprisingly large variety of problems. The three principal distributions, with ramifications throughout probability theory are the binomial distribution, the normal distribution (to be introduced in the following chapter), and the Poisson distribution . begin{align*} p(k; lambda) = frac{ lambda^k}{k!}e^{- lambda} tag{16} end{align*}which we shall now consider on its own merits. . We note first that on adding the equations (16) for $k = 0,1,2, ldots$ we get on the right side $e^{- lambda}$ times the Taylor&#39;s series for $e^ lambda$. Hence, for any fixed $ lambda$, the quantities $p(k; lambda)$ add to unity, and therefore it is possible to conceive of an ideal experiment in which $p(k; lambda)$ is the probability of exactly $k$ successes. We shall now indicate by many physical experiments and statistical observations actually lead to such an interpretation of (16). The examples of the next section will illustrate the wide range and the importance of the various applications of (16). The true nature of the Poisson distribution will become apparent only in connection with the theory of stochastic processes. . Consider a sequence of random events occurring in time, such as, radioactive disintegrations or incoming calls at a telephone exchange. Each event is represented by a point on the time axis, and we are concerned with chance distributions of points. There exists many different types of such distributions, but their study belongs to the domain of continuous probabilities which we have postponed to the second volume. Here, we shall be content to show that the simplest physical assumptions lead to $p(k; lambda)$ as the probability of finding exactly $k$ points (events) within a fixed interval of specified length. Our methods are necessarily crude, and we shall return to the same problem with more adequate methods in chapter XVII. . The physical assumptions which we want to express mathematically are that the conditions of the experiment remain constant in time, and that non-overlapping time intervals are stochastically independent in the sense that information concerning the number of events in one interval reveals nothing about the other. The theory of probabilities in a continuum makes it possible to express these statements directly, but being restricted to discrete probabilities, we have to use an approximate finite model and pass to the limit. . Imagine that the unit time interval divided into a great number $n$ of intervals, each of length $1/n$. Either a particular subinterval is empty or it contains atleast one of our random points(or events) and we agree to call the two possibilities failure and success, respectively. The probability $p_n$ of success must be the same for all $n$ subintervals, since they have the same length. . Think of the intervals as bins and the events such as an emission of an $ alpha$-particle by a radioactive element as a ball. Then, a ball is equally likely to be placed in any of the $n$ bins, as they are intervals of equal length. . The assumed independence of non-overlapping intervals then implies that we have $n$ Bernoulli trials and the probability of exactly $k$ successes is given by $b(k;n,p)$. Now the number of successes is not necessarily the same as the number of random points, since a subinterval may contain several random points. Because, when a bin is assigned its first ball, that counts as a success, any subsequent placement of balls into this bin are technically failures. . However, it is natural to introduce the additional assumption that the probability of two or more random points($ alpha$-particls being emitted) during a very short teeny-weeny time interval is negligible. So, if you divvy up the unit time interval in a very large number $n$ of subintervals, it is unlikely two have two or more random points in the same sub-interval. . In this limiting case, the probability of finding exactly $k$ random points in the unit time interval is given by the limit of $(b,k;n,p_n)$ as $n to infty$. When we divide each subinterval into two parts of equal length, we find that, the probability that one or more balls falls into this sub-interval equals, $p_n = p_{2n} + p_{2n} - p_{2n}^2 = 2p_n - p_{2n}^2$ by inclusion-exclusion. . This equation states that success in the left half, or success in the right half, or in both. It follows that $p_n &lt; 2p_{2n}$. So, $p_n &lt; 2p_{2n} &lt; 4p_{4n} &lt; 8p_{8n} &lt; ldots$. Thus, $np_n$ increases monotonically. If $np_n to lambda$, then $b(k;n,p_n) approx b(k;n, frac{ lambda}{n}) to p(k; lambda)$ and we find that (16) as the the probability that there is a total of $k$ random points cotained in our unit interval. . Note, that by the weak LLN formulation, the average number of successes; or the average number of random points($ alpha$-particles emitted) in a unit interval of time is $np_n$. The assumption, $np_n to infty$ leads to no sensible result, as it would imply infinitely many random points even in the smallest interval. . If instead of the unit interval, we take an arbitrary interval of length $t$ and again use a subdivision into intervals of length $1/n$, then we have Bernoulli trials with the same probability $p_n$ of success, but the number of trials is the integer nearest to $nt$ rather than $n$. The passage to the limit is the same, but we get $nt cdot p_n to lambda t$ instead of $ lambda$. . . This leads us to consider, . begin{align*} p(k; lambda t) = e^{- lambda t} frac{( lambda t)^k}{k!} tag{17} end{align*}as the probability of finding exactly $k$ points in a fixed interval of length $t$. . . In particular, the probability of no poinnt in an interval of length $t$ is: . begin{align*} p(0; lambda t) = e^{- lambda t} tag{18} end{align*}and the probability of one or more points is therefore $1 - e^{- lambda t}$. . The parameter $ lambda$ is a physical constant which determines the density of points on the $t$-axis. The larger $ lambda$ is, the smaller is the probability of finding no point. Suppose that a physical experiment is repeated a great number $N$ of times, and that each time we count the number of events in an interval of fixed length $t$. Let $N_k$ be the number of times that exactly $k$ events are observed. Then, . begin{align*} N_0 + N_1 + N_2 + ldots = N tag{19} end{align*}The total number of points observed in the $N$ experiments is: . begin{align*} N_1 + 2N_2 + 3N_3 + ldots = T tag{20} end{align*}and $T/N$ is average. If $N$ is large, we expect that . begin{align*} N_k approx N p(k; lambda t) tag{21} end{align*}(this lies at the root of all applications of probability and will be justified and made more precise by the law of large numbers in chapter X). Substituting from (21) into (20), we find . begin{align*} T &amp; approx N {p(1; lambda t) + 2p(2; lambda t) + 3p(3; lambda t) + ldots } tag{22} &amp;= Ne^{- lambda t} sum_{k=1}^{ infty} frac{( lambda t)^k}{k!} &amp;= Ne^{- lambda t} lambda t sum_{k=0}^{ infty} frac{( lambda t)^k}{k!} &amp;= Ne^{- lambda t} lambda t e^{ lambda t} &amp;= N lambda t end{align*}and hence, . begin{align*} lambda t = frac{T}{N} tag{23} end{align*}This relation gives us a means of estimating $ lambda$ from observations and of comparing the theory with experiments. The examples of the next section will illustrate this. . Spatial Distributions. . We have considered the distribution of random events or points along the $t$-axis, but the same argument applies to the distribution of points in plane or space. Instead of intervals of length $t$, we have domains of length, area or volume $t$, and the fundamental assumption is that the probability of finding $k$ points in any specified domain depends only on the area or volume of the domain, but not it&#39;s shape. Otherwise, we have the same assumptions as before: (1) If $t$ is small, the probability of finding more than one point in a domain of volume $t$ is small as compared to $t$; (2) non-overlapping domains are mutually independent. To find the probability that a domain of volume $t$ contains exactly $k$ random points, we subdivide it into $n$ subdomains and approximate the required probability of $k$ successes in $n$ trials. This means neglecting the possibility of finding more than one point in the same subdomain, but our assumption (1) implies that the error tends to zero as $n to infty$. In the limit we again get the Poisson distribution (17). Stars in space, raisins in cake, weed seeds among grass seeds, flaws in materials, animal litters in fields are distributed in accordance with the Poisson law. . Observations Fitting the Poisson Distribution. . Radioactive disintegrations. A radioactive substance emits $ alpha$-particles; the number of particles reaching a given portion of space during time $t$ is the best-known example of random events obeying the Poisson law. Of course, the substance continues to decay and in the long run, the density of $ alpha$-particles will decline. However, with radium it takes years before a decrease of matter can be detected; for relatively short periods, the conditions may be considered constant, and we have an ideal realization of the hypotheses which led to the Poisson distribution. . In a famous experiment by Ernest Rutherford, a radioactive substance was observed during $N=2608$ time intervals of $7.5$ seconds each; the number of particles reaching a counter was obtained for each period. The below table records the number $N_k$ of the periods with exactly $k$ particles. The total number of particles is $T = sum k N_k = 10,094$, the average $T/N=3.870$. The theoretical values $Np(k;3.870)$ are seen rather close to the observed numbers $N_k$. . begin{array}{ccc} hline k &amp; N_k &amp; Np(k;3.870) hline 0 &amp; 57 &amp; 54.399 1 &amp; 203 &amp; 210.523 2 &amp; 383 &amp; 407.361 3 &amp; 525 &amp; 525.496 4 &amp; 532 &amp; 508.418 5 &amp; 408 &amp; 393.515 6 &amp; 273 &amp; 253.817 7 &amp; 139 &amp; 140.325 8 &amp; 45 &amp; 67.882 9 &amp; 27 &amp; 29.189 k geq 10 &amp; 16 &amp; 17.075 hline text{ Total } &amp; 2608 &amp; 2608.00 end{array} Flying bomb hits on London. As an example of a spatial distribution of random points consider the statistics of flying-bomb hits in the South of London during World War II. The entire area is divided into $N=576$ small areas of $t= frac{1}{4}$ square kilometers each, and records the number $N_k$ of areas with exactly $k$ hits. The total number of hits is $T = sum k N_k = 537$, the average $ lambda t = T/N = 0.9323$. The fit of the Poisson distribution is surprisingly good. It is interesting to note that most people believed in a tendency of the points of impact to cluster. If this were true, there would be a higher frequency of areas with either many hits or no hit and a deficiency in the intermediate classes. . Chromosome interchanges in cells. Irradiation by X-rays produces certain processes in organic cells which we call chromosome interchanges. As long as radiation continues, the probability of such interchanges remains constant, and according to theory, the numbers $N_k$ of the cells with exactly $k$ interchanges should follow a Poisson distribution. The theory is also able to predict the dependence of the parameter $ lambda$ on the intensity of radiation, the temperature etc., but we shall not enter into these details. . Connections to the wrong number. A total of $N=267$ telephone numbers were observed; $N_k$ indicates how many number had exactly $k$ wrong connections. The Poisson distribution $p(k;8.74)$ shows again an excellent fit. . Bacteria and blood counts. Consider a photograph of a petri plate with bacterial colonies, whch are visible under the microscope as dark spots. The plate is divided into small squares. The observed number of squares with exactly $k$ dark spots was $Np(k; lambda t)$. We have here an important practical application of the poisson distribution to spatial distributions of random points. . Waiting times. The negative Binomial Distribution. . Consider a succession of $n$ Bernoulli trials and let us inquire how long it will take for the $r$th success to turn up. Here $r$ is a fixed positive integer. The total number of successes in $n$ trials may, of course, fall short of $r$, but the probability that the $r$th success occurs at the trial number $ nu leq n$ is clearly independent of $n$ and depends only on $ nu,r,p$. Since, necessarily $ nu geq r$, it is prefereable to write $ nu = k + r$. The probability that the $r$th successes occurs at the trial number $r + k$ (where $k=0,1,2, ldots$) will be denoted by $f(k;r,p)$. It equals the probability that exactly $k$ failures preced the $r$th success. This event occurs if, and only if, among the $r+ k - 1$ trials there are exactly $k$ failures, and the following, or $(r+k)$th trial, results in success; the corresponding probabilities are ${{r + k - 1} choose k} cdot p^{r-1}q^k$ and $p$, so that: . begin{align*} f(k;r,p) = {{r + k - 1} choose k} cdot p^r q^k tag{24} end{align*} . Since, for any $a &gt; 0$ . begin{align*} {-a choose k} &amp;= frac{(-a)(-a-1) cdots(-a-(k-1))(-a-k)(-a-(k+1)) cdots (-3)(-2)(-1)}{(-a-k)!k!} &amp;= frac{(-a)(-a-1) cdots(-a-(k-1))}{k!} &amp;= (-1)^k frac{(a+k-1) cdots (a+1)a}{k!} &amp;= (-1)^k frac{(a+k-1) cdots (a+1)a(a-1)!}{k!(a-1)!} &amp;= (-1)^k frac{(a+k-1)!}{(a-1)!k!} &amp;= (-1)^k {a + k - 1 choose k} end{align*} . We find the alternative form: . begin{align*} f(k;r,p) = {-r choose k}p^r (-q)^k tag{25} end{align*} . Suppose now that Bernoulli trials are continued as long as necessary for $r$ successes to turn up. A typical sample point is represented by a sequence containing an arbitrary number, $k$, of letters $F$ and exactly $r$ letters $S$, the sequence terminating by an $S$; the probabilty of such a point is, by definition $p^r q^k$. We must ask, however, whether it is possible that the trials never end, that is, whether an infinite sequence of trials may produce fewer than $r$ successes. Now, $ sum_{k=0}^{ infty}f(k;r,p)$ is the probability that the trials never end, that is, whether an infinite sequence of trials may produce fewer than $r$ successes. Now, $ sum_{k=0}^{ infty}f(k;r,p)$ is the probability that the $r$th success occurs after finitely many trials; accordingly, the possibility of an infinite sequence with fewer than $r$ successes can be discounted if, and only if, . begin{align*} sum_{k=0}^{ infty}f(k;r,p) = 1 tag{26} end{align*}To prove that (26) holds, it suffices to note that: . begin{align*} frac{1}{1-q} &amp;= 1 + q + q^2 + q^3 + ldots + q^r + q^{r+1} + q^{r+2} + ldots frac{1}{(1-q)^2} &amp;= 1 + 2q + 3q^2 + 4q^3 + ldots + rq^{r-1} + (r+1)q^r + (r+2)q^{r+1} + ldots quad { text{ Differentiating on both sides} } frac{1 cdot 2}{(1-q)^3} &amp;= 2 cdot 1 + 3 cdot 2q + 4 cdot 3q^2 + ldots + r(r-1)q^{r-2} + (r+1)rq^{r-1} + (r+2)(r+1)q^{r} + ldots quad { text{ Differentiating on both sides} } frac{(r-1)!}{(1-q)^r} &amp;= r! + frac{(r+1)!}{2!}q^{1} + frac{(r+2)!}{3!}q^{2} + ldots quad { text{ Differentiating $r$ times} } frac{1}{(1-q)^r} &amp;= frac{r!}{(r-1)!1!} + frac{(r+(2-1))!}{(r-1)!2!}q^{1} + frac{(r+(3-1))!}{(r-1)!3!}q^{2} + ldots &amp; = sum_{k=0}^{ infty}{{r + k - 1} choose k}q^k &amp;= sum_{k=0}^{ infty}(-1)^k{-r choose k}q^k &amp;= sum_{k=0}^{ infty}{-r choose k}(-q)^k tag{27} end{align*} . From (27), we have that . begin{align*} sum_{k=0}^{ infty}{-r choose k}(-q)^k &amp;= (1-q)^{-r} = p^{-r} sum_{k=0}^{ infty}{-r choose k}p^r(-q)^k &amp;= p^{-r} cdot p^r = 1 sum_{k=0}^{ infty}f(k;r,p) &amp;= 1 tag{28} end{align*} . In our waiting time problem, $r$ is necessarily a positive integer, but the quantity defined by either (24) or (25) is non-negative and holds for any positive $r$. For arbitrary fixed real $r&gt;0$ and $0&lt;p&lt;1$, the sequence $ {f(k;r,p) }$ is called a negative binomial distribution. When $r$ is a positive integer, $ {f(k;r,p) }$ may be interpreted as the probability distribution for the waiting time to the $r$th success; as such it is aalso called the Pascal distribution. For $r = 1$, it reduces to the geometric distribution $ {pq^k }$. . Banach&#39;s Matchbox problem. A certain mathematician always carries one match box in his right pocket and one in his left. When he wants a match, he selects a pocket at random, the successive choices thus constituting Bernoulli trials with $p= frac{1}{2}$. Suppose that initially each box contained exactly $N$ matches and consider the moment when, for the first time, our mathematician discovers that a box is empty. At that moment the other box may contain $0,1,2, ldots,N$ matches and we denote the corresponding probabilities by $u_r$ Let us identify success with the choice of the left pocket. The left pocket will be found empty at a moment when the right pocket contains exactly $r$ matches, if and only if, $N-r$ failures precede the $(N+1)$st success. The probability of this event is $f(N-r;N+1, frac{1}{2})$. The same argument applies to the right pocket and therefore the required probability is: . begin{align*} u_r = 2f(N-r;N+1, frac{1}{2}) ={{2N - r} choose {N-r}} left( frac{1}{2} right)^{2N-r} = {{2N - r} choose {N}} left( frac{1}{2} right)^{2N-r} tag{29} end{align*} . # Distribution of the waiting time to find the any of the matchboxes empty using Plots using Distributions plotlyjs() N = 20 function negative_binomial(k,r,p) return binomial(r + k - 1,k) * (p^r) * (1-p)^k end plot([r for r in 0:N],[2*negative_binomial(N-r,N+1,0.50) for r in 0:N], line=:stem, marker=:circle, c=:red, xlabel=&quot;Number of matches left, r&quot;, ylabel=&quot;Probability&quot;, label=&quot;PMF&quot;) . &lt;!DOCTYPE html&gt; Plots.jl . . The Hypergeometric distribution. . If we have an urn filled with $w$ white balls and $b$ black balls, then drawing $n$ balls out of the urn with replacement yields a $b(k;n,w/(w+b))$ distribution for the number of white balls obtained in $n$ trials, since the draws are independent Bernoulli trials, each with probability $w/(w+b)$ of success. If we instead sample without replacement, then the number of white balls follows a hypergeometric distribution . Consider an urn with $w$ white balls and $b$ black balls. We draw $n$ balls out of the urn at random without replacement, such that all ${w + b} choose n$ samples are equally likely. Let $X$ be the number of white balls in the sample. Then, $X$ is said to have the hypergeometric distribution with parameters $w$, $b$ and $n$; we denote this by $X sim HGeom(w,b,n)$. . As with the binomial distribution, we can obtain the PMF of the hypergeometric distribution from the story. . . Theorem. If $X sim HGeom(w,b,n)$, then the PMF of $X$ is . begin{align*} P(X=k) = frac{{w choose k}{b choose n -k}}{{w+b} choose n} tag{30} end{align*} . . Proof. . To get $P(X=k)$, we first count the number of possible ways to draw exactly $k$ white balls and $n-k$ black balls (without distinguishing between different orderings for getting the same set of balls.) There are ${w choose k}{b choose {n - k}}$ ways to draw $k$ white balls and $n - k$ black balls by the multiplication rule, and there are a total of ${w + b} choose n$ total ways to draw $n$ balls from $w + b$ balls. Since all samples are equally likely, the naive definition of probability gives . begin{align*} P(X = k) = frac{{w choose k}{b choose {n-k}}}{{w + b} choose n} end{align*} . The hypergeometric distribution comes up in many scenarios which, on the surface, have little in common with white and black balls in an urn. The essential structure of the Hypergeometric story is that items in a population are classified using two sets of tags: in the urn story, each ball is either white or black(this is the first set of tags), and each ball is either sampled or not sampled(this is the second set of tags). Furthermore, at least one of these sets of tags is assigned completely at random (in the urn story, the balls are sampled randomly, with all sets of the correct size equally likely). Then, $X sim HGeom(w,b,n)$ represents the number of twice-tagged items: in the urn story, balls that are both white and sampled. . The next two examples show seemingly dissimilar scenarios that are nonetheless isomorphic to the urn story. . Example. Elk capture-recapture. A forest has $N$ elk. Today, $m$ of the elks are captured, tagged and released into the wild. At a later date, $n$ elks are recaptured at random. Assume that the recaptured elks are equally likely to be any set of $n$ of the elk e.g. an elk that has been captured does not learn how to avoid being captured again. . By the story of the hypergeometric, the number of tagged elks in the recaptured sample is $HGeom(m,N-m,n)$. The $m$ tagged elks in this story correspond to the white balls and the $N-m$ untagged elks correspond to the black balls. Instead of sampling $n$ balls from the urn, we recapture $n$ elk from the forest. . Example. Aces in a poker hand In a five card hand drawn at random from well-shuffled deck, the number of aces in the hand $HGeom(4,48,5)$ distribution, which can be seen by thinking of the aces as the white balls and the non-aces as the black balls. Using the hypergeometric PMF, the probability that the hand has exactly three aces is . begin{align*} P(X = 3) = frac{{4 choose 3}{48 choose 2}}{52 choose 5} approx 0.0017 end{align*} . Discrete Uniform distribution. . A very simple story, closely connected to the naive definition of probability describes picking a random number from some finite set of possibilities. . . Story. (Discrete Uniform distribution) Let $C$ be a finite, nonempty set of numbers. Choose one of the numbers uniformly at random (all values in $C$ are equally likely). Call the chosen number $X$. Then, $X$ is said to have the discrete uniform distribution with parameter $C$; we denote this by $X sim DUnif(C)$. . The PMF of $X sim DUnif(C)$ is: . begin{align*} P(X = x) = frac{1}{|C|} tag{31} end{align*}for $x in C$ (and $0$ otherwise), since a PMF must sum to $1$. . . As with questions based on the naive definition of probability, questions based on a discrete uniform distribution reduce to counting problems. Specifically, for $X sim DUnif(C)$ and any $A subseteq C$, we have: . begin{align*} P(X in A) = frac{|A|}{|C|} tag{32} end{align*} . Example. Random slips of paper. There are 100 slips of paper in a hat, each of which has one of the numbers $1,2, ldots,100$ written on to it, with no number appearing more than once. Five of the slips are drawn one at a time. . First consider random sampling with replacement (with equal probabilities). . (a) What is the distribution of how many of the drawn slips have a value of atleast 80 written on them? . (b) What is the distribution of the value of the $j$th draw (for $1 leq j le 5$)? . (c) What is the probability that the number $100$ is drawn atleast once? . Now consider random sampling without replacement (with all sets of five slips equally likely to be chosen). . (d) What is the distribution of how many of the drawn slips have a value of atleast 80 written on them? . (e) What is the distribution of the value of the $j$th draw (for $1 leq j le 5$)? . (f) What is the probability that the number $100$ is drawn in the sample? . . Solution. (a) Let us identify success as drawing a slip with a value of atleast $80$ written on it. We are sampling with replacement. Hence, these are independent and identical Bernoulli trials. The probability of success $p = frac{21}{100}$, the probability of failure $q = frac{79}{100}$. Let $X$ be the number of successes in $n=5$ trials. $X sim Bin(k;5,0.21)$. The PMF of $X$ is given by, . begin{align*} P(X=k) &amp;= {5 choose k}(0.21)^k (0.79)^{5-k} end{align*} (b) Let $X$ be the value of the $j$th draw. $X sim DUnif( {1,2,3, ldots,100 })$. The PMF of $X$ is given by, . begin{align*} P(X = x) &amp;= frac{1}{100} end{align*} (c) Let us identify success with drawing $100$. We are sampling with replacement. Hence, these are independent and identical Bernoulli trials with probability of success, $p = frac{1}{100}$. Let $X$ be the number of times $100$ is drawn. $X sim Bin(k;5, frac{1}{100})$. . begin{align*} P(X geq 1) &amp;= 1 - P(X=0) &amp;= 1 - q^5 &amp;= 1 - left( frac{99}{100} right)^5 end{align*} (d) We are sampling without replacement. $X sim HGeom(21,79,5)$. The PMF of $X$ is given by . begin{align*} P(X=k) = frac{{21 choose k}{79 choose 5-k}}{100 choose 5} end{align*} . (e) Let $X$ be the number drawn in the $j$th trial. We are sampling without replacement, so the draws are not independent. However, still, the unconditional probability of drawing a pre-specified value in the $j$th draw, given that we don&#39;t have any knowledge about the numbers drawn in trials $1, ldots,j-1$, is given by: . begin{align*} P(X = k) = frac{1}{100} end{align*}(f) Let us identify success as including the number $100$ in the sample. We are sampling without replacement, so the draws are not independent. Let $X$ be the number of successes in $n = 5$ trials. $X sim HGeom(1,99,5)$. The probability of exactly one success is, . begin{align*} P(X = 1) &amp;= frac{{1 choose 1}{99 choose 4}}{100 choose 5} end{align*} . Cumulative Distribution Functions. . Another function that describes the distribution of a random variable is the cumulative distribution function(CDF). Unlike the PMF, which only discrete r.v.s possess, the CDF is defined for all r.v.s. . . Definition (Cumulative Distribution Function). The cumulative distribution function (CDF) of an r.v. $X$ is the function $F_X$ given by $F_X(x) = P(X leq x)$. When there is no risk of ambiguity, we sometimes drop the subscript and just write $F$ for a CDF. . . . Theorem. (Valid CDFs). Any CDF $F$ has the following properties. . 1) Increasing. If $x_1 &lt; x_2$, then $F(x_1) &lt; F(x_2)$. . 2) Right-continuous. The CDF is right continuous, that is . begin{align*} lim_{x to a^{+}} F(x) = F(a) end{align*}3) Convergence to $0$ and $1$ in the limits: . begin{align*} lim_{x to - infty} F(x) = 0 quad text{ and } quad lim_{x to infty}F(x) = 1 end{align*} . Proof. The above criteria are true for all CDFs. . The first criterion is true, because the event $ {X leq x_1 }$ is a subset of the event $ {X leq x_2 }$. So, $P(X leq x_1) leq P(X leq x_2)$. Consequently, $F_X(x_1) leq F_X(x_2)$. . To prove the second criterion rigorously, we need to first show that $ mathbb{P}$ is a continuous set function. That is, if $A_1 supseteq A_2 supseteq A_3 ldots$ and $A = bigcap_{n=1}^{ infty}A_n$, then as $n to infty$, $P(A_n) to P(A)$. We skip this rigorous proof for the moment, but see that it is intuitively true from the graph of a CDF. . For the third criterion, we have $F(x) = 0$ for $x&lt;0$ and . begin{align*} lim_{x to infty}F(x) = lim_{x to infty} P(X leq [[x]]) = lim_{x to infty} sum_{n=0}^{[[x]]}P(X =n) = sum_{n=0}^{ infty}P(X=n) = 1 end{align*}The converse is true too: we will show that given any function $F$ that meets this criteria, we can construct a random variable $X$ whose CDF is $F$. .",
            "url": "https://quantophile.github.io/mathematical-finance/probability-theory/2022/03/06/The-Binomial-and-Poisson-Distributions-(summary).html",
            "relUrl": "/probability-theory/2022/03/06/The-Binomial-and-Poisson-Distributions-(summary).html",
            "date": " • Mar 6, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Probability and Counting Methods.",
            "content": "Introduction. . The mathematical theory of probability gains practical value and an intuitive meaning in connection with real or conceptual experiments such as tossing a coin once, tossing a coin one hundred times, throwing three dice, arranging a deck of cards, matching two decks of cards, playing roulette, observing the life-span of a radio-active atom or a person, the number of busy trunklines in a telephone exchange, a random noise in an electrical communication system, routine quality control of a production process, frequency of accidents, the position of a particle under diffusion. . All these descriptions are rather vague, and in, order to render the theory meaningful, we have to agree on what we mean by possible results of the experiment or observation in question. . . Definition. (Event) The results of an experiment or observations are called events. . . We speak of the event that (i) rolling a dice resulted in face value $6$, (ii) tossing a coin resulted in a heads, (iii) of five coins tossed more than three fell heads. . We shall distinguish between compound(decomposable) and simple(elementary) events. For example, saying that a throw with two dice resulted in &quot;sum six&quot; amounts to saying that it resulted in . $$ {(1,5),(2,4),(3,3),(4,2),(5,1) }$$ . and this enumeration decomposes the event &quot;sum six&quot; into five simple events. Similarly, the event &quot;two odd faces&quot; admits of the decomposition . $$ begin{align*} {&amp;(1,1),(1,3),(1,5), &amp;(3,1),(3,3),(3,5) &amp;(5,1),(5,3),(5,5) } end{align*}$$ . into nine simple events. Note that if a throw results in $(3,3)$, then the same throw also results in the events &quot;sum six&quot; and &quot;two odd faces&quot;; these events are not mutually exclusive and hence may occur simultaneously. . The Sample Space. . (a) Distribution of three balls in three cells. . $$ begin{bmatrix} abc &amp; - &amp; - - &amp; abc &amp; - - &amp; - &amp; abc ab &amp; c &amp; - ac &amp; b &amp; - bc &amp; a &amp; - ab &amp; - &amp; c ac &amp; - &amp; b bc &amp; - &amp; a end{bmatrix} quad begin{bmatrix} a &amp; bc &amp; - b &amp; ac &amp; - c &amp; ab &amp; - a &amp; - &amp; bc b &amp; - &amp; ca c &amp; - &amp; ab - &amp; a &amp; bc - &amp; b &amp; ca - &amp; c &amp; ab end{bmatrix} quad begin{bmatrix} - &amp; ab &amp; c - &amp; ac &amp; b - &amp; bc &amp; a a &amp; b &amp; c a &amp; c &amp; b b &amp; a &amp; c b &amp; c &amp; a c &amp; a &amp; b c &amp; b &amp; a end{bmatrix} $$ If we want to speak about experiements or observations in a theoretical way and without ambiguity, we must first agree on the simple events representing the thinkable outcomes; they define the idealized experiment. It is usual to refer to a simple event such as $ {a|b|c }$ as sample points or points for short. . . Definition.(Sample point) Every indecomposable result of an experiment is a simple event, and is called a sample point. . . When distributing $3$ balls to $3$ cells, each ball $i=1,2,3$ can be assigned to one of three cells, so there are $3 times 3 times 3 = 3^3$ sample points. All events connected with the experiment with this experiement can be described in terms of the sample points. For example, the event &quot;each cell is singly occupied&quot; can be described by collection of the following $6$ sample points: . $$ {(a,b,c),(a,c,b),(b,a,c),(b,c,a),(c,a,b),(c,b,a)$$ . The experiment of distributing $10$ balls to $10$ has $10^{10}$ outcomes or sample points. . . Definition. (Sample space) The sample $S$ of an experiment is the collection of all possible outcomes of the experiment, i.e. all the sample points. . . The sample space of an experiment can be finite, countably infinite (similar to the set $ mathbf{N}$) or uncountable (similar to the set $ mathbf{R})$. When the sample space is finite, we can visualize it, enumerate it like in the example above of the distribution of $3$ balls in $3$ cells. . (b) Distribution of $r$ balls in $n$ cells. THe more general case of distributing $r$ balls to $n$ cells can be studied in the same manner. Each ball $i=1,2, ldots,r$ can be assigned one of $1,2,3,...n$ cells. So, this experiment has $ underbrace{n times n times ldots n}_{r text{ terms}} = n^r$ outcomes or sample points. . To us, the sample space defines the idealized experiment. We use the picturesque language of balls and cells, but the same sample space admits a great variety of different practical interpretations. To clarify this point, we list here a number of situations in which the intuitive background varies; all are however, abstractly equivalent to the scheme of placing $r$ balls into $n$ cells, in the sense that the outcomes differ only in their verbal description. . (b,1) Birthdays. The possible configurations of the the birthdays of $r$ people correspond to the different arrangements of $r$ balls in $n=365$ cells. (assuming one calendar year to have $365$ days). . (b,2) Accidents. Classifying $r$ accidents according to the week days when they occurred is equivalent to placing $r$ balls into $n=7$ cells. . (b,3) In firing at $n$ targets, the hits correspond to the balls, and the targets to the cells. . (b,4) Sampling. Let a group of $r$ be classifed according to say age, or profession. The classes play the role of our cells, the people that of balls. . (b,5) Irradiation in biology. When the cells of the retina of the eyes are exposed to light, the light particles play the role of balls, and the actual cells are the &quot;cells&quot; of our model. . (b,6) In cosmic ray experiments the particles hitting the Geiger counters represent the balls, and the counters function as cells. . (b,7) An elevator starts with $r$ passengers and stops at $n$ floors. The different arrangements of discharging the passengers are replicas of the different distributions of $r$ balls in $n$ cells. . (b,8) Dice. The possible outcomes of a throw of $r$ dice correspond to placing $r$ balls into $n=6$ cells. When tossing a coin, we in effect dealing with only $n=2$ cells. . (b,9) Random digits. The possible orderings of a sequence of $r$ digits correspond to the distribution of $r$ balls into ten cells called $0,1,2, ldots,9$. . (b,10) The sex distribution of $r$ persons. Here we have $n=2$ cells and $r$ balls. . (b,11) Theory of photographic emulsions. A photographic plate is covered with grains sensitive to light quanta : a grain reacts if it is hit by a certain number, $r$ of the quanta. For the theory of black-white contrast, we must know how many cells are likely to be hit by $r$ quanta. We have here an occupancy problem where the grains correspond to the cells, and the light quant to the balls. . We can now make the following statement about sample space and events. . Every thinkable outcome of a random experiment can be described by a sample point. We define the term event to mean the same as any aggregate of sample points. . . Example. (Coin Flips). A coin is flipped $10$ times. Writing Heads as $H$ and Tails as $T$, a possible outcome is $HHHTHHTTHT$, and the sample space is the set of all possible strings of length $10$ of $H$s and $T$s. We can (and will) encode $H$ as $1$ and $T$ as $0$, so that an outcome is a sequence $(s_1, ldots,s_10)$ with $s_j in {0,1 }$ and the sample space $S$ is the set of all such sequences. Now, let&#39;s look at some events: . . Let $A_1$ be the event that the first flip is Heads. As a set . $$A_1 = {(1,s_2, ldots,s_{10}) : s_j in {0,1 } text{ for }2 leq j leq 10 }$$ . This is subset of the sample space, so it is indeed an event; saying that $A_1$ occurs is the same thing as saying that the first flip is Heads. Similarly, let $A_j$ be the event that the $j$th flip is Heads for $j=2,3,4, ldots,10$. . Let $B$ be the event that at least one flip was Heads. As a set, . $$B = bigcap_{j=1}^{10} A_j$$ . Let $C$ be the event that all the flips were Heads. As a set, . $$C = bigcap_{j=1}^{10} A_j$$ . Let $D$ be the event that there were at least two consecuetive heads. As a set, . $$D = bigcap_{j=1}^{9}(A_j cap A_{j+1})$$ . . Example. (Pick a card, any card). A standard deck of playing cards has $13$ ranks : $ {2,3,4,5,6,7,8,9,10, text{Jack}, text{Queen}, text{King}, text{Ace} }$ $ times$ $4$ suits : $ { text{Heart}, text{Spade}, text{Club}, text{Diamond} }$. Pick a card from a standard deck of $52$ cards. The sample space $S$ is the set of all $52$ cards (so there are $52$ sample points) one for each card. Consider the following four events: . $A$: card is an ace. | $B$: card has a black suit. | $D$: card is a diamond. | $H$: card is a heart. | . . As a set, $H$ consists of $13$ cards: . $ { text{Ace of hearts}, text{Two of Hearts}, ldots, text{King of hearts} }$ . We can create various other events in terms of $A,B,D,H$. For example, $A cap H$ is the event that the card is the Ace of Hearts, $A cap B$ is the event $ { text{Ace of spades}, text{Ace of clubs} }$ and $A cup D cup H$ is the event that the card is red or an ace. . There are many other events that could be defined using this sample space. In fact, the counting methods introduced later in this chapter show that there are $2^{52} approx 4.5 times 10^{15}$ events in this problem, even though there are only $52$ sample points. . Relation amongst events. . Definition. We use the notation $A = 0$ or $A = emptyset$ to express that the event $A$ is a null event and contains no sample points (is impossible). The zero must be interpreted in a symbolic sense and not as a numeral. . . . Definition. The event consisting of all points not contained in the event $A$ will be called the complementary event (or negation of $A$) and will be denoted by $A^C$. In particular, $S^C = emptyset$. . . With any two events $A$ and $B$ one can associate two new events defined by the conditions &quot;both $A$ and $B$ occur&quot; and &quot;atleast one of $A$ and $B$&quot; occurs. These events are denoted by $A cap B$ and $A cup B$. The event $A cap B$ contains all the sample points which are common to $A$ and $B$. If $A$ and $B$ exclude each other, then there are no points common to $A$ and $B$ and the event $A cap B$ is impossible, analytically this situation is described by $A cap B = emptyset$, which should be read as $A$ and $B$ are mutually exclusive. . Mathematically, . $$A cup B = {x : x in A text{ or } x in B }$$ . and . $$A cap B = {x : x in A text{ and } x in B }$$ . The event $A cap B^C$, means that both $A$ and $B^C$ should occur, in other words, $A$ but not $B$ occurs. This is also known as set difference and denoted by the event $A - B$. . Similarly, $A^C cap B^C$ means that neither $A$ nor $B$ occurs. The event $A cap B$ means that atleast one of the events $A$ and $B$ occurs; it contains all sample points except those that belong neither to $A$ nor to $B$. Thus, . $$A cup B = (A^C cap B^C)^C$$ . De-Morgan&#39;s Laws . . Theorem. Let $A$ and $B$ be any two events. We claim: . $$ begin{align*} (A cup B)^C &amp;= A^C cap B^C (A cap B)^C &amp;= A^C cup B^C end{align*} $$ . Proof. . (I) $ Longrightarrow$ direction. . Let $x$ be a point in $(A cup B)^C$. Then $x$ belongs to neither $A$, nor $B$. Consequently, $x in (A^C cap B^C)$. So, $(A cup B)^C subseteq A^C cap B^C$. . $ Longleftarrow$ direction. . Let $x$ be a point in $A^C cap B^C$. By definition, $x$ belongs to both the sets $A^C$ and $B^C$. So, $x notin A$ and $x notin B$. Consequently, $x$ belongs to a set, that contains all points except those from either $A$ or $B$. Thus, $x in (A cup B)^C$. Therefore $(A^C cap B^C) subseteq (A cup B)^C$. . Thus, $(A cup B)^C subseteq A^C cap B^C$ and $(A^C cap B^C) subseteq (A cup B)^C$. Consequently, . $$(A cup B)^C = (A^C cap B^C)$$ . (II) It is an easy exercise to prove this proposition. . Discrete Sample Space. . Definition(Discrete Sample Space). A sample space is called discrete if it contains a countable number of points. A countable set is one whose elements have a one-to-one correspondence with the natural numbers. . . (a) Let us toss a coin as often as necessary to turn up one head. The points of the sample space can then be enumerated as: . $$ {H,TH,TTH,TTTH,TTTTH, ldots }$$ . This is countable and hence discrete sample space. . (b) Let us pick a real number at randomly from the interval $[0,1]$. The points of this sample space cannot be enumerated. There is no way to list them all, and you cannot have a one-to-one correspondence between the points in $[0,1]$ and the natural numbers. You cannot count them. Hence, this sample space . $$S = {s : s in [0,1] }$$ . is not discrete. . Basic definitions and conventions. . Fundamental Convention. Given a discrete sample space $S$ with sample points $ {s_1,s_2 , ldots }$ we shall assume that with each point $s_j$, there is associated a number, called the probability of $s_j$ and denoted by $P( {s_j })$. This number is non-negative and less than unity. We have the following axioms: . (1) The probability of any event $A$ is the sum of the probabilities of all sample points in it. . $$P(A) = sum_{s in A}P( {s })$$ . (2) By convention, the probability of the entire sample space $S$ is taken to be unity. . $$P(S) = sum_{s in S} P( {s }) = 1$$ . Intuitively, this holds because, when a random experiment is performed, no matter what the outcome, it always belongs to $S$. So, the event $S$ is a certain event. . (3) The probability of an event $A$ lies between $0$ and $1$. . Since $A subseteq S$, the sample points in $A$ are a subset of the sample points in $S$. Their total contribution $ sum_{s in A}P( {s })$ must be non-negative and less than $ sum_{s in S}P( {s }) = 1$. Hence, . $$0 leq P(A) leq 1$$ . . Boole&#39;s Inequality. . Consider now two arbitrary events $A_1$ and $A_2$. To compute the probability $P {A_1 cup A_2 }$ that atleast $A_1$ or $A_2$ occur, we have to add the sample points contained in either $A_1$ or in $A_2$, but each point is to be counted only once. We have the following inequality: . $$P(A_1 cup A_2) leq P(A_1) + P(A_2)$$ . Now, if $s$ is any point contained in both $A_1$ and in $A_2$, then $P( {s })$ occurs twice in the right hand member but once in the left hand member. Therefore, the right side exceeds the left side by a factor $P(A_1 cap A_2)$. . Inclusion-Exclusion Principle. . Theorem. For any two events $A_1$ and $A_2$, the probability that atleast one of $A_1$ or $A_2$ occur is given by, . $$P(A_1 cup A_2) = P(A_1) + P(A_2) - P(A_1 cap A_2)$$ . If $P(A_1 cap A_2) = 0$, that is if $A_1$ and $A_2$ are mutually exclusive then the above equation reduces to : . $$P(A_1 cup A_2) = P(A_1) + P(A_2)$$ . . . Example. A coin is tossed twice. For the sample space, we take the four points $HH$, $HT$, $TH$ and $TT$ and associate with each the probability $ frac{1}{4}$. Let $A_1$ and $A_2$ be respectively the events, &quot;head at first&quot; and &quot;head at second&quot; trial. . . Then, $A_1$ consists of $ {HH,HT }$ and $A_2$ consists of $ {HH,TH }$. Furthermore, $(A_1 cup A_2)$ consists of $ {HH,HT,TH }$ and $(A_1 cap A_2)$ consists of a single point $ {HH }$. Thus, . $$P(A_1 cup A_2) = frac{1}{2} + frac{1}{2} - frac{1}{4} = frac{3}{4}$$ . Naive definition of Probability. . Historically, the earliest definition of probability of an event was to count the number of ways an event could happen and divide by the total number of outcomes for the experiement. We call this the naive definition of probability; and it relies on strong assumptions. Neverhthless, it is important to understand, and it is extremely useful when not misused. . . Definition. (Naive Definition of Probability). Let $A$ be an event for an experiment with a finite sample $S$. The probability of the event $A$ is . $$P(A) = frac{ text{Number of sample points in A}}{ text{number of sample points in S}} = frac{n(A)}{n(S)}$$ . if and only if all sample points are equally likely (equiprobable). . . Elements of Combinatorial Analysis. . The purpose of this section is to derive a few basic formulas and to develop the corresponding basic probabilistic background. In the study of the simple games of chance, sampling procedures, occupancy and order problems, we are usually dealing with finite sample spaces in which the same probability is attributed to all points. To compute the probability of the event $A$ we have then to divide the number of sample points in $A$ (&quot;favorable cases&quot;) by the total number of sample points (&quot;possible cases&quot;). This is facilitated by the systematic use of few rules which we shall now proceed to review. . . Pairs. With $m$ elements $a_1, ldots,a_m$ and $n$ elements $b_1, ldots,b_n$, it is possible to form $mn$ pairs $(a_j,b_k)$ containing one element from each group. . . Proof. Arrange the pairs in a rectangular array in the form of a multiplication table with $m$ rows and $n$ columns so that $(a_j,b_k)$ stands at the intersection of the $j$th row and the $k$th column. Then, each pair appears once and only once, and the assertion becomes obvious. . . Multiplets. Given $n_1$ elements $a_1, ldots,a_{n1}$ and $n_2$ elements $b_1, ldots,b_{n_2}$ upto $n_r$ elements $x_1,x_2, ldots,x_{n_r}$, it is possible to form $n_1 cdot n_2 cdots n_r$ ordered $r$-tuplets $(a_{j_1},b_{j_2}, ldots,x_{j_r})$ containing one element of each kind. . . Proof. If $r = 2$, the assertion reduces to the first rule. If $r = 3$, then take the pair $(a_i,b_j)$ as element of a new kind. There are $n_1n_2$ such pairs and $n_3$ elements $c_k$. Each triple $(a_i,b_j,c_k)$ is itself a pair consisting of $(a_i,b_j)$ and an element $c_k$; the number of triplets is therefore $n_1 n_2 n_3$. Proceeding by induction, the assertion follows for every $r$. . Perhaps the simplest and the most useful way of describing the last theorem is as follows. To form an $r$-tuplet $(a_{j_1},b_{j_2}, ldots,x_{j_r})$ we have to choose one $a$, one $b$ etc. We have to perform $r$ selections in all and have in succession $n_1,n_2, ldots,n_r$ possibilities to choose from. It is asserted that this procedure can lead to $n_1 cdot n_2 cdots n_r$ different results. . . Examples. . (a) Multiple classifications. Suppose that people are classified according to sex, marital status, and profession. The various categories play the role of elements. If there are $17$ professions, then we have $2 cdot 2 cdot 17 = 68$ classes in all. . (b) In an agricultural experiment three different treatments are to be tested (for example, the application of a fertilizer, a spray and temperature). If these treatments can be applied at $r_1$,$r_2$ and $r_3$ levels or concentrations, respectively, then there exist a total of $r_1 r_2 r_3$ combinations or ways of treatment. . (c) Placing balls into cells amounts to choosing one cell for each ball. With $r$ balls, we have $n$ choices for each ball, therefore $r$ balls can be placed in $n$ cells in $n^r$ different ways. For example, considering the faces of a die as cells, the experiment of throwing a die $r$ times has $6^r$ possible outcomes. . (d) A set with $n$ elements has $2^n$ subsets including the empty set $ emptyset$ and the set itself. This follows from the multiplication ruule, since for each element we can choose whether to include it or exclude it. For example, $ { 1,2,3 }$ has $8$ subsets, $ emptyset, {1 }, {2 }, {3 }, {1,2 }, {1,3 }, {2,3 }, {1,2,3 }$. . . Ordered samples. . Definition (Population). A population is a set, an aggregate of $n$ elements without regard to their order. . . . Definition (Sample). Consider the set or population of $n$ elements $ {a_1,a_2, ldots,a_n }$. Any ordered arrangement $(a_{j_1},a_{j_2}, ldots,a_{j_r})$ of $r$ symbols drawn from our population is called an ordered sample of size $r$ . . A sample is therefore an ordered $r$-tuple. . . For example, consider the population of $2$ elements $ {0,1 }$. Sampling $3$ times from this population could result in : $(0,0,0)$, $(0,0,1)$, $(0,1,0)$, $(0,1,1)$, $(1,0,0)$, $(1,0,1)$, $(1,1,0)$ and $(1,1,1)$. Note that, the sample $(0,0,1)$ is different from $(1,0,0)$ because samples are ordered tuples. . For an intuitive picture, we can imagine that the elements are selected one by one. Two procedures are then possible. First, sampling with replacement : here each selection is made from the entire population, so that same element can be drawn more than once. The samples are then arrangements in which repetitions are permitted. Second, sampling without replacement: each element once chosen is removed from the population, so that the sample becomes an arrangement without repetitions. Obviously in this case, the sample size $r$ cannot exceed the population size $n$. . In sampling with replacement each of the $r$ elements can be chosen in $n$ ways: the number of possible samples is therefore $n^r$, as can be seen from the last theorem with $n_1 = n_2 = ldots = n$. In sampling without replacement, we have $n$ possible choices for the first element, only $n-1$ for the second, $n-2$ for the third etc. Using the same rule, we see that in this case, we have $n(n-1) cdots (n-r+1)$ choices in all. Products of this type appear so often that it is convenient to introduce the notation . $$(n)_r = n(n-1) cdots (n-r+1) = frac{n!}{(n-r)!}$$ . and read it as &quot;n permute r&quot;. . . Theorem. For a population of $n$ elements and a prescribed sample of size $r$, there exist $n^r$ different samples with replacement and $(n)_r$ samples without replacement. . . We note the special case where $r = n$. In sampling without replacement, a sample of size $n$ includes the whole population and represents a reordering (or permutation) of its elements. Accordingly, $n$ elements $a_1, ldots,a_n$ can be ordered in $(n)_n = n cdot (n-1) cdots 1$ ways. Instead of $(n)_n$ we write $n!$, which is the more usual notation. We see that our theorem has the following corollary therefore: . . Corollary. The number of different orderings of $n$ elements is . $$n! = n cdot (n-1) cdot (n-2) cdots 1$$ . . Drawing $r$ elements from a population of size $n$ is an experiment whose possible outcomes are samples of size $r$. Their number is $n^r$ or $(n)_r$, depending on whether or not replacement is used. In either case, our conceptual experiment is described by a sample space in which each individual point is a sample of size $r$. . Thinking of any problem as a sampling experiment is an very important strategy for problem-solving. We can always try to cast the problem at hand, into (1)an allocation problem (placing $r$ balls in $n$ cells) (2) or a sampling problem (sampling $r$ times from a population of size $n$). . For example, if a coin is tossed $r=2$ times, this is like placing $r=2$ distinct balls in $n=2$ cells labelled &#39;H&#39; and &#39;T&#39;. Equivalently, we are sampling $r=2$ symbols with replacement from the population $ {H,T }$. The sample space consists of $2^2 = 4$ points: . $$ S = {(H,H),(H,T),(T,H),(T,T) } $$Each $2$-tuple, each sample of size $2$, such as $(H,T)$ or $(T,H)$ forms a point in the sample space $S$. The sample $(H,T)$ is different from $(T,H)$, because they are ordered tuples, and we distinguish between the two outcomes - (i) coin 1 lands heads, coin 2 lands tails vis-a-vis (2) coin 1 lands tails, coin 2 lands heads. . If a die is thrown $r=3$ times, this is like placing $r=3$ balls in $n=6$ cells having ID {1,2,3,4,5,6}. Equivalently, we are sampling $r=3$ times with replacement from the population $ {1,2,3,4,5,6 }$. The entire sample space consists of $6^3=216$ points. . $$ begin{align*} S= {&amp;(1,1,1),(1,1,2), ldots,(1,1,6), &amp;(1,2,1),(1,2,2), ldots,(1,2,6), &amp; vdots &amp;(6,6,1),(6,6,2), ldots,(6,6,6) } end{align*} $$Each $3$-tuple, each sample of size $3$, such as $(1,1,1)$ or $(1,3,5)$ forms a point in the sample space $S$. . So far we have not spoken of probabilities associated with our samples. Usually, we shall assign equal probabilities to all of them and then speak of random samples. The word random is not well defined, but when applied to samples or selections it has a unique meaning. Whenever we speak of random samples of fixed size $r$ the adjective random is to imply that all possible samples have the same probability, namely, $ frac{1}{n^r}$ in sampling with replacement and $ frac{1}{(n)_r}$ in sampling without replacement. . Examples. . We consider random samples of fixed size $r$ with replacement taken from a population of the $n$ elements $a_1, ldots,a_n$. We are interested in the event $A$, that in such a sample $(a_{j_1}, ldots,a_{j_r})$ no element appears twice, that is, that our sample could have been also obtained by sampling without replacement. The last theorem shows that there exist $n^r$ different samples in all, of which $(n)_r$ satisfy the stipulated condition. Assuming that all samples have equal probability, we conclude that probability of no repetition in our sample is . $$p = frac{(n)_r}{n^r} = frac{n cdot (n-1) cdot (n-r+1)}{n^r}$$ . The following concrete interpretations of the above generic statement are very interesting: . (a) Random sampling numbers. Let the population consist of ten digits $0,1,2, ldots,9$. Every succession of five digits represents a sample of size $r = 5$ and we assume that each such arrangement has probability $10^{-5}$. The probability that all five consecutive random digits are all different is $p=(10)_5 (10)^{-5}$. . (b) Birthday problem. There are $k$ people in a room. Assume each person&#39;s birthday is equally likely to be any of the $365$ days of the year (we exclude February 29), and that people&#39;s birthdays are independent (we assume there are no twins in the room). What is the probability that two or more people in the group have same birthday? . Each person&#39;s birthday will fall on exactly one of the calendar days $ {1,2,3, ldots,365 }$. Thus, we are sampling $k$ times from the population $ {1,2,3, ldots,365 }$. The number of ordered samples of size $k$, without replacement(repetition) are $(365)_k$. In total, there are $365^k$ samples with replacement. . So, . $$ begin{align*} P( text{No Birthday match}) &amp;= frac{(365)_k}{365^k} &amp;= frac{365 cdot 364 cdots (365-k+1)}{365 cdot 365 cdots 365} &amp;= 1 cdot left(1 - frac{1}{365} right) left(1 - frac{2}{365} right) cdots left(1 - frac{k-1}{365} right) end{align*}$$ Thus, . $$ begin{align*} P( text{Atleast one birthday match}) &amp;= 1 - P( text{No Birthday Match}) &amp;= 1 - left(1 - frac{1}{365} right) left(1 - frac{2}{365} right) cdots left(1 - frac{k-1}{365} right) end{align*}$$ Let&#39;s plot the probability of atleast one birthay match as a function of $k$. The first value of $k$ for which the probability of a match exceeds $0.50$ is $k=23$. Thus, in a group of $23$ people, there is better than $50 %$ chance that two or more them will share the same birthday. By the time we reach $k=57$, the probability of a match exceeds $99 %$. . def prob_atleast_one_bday_match(k:int) : prod = 1 for i in range(1,k): prod = prod * (1 - i/365) return 1 - prod . import numpy as np import matplotlib.pyplot as plt x = [k for k in range(1,101)] y = [prob_atleast_one_bday_match(k) for k in range(1,101)] plt.grid(True) plt.xlabel(&#39;k&#39;) plt.ylabel(&#39;probability of atleast one match&#39;) plt.plot(x,y,&#39;o&#39;) plt.show() . The birthday problem is much more than fun party game and much more than a way to build intuition about coincidences, there are also important applications in statistics and computer science. . Note. (Labeling objects). Drawing a sample from a population is a very fundamental concept in statistics. It is important to think of the objects or people in the population as named or labeled. For example, if there are $n$ balls in a jar, we can imagine that they have labels from $1$ to $n$, even if the balls look the same to the human eye. In the birthday problem, we can give each person an ID (identification) number. . (c) Elevator. An elevator starts with $r=7$ passengers and stops at $n=10$ floors. What is the probability $p$ that no two passengers leave at the same floor. Assume that all arrangements of discharging passengers leave at the same floor. . We are sampling $7$ times from the population $ {1,2,3, ldots,10 }$. The number of ordered samples of size $r=7$ without replacement (repetition) are $(10)_7$. The total number of samples with replacement are $10^7$. As each sample point is equally likely, we have: . $$P( text{No two passengers leave at the same floor}) = frac{(10)_7}{10^7}$$ . Example. If we roll two fair dice, which is more likely: a sum of $11$ or a sum of $12$? . We are sampling two times with replacement from the population {1,2,3,4,5,6}. The total number of sample points in th sample space are $6^2 = 36$. Each sample point is equally likely. Further, a sum of $11$ is obtained when two throws result in either $(5,6)$ or $(6,5)$. Therefore, . $$P( text{Sum of throws equals }11) = frac{2}{36}$$ . A sum of $12$ is obtained when the two throws result in $(6,6)$. Therefore, . $$P( text{Sum of throws equals }12) = frac{1}{36}$$ . Subpopulations or subsets. . As before we use the term population of size $n$ to denote an aggregate of $n$ elements without regard to their order. Two populations are considered different if one contains an element not contained in the other. For example, the sets $ {1,2,3 }$ and $ {1,2 }$ are different, because $ {1,2 }$ does not contain the element $3$. On the other hand, the set $ {1,2,3 }$ and $ {3,2,1 }$ are identical populations. . We choose $r$ elements and form a subset out of a given population of $n$ elements. . This is similar to the following conceptual experiment. We are given two jars - a big jar &#39;A&#39; and a small jar &#39;B&#39;. Jar &#39;A&#39; has $n$ slips of paper labelled $1,2,3, ldots,n$. We randomly draw $r$ slips and drop them in jar &#39;B&#39;. Jar &#39;B&#39; now contains a subpopulation of Jar &#39;A&#39;. . How many different subsets or subpopulations of size $r$ can be formed from a population of size $n$? . For example, suppose Jar &#39;A&#39; has three slips of paper labelled $ {1,2,3 }$. We randomly draw $r=2$ slips and drop them in Jar &#39;B&#39;. Jar &#39;B&#39; could contain one of 3 subsets $ {1,2 }$, $ {1,3 }$ or $ {2,3 }$. Assume that, the random experiment results in Jar &#39;B&#39; containing the subset $ {1,2 }$. Note that, we don&#39;t distinguish between the subsets $ {1,2 }$ and $ {2,1 }$. The constitution of the jar is the same, as long as the elements in the jar are the same. So, we don&#39;t care about the order in which the slips $1$ and $2$ were drawn. . Now, each subpopulation of size $r$ can be arranged in $r!$ different orders and in this way produces $r!$ samples without repetition. Conversely, each such sample of size $r$ contains $r$ different elements and thus defines a subpopulation of size $r$. We know that, there exist $(n)_r$ samples of the described sort. If $x$ is the number of subpopulations of size $r$, then obviously the number of ordered samples is $x cdot r!$, and we conclude that $x = (n)_r / r!$. . Number of this kind are known as binomial coefficients, and the standard notation for them is: . $${n choose r} = frac{(n)_r}{r!} = frac{n!}{r!(n-r)!}$$ . read as &quot;n choose r&quot;. . We have now proved: . . Theorem. The number of subsets(subpopulations) of size $k$ of a set(population) of $n$ elements is : . $${n choose k} = frac{n!}{k!(n-k)!}$$ . . Now choosing the $r$ elements to be taken out of the population, is the same as choose the $(n-r)$ elements which are to stay in. It is therefore clear, that : . $${n choose r} = {n choose n-r}$$ . Example. (Club Officers). In a club with $n$ people, there are $(n)_3= frac{n(n-1)(n-2)}{3!}$ ways to choose a president, a vice-president and a treasurer. This is like sampling $3$ times from the population $ {1,2,3, ldots,n }$. . There are ${n choose 3}$ ways to choose $3$ officers without pre-determined titles. This is like forming a subset of size $3$ from a population of size $n$. . Example. (Permutations of a word). How many ways are there to permute the letters in the word LALALAAA? . The word LALALAAA is an $8$-letter word. Suppose, we have $8$ cells or places labelled $1,2,3,4,5,6,7,8$. . $$ begin{matrix} _ &amp; _ &amp; _ &amp; _ &amp; _ &amp; _ &amp; _ &amp; _ 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 end{matrix} $$Let&#39;s first choose where the $5$ A&#39;s go. We need to select $r=5$ places from the population $ {1,2,3,4,5,6,7,8 }$. For the sake of simplicity, suppose we pick the spots $(1,2,3,4,5)$. Note that, even if we chose the places $(5,4,3,2,1)$ in that order, the outer appearance of the word A A A A A A _ remains the same. So, we need to choose $r=5$ places without regard to their order. . In other words, we select a subset of size $5$ from the place population $ {1,2,3,4,5,6,7,8 }$ of size $8$. This can be done ${8 choose 5}$ distinguishable ways. Having chosen where the $5$ A&#39;s go, equivalently means choosing where the $3$ L&#39;s are allocated. . So, to sum up, there are: . $${8 choose 5} = frac{8!}{3!5!}= frac{8 cdot 7 cdot 6}{6} = 56 text{ permutations.}$$ . Example. (Permutations of a word continued). How many ways are there to permute the letters in the word STATISTICS? . The word STATISTICS is a $10$-letter word. Suppose we have $10$ cells or positions labelled $1,2,3,4,5,6,7,8,9,10$. . Let&#39;s first choose where the $3$ S&#39;s go. We select a subset of size $3$ from the population of size $10$. This can be done in ${10 choose 3}$ distinguishable ways. We now have $7$ vacant positions to be occupied. . Next, choose where the $3$ T&#39;s go. We select a subset of size $3$ from a population of size $7$. This can be done in ${7 choose 3}$ distinguishable ways. We now have $4$ vacant positions to be occupied. . Next, choose where the $2$ I&#39;s go. We must select a subset of size $2$ from a population of size $4$. This can be done in ${4 choose 2}$ distinguishable ways. We now have $2$ vacant positions still to be occupied. . Next, choose where the $1$ A will go. We select a subset of size $1$ from a population of size $2$. This can be done in ${2 choose 1}$ distinguishable ways. We now have only $1$ vacant position still to be occupied. . Effectively we have allocated the letter $C$ to the only vacant place available. . By the multiplication rule, the number of permutations of the word LALALAAA are: . $${10 choose 3}{7 choose 4}{4 choose 2}{2 choose 1}{1 choose 1}=50400 text{ permutations}$$ . Example (Poker). In Poker, a $5$-card hand is dealt from a standard well-shuffled $52$-card deck. Since the order of the cards in a hand is irrelevant, by the last theorem, there are ${52 choose 5}=2,598,960$ hands at Poker. What is the probability that the hand at Poker contains different face values? . A standard $52$-card deck of playing cards has $13$-face values $ times$ $4$ suits. We select a subset of $5$-face values from the &quot;face-values&quot; population of size $13$. This can be done in ${13 choose 5}$ distinguishable ways. We sample $r=5$ times with replacement from the &quot;suits&quot; population $ { text{Heart}, text{Spade}, text{Club}, text{Diamond} }$ of size $4$. There $4^5$ such samples. It follows that: . $$P( text{Poker hand contains 5 distinct face-values}) = frac{{13 choose 5} cdot 4^5}{52 choose 5}$$ . Example (Full house in Poker). A $5$-card hand is dealt from a standard well-shuffled $52$-card deck. The hand is called a full house in poker, if it consists of three cards of some rank and two cards of another rank e.g. three 7&#39;s and two 10&#39;s in any order. What is the probability of a full house? . Solution. . All of the ${52 choose 5}$ possible hands are equally likely, so the naive definition of probability is applicable. To find the number of full house hands, use the multiplication rule. There are $13$ choices for first rank. There are ${4 choose 3}$ distinguishable ways of selecting $3$-suits for the three cards of first rank. For concreteness, assume that we have three 7&#39;s. There are $12$ choices for the second rank. (Now, we are free to choose from any $4$-suits for these cards, because they have a different rank.) There are $ {4 choose 2 }$ distinguishable ways of selecting $2$-suits for the two cards of the other rank. . $$P( text{full house}) = frac{13 {4 choose 3} 12 {4 choose 2}}{52 choose 5}$$ . Example. Each of the $48$ states has two senators. We consider the events that in a committee of $48$ senators chosen at random: (1) a given state is represented (2) all states are represented. . First, the order of senators in a committee is irrelvant, so there are ${96 choose 48}$ committees that can be formed. . (1) In the first case, it is better to calculate the probability $q$ of the complementary event, namely, that the given state is not represented. There are $96$ senators, and $94$ are not from the given state. Hence, . $$q = frac{94 choose 48}{96 choose 48}$$ . Hence, the desired probability is . $$p = 1 - q = 1 - frac{94 choose 48}{96 choose 48}$$ . (2) For the second case, we would like each state to be represented. Each of the 48 states have $2$ choices to send out a senator. Hence, the desire probability is : . $$P( text{all states are represented}) = frac{2^{48}}{96 choose 48}$$ . Example. (An occupancy problem) Consider once more a random distribution of $r$ balls in $n$ cells (i.e. each of $n^r$ possible arrangements are equally likely and have probability $n^{-r}$). We are interested to find the probability $p_k$ that a specified cell contains exactly $k$ balls $(k=0,1,2, ldots,r)$. . We note that the $k$ balls that go into the specified cell can be chosen in ${r choose k}$ distinguishable ways and the remaining $r - k$ balls can be placed into the remaining $(n-1)$ cells in $(n-1)^{(r-k)}$ ways. It follows that: . $$p_k = {r choose k} cdot frac{1}{n^r} cdot (n-1)^{(r-k)} = {r choose k} cdot frac{1}{n^k} cdot left(1- frac{1}{n} right)^{(r-k)} $$ . This is a special case of the so-called binomial distribution, which will taken up in chapter III. . Example. (Newton-Pepys Problem.) Issac Newton was consulted with the following problem by Samuel Ppery, who wanted the information for gambling puposes. Which of the following events has the highest probability? . $A$: Atleast one $6$ appears when $6$ fair dice are rolled. . $B$: Atleast two $6$&#39;s appear when $12$ fair dice are rolled. . $C$: Atleast three $6$&#39;s appear when $18$ fair dice are rolled. . Solution. The three experiments have $6^6$, $6^{12}$ and $6^{18}$ possible outcomes, respectively, and by symmetry the naive definition applied in all three experiments. . $A$: Instead of counting the number of ways to obtain atleast one $6$, it is easier to count the number ways to get no $6$&#39;s. Getting no $6$&#39;s is equivalent to sampling the numbers $1$ through $5$ with replacement $6$ times, so $5^6$ outcomes are favourable to $A^C$ (and $6^6 - 5^6$ are favorable to $A$). Thus, . $$P(A) = 1 - frac{5^6}{6^6}$$ . p_a = 1 - (5**6)/(6**6) p_a . 0.6651020233196159 . $B$: Again we count the outcomes in $B^C$ first. There are $5^12$ ways to get no $6$&#39;s in $12$ die rolls. There are ${12 choose 1}5^11$ ways to get exactly one $6$: we first choose which die lands $6$, then sample the numbers $1$ through $5$ with replacement for the other $11$ dice. Adding these, we get the number of ways to fail to obtain atleast two $6$&#39;s. Then: . $$P(B) = 1 - frac{5^12 + {12 choose 1}5^11}{6^12}$$ . p_B = 1 - (5**12 + 12 * (5**11))/(6**12) p_B . 0.6186673737323087 . We count the outcomes in $C^C$, that is the number of ways to get zero, one or two $6$&#39;s in $18$ die rolls. There are $5^{18}$ ways to get no sixes, ${18 choose 1}5^{17}$ ways to get one $6$ and ${18 choose 2}5^{16}$ to get two sixes. . $$P(C) = 1 - frac{5^{18} + {18 choose 1}5^{17} + {18 choose 2}5^{16}}{6^{18}}$$ . p_c = 1 - (5**18 + 18 * (5**17) + (18 * 17)/2 * (5**16))/(6**18) p_c . 0.5973456859477232 . Example. (a) How many paths are there from the point $(0,0)$ to the point $(110,111)$ in the plane such that each step either consists of going one unit up or unit to the right? . (b) How many paths are there from $(0,0)$ to $(210,211)$, where each step consists of going one unit up or one unit to the right, and the path has to go through $(110,111)$? . Solution. . (a) Let&#39;s encode a step of going one unit up as $u$ and a step of going one unit to the right as $r$. All paths that start at $(0,0)$ and terminate at $(110,111)$ on a grid, must have $110$ u&#39;s and $111$ r&#39;s, and total length $221$. A valid path is any sequence of the u&#39;s and r&#39;s. Let us label the places as $1,2,3, ldots,221$. . Let&#39;s choose where the u&#39;s will go. We must select a subset of size $110$ from a population of size $221$. There are ${221 choose 110}$ distinguishable ways of doing this. Equivalently, this determines where the r&#39;s go. . $$ text{Number of paths from the origin to the point (110,111)} = {221 choose 110}$$ . (b) There are ${200 choose 100}$ paths from the point $(110,111)$ to $(210,211)$. Consequently, by the multiplication rule, the required number of paths are: . $$ text{Total number of paths} = {221 choose 110}{200 choose 100}$$ .",
            "url": "https://quantophile.github.io/mathematical-finance/probability-theory/2022/03/06/Probability-and-Counting-Methods.html",
            "relUrl": "/probability-theory/2022/03/06/Probability-and-Counting-Methods.html",
            "date": " • Mar 6, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Joint Distributions.",
            "content": "Introduction. . The individual distributions of two random variables do not tell us anything about whether the random variables are independent or dependent. Although, the PMF of $X$ is a complete blueprint for the distribution of $X$ and the PMF of $Y$ is a complete blueprint for the distribution of $Y$, these individual PMFs are missing important information about the dependence structure of $X$ and $Y$. . Of course, in real life, we usually care about the relationship between multiple random variables in the same experiment. To give just a few examples: . Medicine : To evaluate the effectiveness of a treatement, we may take multiple measurements per patient, an ensemble of blood pressure, heart rate, and cholesterol reading can be more informative than any of these measurements considered separately. | Genetics : To study the relationships between various genetic markers and a particular disease, if we only looked separately at distributions for each genetic marker, we could fail to learn about whether an interaction between the markers is related to the disease. | Time-Series: To study how something evolves over time, we can often make a series of measurements over time, and then study the series jointly. There are many applications of such series, such as global temperatures, stock prices, or national unemployment rates. The series of measurements considered jointly can help us deduce trends for the purpose of forecasting future measurements. | . This write-up considers joint-distributions, also called multi-variate distributions, which capture the previously missing information about how multiple random variables interact. We introduce multivariate analogs of the CDF, PMF and the PDF in order to provide a complete specification of the relationship between multiple random variables. After this ground-work is in place, we&#39;ll study a couple of famous named multivariate distributions, generalizing the Binomial and Normal distributions to higher dimensions. . Joint, Marginal and Conditional. . The three key concepts for this section are joint, marginal and conditional distributions. Recall that the distribution of a single random variable $X$ provides complete information about the probability of $X$ falling into any subset of the real line. Analogously, the joint distribution of two random variables $X$ and $Y$ provides complete information about the probability of the random vector $(X,Y)$ falling into any subset of the plane. The marginal distribution of $X$ is the individual distribution of $X$, ignoring the value of $Y$ and the conditional distribution of $X$ given $Y=y$ is the updated distribution for $X$ after observing $Y=y$. We&#39;ll look at these concepts in the discrete case first, then extend them to the continuous case. . Discrete . The most general description of the joint distribution of two random variables is the joint CDF which applies to discrete and continuous random variables alike. . . Definition. (Joint CDF). The joint CDF of the random variables $X$ and $Y$ is the function $F_{X,Y}$ given by : . $$F_{X,Y}(x,y) = P(X leq x, Y leq y)$$ . . The joint CDF of a vector $n$ random variables $(X_1,X_2, ldots,X_n)$ is described analogously. . Unfortunately, the joint CDF of discrete random variables is not a well-behaved function; as in the univariate case, it consists of jumps and flat regions. For this reason, with discrete random variables, we usually work with the joint PMF, which also determines the joint distribution and is much easier to visualize. . . Definition. (Joint PMF). The joint PMF of discrete random variables $X$ and $Y$ is the function $f_{X,Y}$ given by, . $$f_{X,Y}(x,y) = P(X = x,Y=y)$$ . . The joint PMF of a vector of $n$ discrete random variables $(X_1,X_2, ldots,X_n)$ is described analogously. . Just as univariate PMFs must be nonnegative and sum to $1$, we require valid joint PMFs to be non-negative and sum to $1$, where the sum is taken over all possible values of $X$ and $Y$: . $$ sum_{x} sum_{y}P(X = x,Y = y) = 1$$ . The joint PMF determines the distribution because we can use it to find the probability of the event $(X,Y) in A$ for any set $A$ of points in the support of $(X,Y)$. All we have to do is sum the joint PMF over $A$: . $$P((X,Y) in A) = { sum sum}_{(x,y) in A} P(X = x, Y = y)$$ . import matplotlib.pyplot as plt import numpy as np X = np.random.rand(10000) Y = np.random.rand(10000) plt.hist2d(X, Y, bins=40) plt.show() . The above figure shows what the joint PMF of two discrete random variables could look like. The brightness of the colors at any point $(x,y)$ represents the probability $P(X = x, Y =y)$. For the joint PMF to be valid, the total height of the vertical bars must be $1$. . From the joint distribution $(X,Y)$ we can easily get the distribution of $X$ alone by summing over the possible values of $Y$. This gives us the familiar PMF of $X$ that we have seen earlier. In the context of joint distributions, we will call it the marginal or unconditional distribution of $X$, to make it clear that we are referring to the distribution of $X$ alone, without regard for the value of $Y$. . . Definition. (Marginal PMF). For discrete random variables $X$ and $Y$, the marginal PMF of $X$ is . $$ begin{align*} f_X(x) &amp;= P(X = x) = sum_{y} f_{X,Y}(x,y) = sum_{y} P(X = x, Y = y) f_Y(y) &amp;= P(X = y) = sum_{x} f_{X,Y}(x,y) = sum_{x} P(X = x, Y = y) end{align*} $$ . Example. Suppose a dice is rolled $N = 5$ times. Let $X$ be the number of ones and $Y$ be the number of twos. The pair $(X,Y)$ follows the $Multinomial(5,1/6,1/6)$ distribution. The joint PMF $f_{X,Y}(x,y)$ can be computed as follows. . import numpy as np from scipy.special import binom class multinomial: def __init__(self, numTrials, successProbabilities): self.n = numTrials self.p = successProbabilities def pmf(self, x): size = len(x) probability = 1.0; k = 0; sumProbabilities = 0.0; for j in range(size): probability = probability * binom(self.n - k,x[j]) * (self.p[j]**x[j]) k += x[j] sumProbabilities += self.p[j] if sumProbabilities &lt; 1.0: probability = probability * (1 - sumProbabilities)**(self.n - k) return probability . We can now create a multinomial random variable and print it&#39;s joint PMF. . rv = multinomial(numTrials=5,successProbabilities=[1/6,1/6]) x = np.array([0,1,2,3,4,5]) y = np.array([0,1,2,3,4,5]) jointPMF = np.zeros(shape=(6,6)) for j in range(6): for k in range(6): jointPMF[j][k] = rv.pmf([x[j],y[k]]) print(jointPMF) . [[ 1.31687243e-01 1.64609053e-01 8.23045267e-02 2.05761317e-02 2.57201646e-03 1.28600823e-04] [ 1.64609053e-01 1.64609053e-01 6.17283951e-02 1.02880658e-02 6.43004115e-04 0.00000000e+00] [ 8.23045267e-02 6.17283951e-02 1.54320988e-02 1.28600823e-03 0.00000000e+00 0.00000000e+00] [ 2.05761317e-02 1.02880658e-02 1.28600823e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00] [ 2.57201646e-03 6.43004115e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00] [ 1.28600823e-04 0.00000000e+00 -0.00000000e+00 0.00000000e+00 -0.00000000e+00 0.00000000e+00]] . The probability of the event $ {X = x_j, Y = y_k }$ is given by jointPMF[j][k]. . The marginal PMF of $X$ and $Y$ can be easily computed as: . print(&quot; nMarginal PMF of X&quot;) print(np.sum(jointPMF, axis=0)) print(&quot; nMarginal PMF of Y&quot;) print(np.sum(jointPMF, axis=1)) . Marginal PMF of X [4.01877572e-01 4.01877572e-01 1.60751029e-01 3.21502058e-02 3.21502058e-03 1.28600823e-04] Marginal PMF of Y [4.01877572e-01 4.01877572e-01 1.60751029e-01 3.21502058e-02 3.21502058e-03 1.28600823e-04] . . Definition. (Conditional PMF). . For discrete random variables $X$ and $Y$, the conditional PMF of $X$ given $Y = k$ is, . $$ f_{X|Y=k}(x) = P(X = x|Y = k) = frac{P(X = x, Y = k)}{P(Y = k)} = frac{f_{X,Y}(x,k)}{f_Y(k)} $$The conditional PMF of $Y$ given $X = k$ is, . $$ f_{Y|X=k}(y) = P(Y = y|X = k) = frac{P(Y = y, X = x)}{P(X = k)} = frac{f_{X,Y}(k,y)}{f_X(k)} $$ . For instance, in the dice example, the conditional PMF of the number of ones, i.e. $X$, given that we observe two, $3$ times, i.e. $Y=3$ is given by: . print(&quot; nConditional PMF P(X=x|Y=3)&quot;) conditionalPMF = jointPMF[:][3]/np.sum(jointPMF[:][3]) print(conditionalPMF) . Conditional PMF P(X=x|Y=3) [0.64 0.32 0.04 0. 0. 0. ] . . Definition. (Independence of discrete random variables). The random variables $X$ and $Y$ are said to be independent, if for all $x,y in mathbb{R}$, we have: . $$F_{X,Y}(x,y) = F_X(x) cdot F_Y(y)$$ . If $X$ and $Y$ are discrete, this is equivalent to the condition: . $$P(X = x, Y =y) = P(X = x) cdot P(Y = y)$$ . $ forall x,y in mathbb{R}$ and it is also equivalent to the condition: . $$P(Y = y|X = x) = P(Y = y)$$ . $ forall x,y in mathbb{R}$ such that $P(X = x) &gt; 0$. . . Using the terminology from this post, the definition says that for independent random variables, the joint CDF factors into the product of the marginal CDFs, or that the joint PMF factors into the product of the marginal PMFs. Remember that in general, marginal distributions do not determine the joint distribution: this is the reason why we wanted to study joint distributions in the first place! But, in the special case of independence, the marginal distributions are all we need in order to specify the joint distribution; we can get the joint PMF by multiplying the marginal PMFs. . Another way of looking at independence is that all the conditional PMFs are the same as the marginal PMF. That is, starting with the marginal PMF of $Y$, no updating is necessary when we condition on the event $ {X = x_j }$, regardless what $x_j$ is. . We&#39;ll do one more example of discrete joint distribution to round out this section. We&#39;ve named it the chicken-egg story; in it, we use wishful thinking to find a joint PMF, and our efforts land us a surprising independence result .",
            "url": "https://quantophile.github.io/mathematical-finance/probability-theory/2022/03/06/Joint-Distributions.html",
            "relUrl": "/probability-theory/2022/03/06/Joint-Distributions.html",
            "date": " • Mar 6, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Discrete Random Variables.",
            "content": "According to the definition given in calculus textbooks, the quantity $y$ is called a function of the real number $x$, if to every $x$ there corresponds a value $y$. This definition can be extended to cases where the independent variable is not a real number. Thus, we call the distance a function of a pair of points; the perimeter of a triangle is a function defined on the set of triangles; a sequence $(a_n)$ is a function for all positive integers; the binomial coefficient ${x choose k}$ is a function defined for pairs of numbers $(x,k)$ of which the second is a non-negative integer. In the same sense, we can say the the number $S_n$ of successes in $n$ Bernoulli trails is a function defined on the same space; to each of the $2^n$ points in this space, there corresponds a number $S_n$. . . Definition (Random Variable). A function defined on a sample space is called a random variable. . . Typical random variables are the number of aces in a hand at bridge, the number of successes in $n$ Bernoulli trials, the waiting time for the $r$th success etc. In each case, there is unique rule which associates a number $X$ with any sample point $ omega$. The classical theory of probability was devoted mainly to a study of gambler&#39;s gain, which is again a random variable; in fact every random variable can be interpreted as the gain of a real or imaginary gambler in a suitable game. The position of a particle under diffusion, the energy, temperature of physical systems are random variables, but they are defined in non-discrete sample spaces, and their study is therefore deferred. In the case of a discrete sample space, we can actually tabulate any random variable $X$ by enumerating in some order all points of the space and associating with each the corresponding value of $X$. . Let $X$ be a random variable and let $x_1,x_2, ldots$ be the values which it assumes; in most of what follows the $x_j$ will be integers. The aggregate of all sample points on which $X$ assumes the fixed value $x_j$ forms the event $X = x_j$; its probability is denoted by $P {X = x_j }$. . The function . $$ begin{aligned} P(X=x_j) = f(x_j) quad (j=1,2, ldots) tag{1} end{aligned} $$ . is called the probability mass function (PMF) of the random varibale $X$. Clearly, . $$ begin{aligned} f(x_j) geq 0, quad sum f(x_j) = 1 tag{2} end{aligned} $$ . With this terminology we can say that in Bernoulli trials, the number of successes $S_n$ is a random variable with the probability mass function: . $$ begin{aligned} P(X=k) = {n choose k}p^k q^{n-k} tag{3} end{aligned} $$ . whereas the number of trials up to and including the first success is a random variable with the PMF: . $$ begin{aligned} P(X=k) = q^{k-1}p tag{4} end{aligned} $$ . using Plots using Distributions plotlyjs() N = 20 function binomial_pmf(k,n,p) return binomial(n,k)*(p^k)*(1-p)^(n-k) end plot([k for k in 0:N],[binomial_pmf(k,N,0.5) for k in 0:N], line=:stem, marker=:circle, c=:blue, xlabel=&quot;x&quot;, ylabel=&quot;Probability&quot;, label=&quot;Binomial PMF&quot;) . &lt;!DOCTYPE html&gt; Plots.jl . . using Plots using Distributions plotlyjs() N = 20 function first_success_pmf(k,p) return (1-p)^(k-1)*p end plot([k for k in 1:N],[first_success_pmf(k,0.5) for k in 1:N], line=:stem, marker=:circle, c=:blue, xlabel=&quot;x&quot;, ylabel=&quot;Probability&quot;, label=&quot;First success PMF&quot;) . &lt;!DOCTYPE html&gt; Plots.jl . . Consider now two random variables $X$ and $Y$ defined on the same spample space, and denote the values which they assume respectively by $x_1,x_2, ldots$ and $y_1,y_2, ldots$; let the corresponding probability mass functions be $ {f_X(x_j) }$ and $ {f_Y(y_k) }$. The aggregate of the sample points points in which the two conditions $X=x_j$ and $Y=y_k$ are satisfied forms an event whose probability will be denote by $ {P(X=x_j, Y=y_k }$. The function . $$ begin{aligned} P {X=x_j,Y=y_k } = f_{X,Y}(x_j,y_k) tag{5} end{aligned} $$ . is called the joint PMF of $X$ and $Y$. It is best exhibited in the form a double entry table. Clearly, . $$ begin{aligned} f_{X,Y}(x_j,y_k) geq 0, quad sum_{j,k}f_{X,Y}(x_j,y_k) = 1 tag{6} end{aligned} $$ . Moreover, for every fixed $j$, . $$ begin{aligned} f_{X,Y}(x_j,y_1) + f_{X,Y}(x_j,y_2) + f_{X,Y}(x_j,y_3) + ldots &amp;= P { X=x_j } = f_X(x_j) tag{7} end{aligned} $$ . and for every fixed $k$, . $$ begin{aligned} f_{X,Y}(x_1,y_k) + f_{X,Y}(x_2,y_k) + f_{X,Y}(x_3,y_k)+ ldots &amp;= P {Y=y_k } = f_Y(y_k) tag{8} end{aligned} $$ . In other words, by adding the probabilities in individual rows and columns, we obtain the probability distributions of $X$ and $Y$. They may be exhibited as shown in the table below and are then called marginal PMFs. The adjective marginal refers to the outer appearance in the double-entry table and is also used for stylistic clarity when the joint PMF of the two random variables and also their individual (marginal) PMFs appear in the same context. Strictly speaking, the adjective marginal is redundant. . The notion of joint PMF carries over a vector of random variables. . Dice. In $n$ throws of an ideal die, let $X_1$, $X_2$, $X_3$ respectively denote the number of ones, twos and threes. The probability $P(X_1 = k_1,X_2=k_2,X_3 = k_3)$ that the $n$ throws result in $k_1$ ones, $k_2$ twos and $k_3$ threes and $n-k_1 - k_2 - k_3$ other faces is given by the multinomial distribution, with $p_1 = p_2 = p_3 = frac{1}{6}$ and $p_4 = frac{1}{2}$, that is . $$ begin{aligned} P(X_1 = k_1,X_2=k_2,X_3 = k_3) &amp;= frac{n!}{k_1! k_2! k_3! (n-k_1-k_2-k_3)!} left( frac{1}{6} right)^{k_1 + k_2 + k_3} left( frac{1}{2} right)^{n-k_1-k_2-k_3} tag{9} end{aligned} $$ . This is the joint PMF of $X_1,X_2,X_3$. Keeping $k_1,k_2$ fixed,and summing (9) over the possible values $k_3=0,1, ldots,n-k_1-k_2$, we get using the binomial theorem, . $$ begin{aligned} &amp; sum_{k_3} P(X_1 = k_1,X_2=k_2,X_3 = k_3) =&amp; sum_{k_3} frac{n!}{k_1!k_2!k_3!(n-k_1-k_2-k_3)!} left( frac{1}{6} right)^{k_1} left( frac{1}{6} right)^{k_2} left( frac{1}{6} right)^{k_3} left( frac{3}{6} right)^{n-k_1-k_2-k_3} =&amp; sum_{k_3} frac{(n-k_3)!}{k_1!k_2!(n-k_1-k_2-k_3)!} left( frac{1}{5} right)^{k_1} left( frac{1}{5} right)^{k_2} left( frac{3}{5} right)^{n-k_1-k_2-k_3} cdot frac{n!}{k_3!(n-k_3)!} left( frac{1}{6} right)^{k_3} left( frac{5}{6} right)^{n-k_3} =&amp; sum_{k_3}P(X_1=k_1,X_2=k_2|X_3=k_3) cdot P(X_3=k_3) =&amp; P(X_1=k_1,X_2=k_2) quad { text{Law of total probability} } tag{10} end{aligned} $$ . This is the joint PMF of $(X_1,X_2)$ which now appears as the marginal of the triple distribution of $X_1,X_2,X_3$. Needless to say that (10) could have been obtained directly from the multinomial distribution. . Sampling. Let a population of $n$ elements be divided into three classes of respective sizes $n_1 = np_1$ $n_2 = np_2$ and $n_3 = np_3$ (where $p_1 + p_2 + p_3 = 1$). Suppose that a random sample of size $r$ is drawn and denote by $X_1$ nd $X_2$ the numbers of representatives of the first and second class in the sample. If the sample is with replacement, $P {X_1 = k_1, X_2 = k_2 }$ is given by the multinomial distribution: . $$ begin{aligned} P(X_1 = k_1, X_2 = k_2) = frac{r!}{k_1! k_2! (r - k_1 - k_2)!}p_1^{k_1} p_2^{k_2} p_3^{r - k_1 - k_2} tag{11} end{aligned} $$ . If the sampling is without replacement, then we can show that $P(X_1 = k_1,X_2=k_2)$ is given by the double hypergeometric distribution and $X_1$ has the simple hypergeometric distribution. . We recapitulate in the formal: . . Definition. A random variable $X$ is a function on a given sample space $ Omega$, that is an assignment of a real number $X( omega)=x$ to each sample point $ omega$. The probability mass function of $X$ is the function defined by, . $$ begin{aligned} P(X = x_j) = f(x_j) tag{12} end{aligned} $$ . If two random variables $X$ and $Y$ are defined on the same sample space, their joint PMF is given by the function $p$ where, . $$ begin{aligned} P {X = x_j, Y = y_k } = p(x_j,y_k) tag{13} end{aligned} $$ . $p$ assigns probabilities to all combinations $(x_j,y_k)$ of values assumed by $X$ and $Y$. This notion carries over, in an obvious manner, to any finite set of variables $X,Y, ldots, W$ defined on the same sample space. . . Independence of random variables. . Just as we had the notion of the independence of events, we can define the independence of random variables. Intuitively, if two random variables are independent, then the value of $X$ gives no information about the value of $Y$ and vice versa. The definition formalizes this idea: . . Definition. (Independence of random variables). The random variables $X$ and $Y$ are said to be independent if . $$ begin{aligned} P(X leq x, Y leq y) = P(X leq x) cdot P(Y leq y) tag{14} end{aligned} $$ . for all $x,y in mathbf{R}$. In the discrete case, this is equivalent to the condition . $$ begin{aligned} P(X = x,Y = y) = P(X = x)P(Y = y) tag{15} end{aligned} $$ . for all combinations $(x,y)$ of values assumed by $X$ and $Y$. . . The definition for more than two random variables is analogous. . . Definition. The random variables $X_1,X_2, ldots,X_n$ are independent if . $$ begin{aligned} P(X leq x_1, ldots,X_n leq x_n) = P(X leq x_1) cdot P(X leq x_2) cdots P(X leq x_n) tag{16} end{aligned} $$ . for all $x_1,x_2, ldots,x_n in mathbf{R}$. . . For infinitely many random variables, we say that they are independent if every finite subset of the random variables is independent. . Comparing this to the criteria for the independent of $n$ events, it may seem strange that the independence of $X_1,X_2, ldots,X_n$ just requires one equality, whereas for events we needed to verify pairwise independence for $n choose 2$ pairs, three-way independence for all $n choose 3$ triplets and so on. However, upon closer examination of the definition, we see that the independence of random variables requires the equality to hold for all possible $x_1, ldots,x_n$ - infinitely many conditions! If we can find even a single list of values $x_1,x_2, ldots,x_n$ for which the above equality fails to hold, then $X_1, ldots,X_n$ are not independent. . Example. In a roll of two fair dice, if $X$ is the number on the first die and $Y$ is the number on the second die, then $X+Y$ is not independent of $X-Y$ since . $$ begin{aligned} P { X + Y = 12, X - Y = 1 } = 0 end{aligned} $$ . whereas, . $$ begin{aligned} P { X + Y = 12 } cdot P {X - Y = 1 } = frac{1}{36} cdot frac{5}{36} end{aligned} $$ . Knowing that the total sum is $12$ tells us the difference must be zero, so the random variables provide information about each other. . If $X$ and $Y$ are independent then it is also true e.g. that $X^2$ is independent of $Y^4$, since if $X^2$ provided information about $Y^4$, then $X$ would give information about $Y$ (using $X^2$ and $Y^4$ as intermediaries: $X$ determines $X^2$, which would give information about $Y^4$, which in turn determines $Y$). More generally, we have the following result. . . Theorem. (Functions of independent random variables). If $X$ and $Y$ are independent random variables, then any function of $X$ is independent of any function of $Y$. . . . Definition (Independent and Identically distributed random variables). We often work with random variables that are independent and have the same distribution. We call such random variables independent and identically distributed. . . Independent and identically distributed are two often confused but completely different concepts. Random variables are independent if they provide no information about each other; they are identically distributed if they have the same PMF (or equivalently the same CDF). Whether two random variables are independent has nothing to do with whether they have the same distribution. We can have random variables that are: . Independent and identically distributed. Let $X$ be the result of a die roll, and let $Y$ be the result of a second, independent die roll. Then $X$ and $Y$ are i.i.d. | Independent and not identically distributed. Let $X$ be the result of a die roll, and let $Y$ be the closing price of the Dow Jones(a stock market index) a month from now. Then, $X$ and $Y$ provide no information about each other (one would fervently hope), and $X$ and $Y$ do not have the same distribution. | Dependent and identically distributed. Let $X$ be the number of heads in $n$ independent coin tosses, and let $Y$ be the number of tails in those same $n$ tosses. Then $X$ and $Y$ are both distributed $Bin(n, frac{1}{2})$, but they are highly dependent: if we know $X$, we know $Y$ perfectly. | Dependent and not identically distributed. Let $X$ be the indicator of whether the majority party retains control of the House of the representatives in the U.S. after the next election, and let $Y$ be the average favorability rating of the majority party in polls taken within a month of the the election. Then, $X$ and $Y$ are dependent, and $X$ and $Y$ do not have the same distribution. | . By taking a sum of i.i.d. Bernoulli random variables, we can write down the story of the Binomial distribution in algebraic form. . . Theorem. If $X sim Bin(n,p)$, viewed as the number of successes in $n$ independent Bernoulli trials with probability of success $p$, then we can write {% raw %} $$ begin{aligned} X = X_1 + X_2 + ldots + X_n end{aligned} $$ . where $X_i$ are i.i.d $Bernoulli(p)$. . . Proof. Let $X_i = 1$ if the $i$th trial was a success, and $0$ if the $i$th trial was a failure. Its as though we have a person assigned to each trial, and we ask each person to raise their hand if their trial was a success. If we count the number of raised hands (which is the same as adding up the $X_i$), we get the total number of successes. . An important fact about the Binomial distribution is that the sum of independent Binomial random variables with the same success probability is also Binomial. . . Theorem. If $X sim Bin(n,p), Y sim Bin(m,p)$ and $X$ is independent of $Y$, then $X+Y sim Bin(n+m,p)$. . . Proof. . We present three proofs, since each illustrates a useful technique. . (i) We can directly find the PMF of $X+Y$ by conditioning on $X$ (or $Y$, whichever we prefer) and using the law of total probability: . $$ begin{aligned} P(X + Y = k) &amp;= sum_{j=0}^{k}P(X + Y = k| X = j) cdot P(X = j) &amp;= sum_{j=0}^{k}P(Y=k-j) cdot P(X=j) &amp;= sum_{j=0}^{k}{m choose {k - j}} p^{k - j} q^{m - (k-j)} {n choose j} p^{j} q^{n - j} &amp;= p^k q^{n + m - k} sum_{j=0}^{k} {m choose {k - j}}{n choose j} &amp;= p^k q^{n + m - k}{{n + m} choose k} quad { text{ Vandermonde&#39;s identity } } end{aligned} $$ . (ii) Representation: A much simpler proof is to represent both $X$ and $Y$ as the sum of i.i.d $Bern(p)$ random variables : $X = X_1 + X_2 + ldots + X_n$ and $Y = Y_1 + Y_2 + ldots + Y_m$ where the $X_i$ and the $Y_j$ are all i.i.d $Bern(p)$. Then, the sum $X+Y$, by the previous theorem (the story of the binomial) is $Bin(n+m,p)$. . (iii) By the story of the binomial, $X$ is the number of successes in $n$ independent trials and $Y$ is the number of successes in $m$ additional independent trials, all with the same success probability, so $X+Y$ is the total number of successes in the $n+m$ trials, which is the story of the $Bin(n + m,p)$ distribution. . Of course, if we have a definition for independence of random variables, we should have an analogous definition for the conditional independence of random variables. . . Definition(Conditional Independence). Random variables $X$ and $Y$ are conditionally independent given a random variable $Z$ if for all $x,y in mathbf{R}$ and all the $z$ in the support of $Z$, . $$ begin{aligned} P { X leq x, Y leq y|Z = z } = P { X leq x | Z = z } cdot P { Y leq y | Z = z } tag{17} end{aligned} $$ . . For discrete random variables, an equivalent definition is to require . $$ begin{aligned} P { X = x, Y = y| Z = z } = P { X = x| Z = z } cdot P { Y = y|Z = z } end{aligned} $$ . . Definition (Conditional PMF). The conditional probability of the event $ {Y = y_k }$, given that $ {X = x_j }$ is given by . $$ begin{aligned} P { Y = y_k | X = x_j } = frac{P {Y = y_k, X = x_j }}{P {X = x_j }} end{aligned} $$ . But, the probability of the event $ {Y = y_k, X = x_j }$ is given by the joint distribution of $X,Y$, $f_{X,Y}(x_j,y_k)$ and the probability of the event $ { X = x_j }$ is given by the marginal distribution of $X$, $f_X(x_j)$. Consequently, the conditional PMF of $Y$ given $X$, written $f_{Y|X=x_j}(y_k)$ is: . begin{aligned} P { Y = y_k | X = x_j } = f_{Y|X=x_j}(y_k) = frac{f_{X,Y}(x_j,y_k)}{f_X(x_j)} tag{18} end{aligned} . . Example. Suppose a dice is rolled $N=5$ times. Let $X$ be the number of ones and $Y$ be the number of twos. The pair $(X,Y)$ follows the $Multinomial(5, frac{1}{6}, frac{1}{6})$ distribution. The joint PMF $f_{X,Y}(x_j,y_k)$ is given by, . N = 5 function multinomial_pmf2(n::Integer,k₁::Integer,k₂::Integer,p₁,p₂) return (factorial(n)/(factorial(k₁)*factorial(k₂)*factorial(n-k₁-k₂)))*(p₁^k₁)*(p₂^k₂)*(1-p₁-p₂)^(n-k₁-k₂) end f(x,y) = multinomial_pmf2(N,x,y,(1/6),(1/6)) f_XY = Matrix{Float64}(undef, N+1, N+1) for x in 0:N for y in 0:N-x f_XY[x+1,y+1] = f(x,y) end end f_XY . 6×6 Array{Float64,2}: 0.131687 0.164609 0.0823045 … 0.00257202 0.000128601 0.164609 0.164609 0.0617284 0.000643004 7.35247e-316 0.0823045 0.0617284 0.0154321 7.35247e-316 7.33184e-316 0.0205761 0.0102881 0.00128601 7.33184e-316 7.33184e-316 0.00257202 0.000643004 7.33184e-316 7.33184e-316 7.33184e-316 0.000128601 7.35247e-316 7.33184e-316 … 7.33184e-316 3.891e-315 . The marginal distributions of $X$ and $Y$ are given by, $ sum_{y_k} f_{X,Y}(x_j,y_k)$, $ sum_{x_j} f_{X,Y}(x_j,y_k)$. . f_X = Array{Float64}(undef,N+1) for x in 0:N for y in 0:N f_X[x+1] = f_X[x+1] + f_XY[x+1,y+1] end end f_Y = Array{Float64}(undef,N+1) for y in 0:N for x in 0:N f_Y[y+1] = f_Y[y+1] + f_XY[x+1,y+1] end end . f_X . 6-element Array{Float64,1}: 0.401877572016461 0.40187757201646096 0.16075102880658437 0.03215020576131688 0.0032150205761316874 0.00012860082304526745 . f_Y . 6-element Array{Float64,1}: 0.401877572016461 0.40187757201646096 0.16075102880658437 0.03215020576131688 0.0032150205761316874 0.00012860082304526745 . The conditional PMF of $Y$ for given $X=3$ is given by, . f_XY[3,:]/f_Y[3] . 6-element Array{Float64,1}: 0.5120000000000001 0.384 0.09599999999999999 0.008 4.573823507e-315 4.560993067e-315 . Indepdence of random variables does not imply conditional independence and vice versa. First let us show why independence does not imply conditional independence. . Example.(Matching Pennies) Consider the simple game called matching pennies. Each of the two players, A and B has a fair penny. They flip their pennies independently. If the pennies match, A wins; otherwise B wins. Let $X$ be 1, if A&#39;s penny lands heads and $-1$ otherwise, and define $Y$ similarly(the random variables $X$ and $Y$ are called random signs). . Let $Z = XY$, which is $1$ if $A$ wins and $-1$ if $B$ wins. Then, $X$ and $Y$ are unconditionally independent, but given $Z = 1$, we know that $X = Y$ (the pennies match). So, $X$ and $Y$ are conditionally dependent given $Z$. . Next, lets see why conditional independence does not imply independence. . Example. (Mystery opponent) Suppose that you are going to play two games of tennis against one of the two identical twins. Against one of the two twins, you are evenly matched, and against the other you have $ frac{3}{4}$ chance of winning. Suppose that you can&#39;t tell which twin you are playing against until after the two games. Let $Z$ be the indicator of playing against the twin with whom you&#39;re evenly matched, and let $X$ and $Y$ be the indicators of victory in the first and second games, respectively. . Conditional on $Z = 1$, $X$ and $Y$ are i.i.d. Bern(1/2), and conditional on $Z = 0$, $X$ and $Y$ are i.i.d. Bern(3/4). So, $X$ and $Y$ are conditionally independent given $Z$. Unconditionally, $X$ and $Y$ are dependent because observing $X=1$ makes it more likely that we are playing the twin who is worse. That is, . $$ begin{aligned} P { Y = 1 | X = 1 } &gt; P { Y = 1 } end{aligned} $$ . Past games give us information which helps us infer who our opponent is, which in turn helps us predict future games! &quot; . Example. (Bernoulli trials with variable probabilities) Consider $n$ independent trials, each of which has only two possible outcomes, $S$ and $F$. The probability of $S$ at the $k$th trial is $p_k$, that of $F$ is $q_k = 1 - p_k$. If $p_k = p$, this scheme reduces to Bernoulli trials. The simplest way of describing it is to attribute the values $1$ and $0$ to $S$ and $F$. The model is then completely described by saying that we have $n$ mutually independent random variables $X_k$ with $P { X_k = 1 } = p_k$, $P { X_k = 0 } = q_k$. These are not identically distributed. . It is clear that the same distribution can occur in conjunction with different sample spaces. If we say that the random vaiable $X$ assumes the values $0$ and $1$ with probabilities $ frac{1}{2}$, then we refer tacitly to a sample space consisting of two points $0$ and $1$. However, the variable $X$ might have been dfined by stipulating that it equals $0$ or $1$, according as the tenth tossing of a coin produces heads or tails; in this case $X$ is defined in a sample space of sequences $(HHT ldots)$, and this sample space has $2^{10}$ points. . In principle, it is possible to restrict the theory of probability to sample spaces defined in terms of probability distributions of random variables. This procedre avoids references to abstract sample spaces and also to terms like &quot;trials&quot; and &quot;outcomes of experiments&quot;. The reduction of probability theory to random variables is a short cut to the use of analysis and simplifies the theory in many ways. However, it also has the drawback of obscuring the probability background. The notion of a random variable remains vague as &quot;something that takes different values with different probabilities.&quot; But random variables are ordinary functions, and this notion is by no means peculiar to probability theory. . Example. Let $X$ be a random variable with possible values $x_1,x_2, ldots$ and corresponding probabilities $f_X(x_1),f_X(x_2), ldots$. If it helps the reader&#39;s imagination, he may always construct a conceptual experiment leading to $X$. For example, subdivide a roulette wheel into arcs $l_1,l_2, ldots$ whose lengths are $f(x_1):f(x_2): ldots$. Imagine a gambler receiving the (positive or negative) amount $x_j$ if the roulette comes to rests at a point of $l_j$. Then, $X$ is the gambler&#39;s gain. In $n$ trials, the gains are assumed to be $n$ independent random variables with the common distribution $ {f_X(x_j) }$. To obtain two varibles with a given joint distribution $ { f_{X,Y}(x_j,y_k) }$, let an arc $l_{j,k}$ correspond to each combination $(x_j,y_k)$, such that their lengths are in the proportion $f_{X,Y}(x_1,y_1): ldots:f_{X,Y}(x_1,y_n):f_{X,Y}(x_2,y_1): ldots$. Think of the two gamblers receiving the amounts $x_j$ and $y_k$ respectively. Then, $X$ is the first gambler&#39;s gain, $Y$ is the second gambler&#39;s gain. . If $X_1,X_2,X_3, ldots$ are random variables defined on the same sample space, then any function $F(X_1,X_2,X_3, ldots)$ is again a random variable. Its distribution can be obtained by simply collecting the terms that correspond to combinations of $(X_1,X_2,X_3, ldots)$ giving the same value of $F(X_1,X_2,X_3, ldots)$. . Expectation. . To achieve reasonable simplicity it is often necessary to describe probability distributions rather summarily by a few typical values. An example is provided by the median which was used above in connection with the waiting times. The median $x_m$ of the distribution is that value assumed by $X$ for which $P {X leq x_m } leq frac{1}{2}$ and also $P {X geq x_m } leq frac{1}{2}$. In other words, $x_m$ is chosen so that the probabilities of $X$ exceeding or falling short of $x_m$ are as close to $ frac{1}{2}$ as possible. . However, among the typical values the expectation or the mean is by far the most important. It lend itself best to analytical manipulations, and it is preferred by statisticians because of a property known as sampling stability. It&#39;s definition follows the customary notion of an average. If in a certain population $n_k$ families have exactly $k$ children, the total number of familieis is $n = n_0 + n_1 + n_2 + ldots$ and the total number of children $m = n_1 + 2n_2 + 3n_3 + ldots$ The average number of children per family is $m/n$. The analogy between probabilities and frequencies suggests the following: . . Definition (Expectation) Let $X$ be a random variable assuming the values $x_1,x_2, ldots$ with corresponding probabilities $f_X(x_1),f_X(x_2), ldots$. The mean or expected value of $X$ is defined by: . $$ begin{aligned} E(X) = sum_{k} {x_k}f_X(x_k) tag{19} end{aligned} $$ . provided that the series converges absolutely. In this case, we say that $X$ has finite expectation. If $ sum |x_k|f(x_k)$ diverges, then we say that $X$ has no finite expectation. . . It goes without saying that the most common random variables have finite expectations; otherwise the concept would be impractical. However, variables without finite expectations occur in connection with important recurrence problems in physics. The terms mean, average and mathematical expectation are synonymous. We also speak of the mean of a distribution instead of referring to a corresponding random variable. The notation $E(X)$ is generally accepted in mathematics and statistics. In pyhysics, $ overline{X}$, $&lt;X&gt;$ are common substitutes for $E(X)$. . Linearity of Expectation. . The most important property of expectation is linearity: the expected value of a sum of random variables is the sum of the expected values. . . Theorem. (Linearity of expectation). For any random variables $X,Y$ and any constant $c$, . $$ begin{aligned} E(X + Y) &amp;= E(X) + E(Y) tag{20} E(cX) &amp;= cE(X) tag{21} end{aligned} $$ . . The second equation says that we can take out constant factors from an expectation; this is both intuitively reasonable and easily verified from the definition. Consider: . $$ begin{aligned} E(cX) &amp;= sum_{x_k} (cx_k)(f_{cX}(cx_k)) end{aligned} $$ . For a discrete PMF, $f_{cX}(cx_k) = P(cX = cx_k) = P(X = x_k) = f_X(x_k)$. . Consequently, . $$ begin{aligned} E(cX) &amp;= sum_{x_k} cx_kf_{X}(x_k) &amp;= c sum_{x_k}x_k f_X(x_k) &amp;= cE(X) end{aligned} $$The first equation, $E(X+Y) = E(X) + E(Y)$, also seems reasonable when $X$ and $Y$ are independent. What may be surprising is that it holds, even if $X$ and $Y$ are dependent! To build intuition for this, consider the extreme case where $X$ always equals $Y$. Then, $X + Y = 2X$, both sides of $E(X + Y)$ are equal to $2E(X)$, so linearity still holds even in the most extreme case of dependence. . Linearity is true for all random variables, not just discrete random variables, but in this chapter we prove it only for discrete random variables. Before proving linearity, it is worthwhile to recall some basic facts about averages. If we have a list of numbers, say $(1,1,1,1,1,3,3,5)$, we can calculate their mean by adding all the values and dividing the length of the list, so that each element of the list gets a weight of $ frac{1}{8}$: . begin{align*} frac{1}{8}(1 + 1 + 1 + 1 + 1 + 3 + 3 + 5) = 2 end{align*}But another way to calculate the mean is to group together all the $1$&#39;s, all the $3$&#39;s and all the $5$&#39;s, and take a weighted average, giving appropriate weights to $1$&#39;s, $3$&#39;s and $5$&#39;s. . $$ begin{aligned} frac{5}{8} cdot 1 + frac{2}{8} cdot 3 + frac{1}{8} cdot 5 = 2 end{aligned} $$ . That insight - that averages can be calculated in two ways, ungrouped or grouped - is all that is needed to prove linearity! Recall, that the random variable $X$ is a function that assigns a real number in $ mathbf{R}$ to every sample point $ omega$ in the sample space $ Omega$. The random variable may assign the same value to multiple outcomes. So, $P(X = x_k) = P { omega in Omega:X( omega) = x_k }$. Therefore, we can write: . $$ begin{aligned} E(X) &amp;= sum_{x_k} x_k P(X = x_k ) &amp;= sum_{x_k} x_k P { omega in Omega : X( omega) = x_k } &amp;= sum_{ omega in Omega} X( omega) P { omega } end{aligned} $$ . We switched from iterating over all $x_k$ to iterating over all $ omega$&#39;s. This corresponds to the ungrouped way of taking averages. The advantage of this definition is, that it breaks down the sample space into the smallest possible units, so we are now using the same weights $P { omega }$ for every random variable defined on this sample space. Now, if $Y$ is a random variable defined over the same sample space, then, . $$ begin{aligned} E(Y) = sum_{ omega in Omega} Y( omega) P { omega } end{aligned} $$ . Consequently, we have: . $$ begin{aligned} E(X + Y) &amp;= sum_{ omega in Omega} [X( omega) + Y( omega)] P { omega } &amp;= sum_{ omega in Omega} X( omega) P { omega } + sum_{ omega in Omega} Y( omega) P { omega } &amp;= E(X) + E(Y) end{aligned} $$ . Another intuition for the linearity of expectation is via the concept of simulation. If we simulate a random experiment a large number of times, say $N$, then the frequency/histogram of the simulated values of $X$ will look very much like the true PMF of $X$. In particular, the arithmetic mean of the simulated values will be very close to the true value of $E(X)$ (the precise nature of this convergence is described by the law of large numbers). . Consider a random experiment e.g. tossing a fair coin $10$ times. And let $X$ be the number of heads and $Y$ be the number of double tails $TT$, observed. The experiment is performed a large number of times $N$. And we write down the values of $X$ and $Y$ each time. For each repetition of the experiment, we obtain an $X$ value and a $Y$ value, and (by adding them) an $X+Y$ value. . Now, we could take the arithmetic mean of the values of $X+Y$, which by the law of large numbers is very close to $E(X+Y)$. We could also take the arithmetic mean of the values of $X$ and values $Y$ and sum them up, which by the law of large numbers is close to $E(X) + E(Y)$. . Linearity of expectation thus emerges as a simple fact about arithmetic (we&#39;re just adding numbers in two different orders)! Notice that nowhere in our argument, did we rely on the fact that $X$ and $Y$ are independent. . Example. (Binomial Expectation) Let $X sim Bin(n,p)$. Then, . $$ begin{aligned} E(X) &amp;= sum_{k=0}^{n}kf_X(k) &amp;= sum_{k=0}^{n}k {n choose k}p^k q^{n-k} &amp;= sum_{k=1}^{n}k {n choose k}p^k q^{n-k} quad { text{ when }k = 0, text{ the first term vanishes } } end{aligned} $$ . Now, . $$ begin{aligned} k{n choose k} &amp;= k cdot frac{n!}{k!(n-k)!} &amp;= n frac{(n-1)!}{(k-1)!(n-k)!} &amp;= n frac{(n-1)!}{(k-1)!((n-1)-(k-1))!} &amp;= n {{n - 1} choose {k - 1}} end{aligned} $$ . Thus, . $$ begin{aligned} E(X) &amp;= sum_{k=1}^{n}k {n choose k}p^k q^{n-k} &amp;= n sum_{k=1}^{n}{{n - 1} choose {k - 1}}p^k q^{n-k} &amp;= np sum_{k - 1 = 0}^{n - 1} {{n - 1} choose {k - 1}}p^{k-1} q^{(n-1) - (k-1)} &amp;= np sum_{j = 0}^{n - 1} {{n - 1} choose j}p^{j} q^{(n-1) - j} &amp;= np (p + q)^{n-1} &amp;= np end{aligned} $$ . Therefore, if $X$ is a $Bin(n,p)$ random variable, $E(X) = np$. . This result is much easier to prove using linearity of expectations. Let&#39;s write $X$ as the sum of $n$ i.i.d $Bern(p)$ random variables. . $$ begin{aligned} X = I_1 + I_2 + ldots + I_n end{aligned} $$ . The expectation of a Bernoulli random variable is, $E(I_j) = p cdot 1 + q cdot 0 = p$. So, we have: . $$ begin{aligned} E(X) &amp;= E(I_1 + I_2 + ldots + I_n) &amp;= E(I_1) + E(I_2) + ldots + E(I_n) quad { text{Linearity of expectations} } &amp;= underbrace{p + p + ldots + p}_{n text{ times }} &amp;= np end{aligned} $$ . Example. (Hypergeometric Expectation) Let $X sim HGeom(w,b,n)$ be the number of white balls in a sample of size of $n$ drawn without replacement from an urn containing $w$ white balls and $b$ black balls. As in the binomial case, we can write $X$ as a sum of Bernoulli random variables. . $$ begin{aligned} X = I_1 + I_2 + ldots + I_n end{aligned} $$ . Consider the $j$th draw. If we have no knowledge of the preceding $(j-1)$ draws, the unconditional probability of drawing a white ball in the $j$th trial is $P {I_j = 1 } = w/(w + b)$, since the white ball is equally likely to be any of the balls. Consequently, $E(I_j) = w/(w + b)$. . Therefore, . $$ begin{aligned} EX &amp;= EI_1 + EI_2 + ldots + EI_n &amp;= frac{nw}{w + b} end{aligned} $$ . Unlike in the Binomial case, the $I_j$ are not independent, since the sampling is without replacement: given that a ball in the sample is white, there is a lower chance that another ball in the sample is white. However, linearity still holds for dependent random variables! . Example. (Geometric Expectation) . Let $X sim Geom(p)$ be the number of trials until the first success. The probability that $k$ failures precede the first success is $P {X = k } = q^{k} p$. The expected number of trials until the first success is given by, . $$ begin{aligned} E(X) &amp;= sum_{k=1}^{ infty}kq^{k}p &amp;=pq sum_{k=1}^{ infty}kq^{k-1} end{aligned} $$ . Now, $$ begin{aligned} frac{1}{1-q} = 1 + q + q^2 + q^3 + ldots end{aligned} $$ . Differentiating on both side with respect to $q$, . $$ begin{aligned} frac{1}{(1-q)^2} = 1 + 2q + 3q^2 + 4q^3 + ldots = sum_{k=1}^{ infty} kq^{k-1} end{aligned} $$ . Hence, . $$ begin{aligned} E(X) &amp;= pq sum_{k=1}^{ infty}kq^{k-1} &amp;= frac{pq}{(1-q)^2} = frac{pq}{p^2} &amp;= frac{q}{p} end{aligned} $$ . In a sequence of independent Bernoulli trials with success probability $p$, if $X$ is the number of failures before the $r$th success, then $X$ is said to have the negative binomial distribution with parameters $r,p$, denoted $NBin(r,p)$. . Both the Binomial and the Negative Binomial distributions are based on independent Bernoulli trials; they differ in the stopping rule and what they are counting. The Binomial counts the number of successes in a fixed number of trials. The Negative Binomial counts the number of failures until a fixed number of successes. . We know that, if $X sim NBin(r,p)$, the PMF of $X$ is given by, . $$ begin{aligned} P { X = k } = {{r + k - 1} choose k}p^r q^k end{aligned} $$ . where $k=0,1,2, ldots$ . . Theorem. Let $X sim NBin(r,p)$, viewed as the number of failures before the $r$th success in a sequence of independent Bernoulli trials with success probability $p$. Then, we can write $X = X_1 + X_2 + ldots + X_r$, where $X_i$ are geometric random variables. . . Proof. Let $X_1$ be the number of failures until the first success, $X_2$ be the number of failures between the first success and the second success, and in general, $X_i$ be the number of failures between the $(i-1)$st success and $i$th success. Then, $X_1 sim Geom(p)$ and similarly for all $X_i$. Furthermore, the $X_i$ are independent, because the trials are independent of each other. Adding the $X_i$, we get the total number of failures preceding the $r$th success, which is $X$. . Example.(Negative Binomial Expectation) Let $X sim NBin(r,p)$. By the previous theorem, we can write $X=X_1+X_2+ ldots+X_r$, where $X_i$ are i.i.d. $Geom(p)$. By linearity of expectations, . $$ begin{aligned} E(X) &amp;= E(X_1) + ldots + E(X_r) &amp;= underbrace{ frac{q}{p} + ldots + frac{q}{p}}_{r text{ times }} &amp;= r cdot frac{q}{p} end{aligned} $$ . Example. (Coupons) Every package of some intrinsically dull commodity includes a smal and exciting plastic object. There are diferent types of object, and each package is equally likely to contain any given type. You buy one package each day. . (a) Find the mean number of days which elapse between the acquisitions of the $j$th new type of object and the $(j+1)$th new type. . (b) Find the mean number of days which elapse before you have a full set of objects. . Solution. . This question asks about waiting times when sampling from a population with replacement. As the sample grows larger and larger, since we are sampling with replacement, the chance that newer elements enter the sample becomes rarer. . (a) Let $X_j$ be the number of days elapsed between the acquisition of the $j$th new type of object and the $(j+1)$th new type. What&#39;s the probability distribution of $X_j$? . $$ begin{aligned} P(X_j = 1) &amp;= frac{c - j}{c} = left(1 - frac{j}{c} right) P(X_j = 2) &amp;= left( frac{j}{c} right) left(1 - frac{j}{c} right) P(X_j = 3) &amp;= left( frac{j}{c} right)^2 left(1 - frac{j}{c} right) vdots end{aligned} $$ . By definition, the mathematical expectation of $X_j$ is, . $$ begin{aligned} E(X_j) &amp;= 1 cdot left(1 - frac{j}{c} right) + 2 cdot left( frac{j}{c} right) left(1 - frac{j}{c} right) + 3 cdot left( frac{j}{c} right)^2 left(1 - frac{j}{c} right) + ldots &amp;= left(1 - frac{j}{c} right) left[1 + 2 cdot left( frac{j}{c} right) + 3 cdot left( frac{j}{c} right)^2 + ldots right] &amp;= left(1 - frac{j}{c} right) cdot frac{1}{ left(1 - frac{j}{c} right)^2} &amp;= frac{1}{1 - frac{j}{c}} &amp;= frac{c}{c - j} end{aligned} $$ . (b) Let $S_r$ be the number of days which elapse until we have $r$ distinct objects in the sample. It follows that, $S_r = 1 + X_1 + X_2 + X_{r-1}$. Therefore, . $$ begin{aligned} E[S_r] &amp;= E[ 1 + X_1 + X_2 + X_{r-1}] &amp;= E[1] + E[X_1] + E[X_2] + ldots + E[X_{r-1}] quad { text{ Linearity of expectation } } &amp;= 1 + frac{c}{c - 1} + frac{c}{c - 2} + ldots + frac{c}{c - r + 1} &amp;= c left[ frac{1}{c} + frac{1}{c-1} + frac{1}{c-2} + ldots + frac{1}{c - r + 1} right] end{aligned} $$ . The mean number of days that elapse until we have a full set of objects is, . $$ begin{aligned} E[S_c] = c left[ frac{1}{c} + frac{1}{c-1} + frac{1}{c-2} + ldots + 1 right] end{aligned} $$ . Example. (An estimation problem) A bowl contains balls numbered $1$ to $N$. Let $X$ be the largest number drawn in $n$ drawings when random sampling with replacement is used. The event $X leq k$ means that each of the $n$ numbers drawn is less than or equal to $k$ and therefore the $P { X leq k } = left( frac{k}{N} right)^n$. Hence the probability distribution of $X$ is given by . $$ begin{aligned} p_k &amp;= P { X = k } &amp;= P {X leq k } - P { X leq k - 1 } &amp;= left( frac{k}{N} right)^n - left( frac{k-1}{N} right)^n end{aligned} $$ . It follows that: . $$ begin{aligned} E[X] &amp;= sum_{k = 1}^{N}k P {X = k } &amp;= N^{-n} sum_{k = 1}^{N} left[k^{n+1} - k cdot (k-1)^{n} right] &amp;= N^{-n} sum_{k = 1}^{N} left[k^{n+1} - (k-1+1) cdot (k-1)^{n} right] &amp;= N^{-n} sum_{k = 1}^{N} left[k^{n+1} - (k-1)^{n+1} - (k-1)^n right] &amp;= N^{-n} left[1 + (2^{n+1} - 1) + (3^{n+1} - 2^{n+1}) + ldots + (N^{n+1} - (N-1)^{n+1}) - sum_{k=1}^{N}(k - 1)^n right] &amp;= N^{-n} left[N^{n+1} - sum_{k=1}^{N}(k - 1)^n right] end{aligned} $$ . For large $N$, the last sum is approximately the area under the curve $y=x^n$ from $x=0$ to $x=N$, that is, $ frac{N^{n+1}}{n+1}$. It follows that for large $N$, . $$ begin{aligned} E[X] approx N - frac{N}{n+1} = frac{n}{n+1}N end{aligned} $$ . If a town has $N=1000$ cars and a sample of $n = 10$ is observed, the expected number of the highest observed license plate (assuming randomness) is about $(10/11) times 10000 = 910$. . Example. (Banach&#39;s Matchbox problem.) In the previous chapter, we found that the distribution of the number of matches $X$ left at the moment when the first box is found empty, is given by: . $$ begin{aligned} f(r) &amp;= {2N - r choose N} frac{1}{2^{2N - r}} end{aligned} $$ . Using the fact that $ sum_{r=0}^{N} f(r) = 1$, we find that: . $$ begin{aligned} N - mu &amp;= sum_{r = 0}^{N}(N-r)f(r) &amp;= sum_{r = 0}^{N}(N-r){2N - r choose N - r} frac{1}{2^{2N - r}} end{aligned} $$ . Note that, the last term of this sum is $0$. Effectively, we iterate $r$ from $0,1, ldots$ to $N-1$. . $$ begin{aligned} N - mu = sum_{r = 0}^{N-1}(N-r){2N - r choose N - r} frac{1}{2^{2N - r}} end{aligned} $$ . We know that $k {n choose k} = n {n - 1 choose k - 1}$. So, we can perform a simple operation on the binomial coefficients as follows: . $$ begin{aligned} (N-r) {2N - r choose N - r} = (2N - r) {2N - r - 1 choose N - r - 1} end{aligned} $$ . Therefore, we have: . $$ begin{aligned} N - mu &amp;= sum_{r = 0}^{N-1}(2N - r) {2N - r - 1 choose N - r - 1} frac{1}{2^{2N - r}} &amp;= sum_{r = 0}^{N-1}(2N + 1 - r - 1){2N - r - 1 choose N - r - 1} frac{1}{2^{2N - r - 1}} cdot frac{1}{2} &amp;= frac{2N + 1}{2} sum_{r = 0}^{N-1}{2N - r - 1 choose N - r - 1} frac{1}{2^{2N - r - 1}} - frac{1}{2} sum_{r = 0}^{N-1}(r+1){2N - r - 1 choose N - r - 1} frac{1}{2^{2N - r - 1}} &amp;= frac{2N+1}{2} sum_{r = 0}^{N-1}f(r+1) - frac{1}{2} sum_{r = 0}^{N-1}(r+1)f(r+1) end{aligned} $$ . The last sum is identical with the sum defining $ mu = E(X)$, and in the first sum, all the terms $f(r)$ except $f(0)$ occur and hence, the terms add to $1 - f(0)$. Thus, we get: . $$ begin{aligned} N - mu = frac{2N + 1}{2}(1 - f(0)) - frac{ mu}{2} tag{22} end{aligned} $$ . Or in other words, . $$ begin{aligned} mu= 2N - (2N + 1) left(1 - {2N choose N} frac{1}{2^{2N}} right) end{aligned} $$ . That is, . . $$ begin{aligned} mu = (2N + 1){2N choose N} frac{1}{2^{2N}} - 1 tag{23} end{aligned} $$ . . Indicator random variables and Matching. . This section contains some liht entertainment in the guise of some illustrations of the uses of indicator random variables. An indicator random variable $I_A$ for an event $A$ is defined to be $1$ if $A$ occurs and $0$ otherwise. So, $I_A$ is a Bernoulli random variable, where success is defined as &quot;$A$ occurs&quot; and failure is defined as &quot;$A$ does not occur&quot;. Some useful properties of indicator random variables are summarized below: . . Theorem. Let $A$ and $B$ be events. Then, the following properties hold: . (1) $$ begin{aligned} (I_A)^k = I_A text{ for any positive integer } k. end{aligned}$$ . (2) $$ begin{aligned} I_{A^C} = 1 - I_A end{aligned}$$ . (3) $$ begin{aligned} I_{A cap B} = I_A cdot I_B end{aligned}$$ . (4) $$ begin{aligned} I_{A cup B} = I_A + I_B - I_{A cap B} end{aligned}$$ . . Proof. . Clearly, property 1 holds, since $0^k = 0$ and $1^k = 1$. Property 2 holds, because $1 - I_A$ is $1$, if and only if $I_A = 0$ and $1 - I_A = 0$ if and only if $I_A = 1$. $I_{A cap B} = I_A cdot I_B$ follows from the laws of boolean algebra. $I_{A cap B} = 1$ if and only if both $I_A = 1$ and $I_B = 1$ and $0$ otherwise. Property 4 holds since, $A cup B = (A^C cap B^C)^C$ . So, . $$ begin{aligned} I_{A cup B} &amp;= 1 - I_{A^C cap B^C} &amp;= 1 - (1 - I_A) cdot(1 - I_B) &amp;= I_A + I_B - I_A cdot I_B &amp;= I_A + I_B - I_{A cap B} end{aligned} $$ . Since the expectation of a Bernoulli random variable is $p$, taking expectation on both sides, we get, . $$ begin{aligned} P {A cup B } = P {A } + P {B } - P {A cap B } end{aligned} $$ . Similarly, let $A_1,A_2, ldots,A_n$ be events. The event atleast one of $A_1,A_2, ldots,A_n$ occurs is given by, $A_1 cup A_2 cup ldots A_n$. If the complement of all events occur simultaneously, then none of $A_1,A_2, ldots,A_n$ occur. Thus, it holds that, . $$ begin{aligned} A_1 cup A_2 ldots A_n &amp;= (A_1^C A_2^C cdots A_n^C)^C I_{A_1 cup A_2 cup ldots A_n} &amp;= 1 - I_{A_1^C A_2^C cdots A_n^C} &amp;= 1 - (1-I_{A_1})(1 - I_{A_2}) cdots(1 - I_{A_n}) &amp;= 1 - 1 + I_{A_1} + I_{A_2} + ldots + I_{A_n} - I_{A_1}I_{A_2} - I_{A_1}I_{A_3} - ldots - I_{A_{n-1}}I_{A_n} + I_{A_1}I_{A_2}I_{A_3} + ldots end{aligned} $$ . Taking expectations on both sides, we get: . $$ begin{align*} P {A_1 cup A_2 cup ldots A_n } = sum_{i} P {A_i } - sum_{i&lt;j} P {A_i A_j } + sum_{i &lt; j &lt; k}P {A_i A_j A_k } tag{24} end{align*} $$ . This is called as the inclusion-exclusion formula. . Example. (Matching continued) We have a well-shuffled deck of $n$ cards, labeled $1$ through $n$. A card is a match if the card&#39;s position in the deck matches the card&#39;s label. Let $X$ be the number of matches; find $E(X)$. . Solution. . First let&#39;s check whether $X$ could have any of the named distributions we have studied. The Binomial and the Hypergeometric are the only two candidates since the value of $X$ must be an integer between $1$ and $n$. But, neither of these distributions are applicable, since $X$ can take the value $n-1$: if $n-1$ cards are matches, then the $n$th card must be a match card as well. So, $X$ does not follow a named distribution we have studied, but we can readily find its mean using indicator random variables. . Let $A_i$ be the event that the $i$th card is a match card; it&#39;s position in the deck matches it&#39;s label. Let $I_i$ be the indicator function of $A_i$, that is: . $$ begin{aligned} I_i = begin{cases} 1 &amp; quad text{ if the }i text{th card in the deck is a matching card } 0 &amp; quad text{ otherwise } end{cases} end{aligned} $$ . Then, . $$ begin{aligned} X = I_1 + I_2 + ldots + I_n end{aligned} $$ . Taking expectations on both sides, we get: . $$ begin{aligned} E[X] &amp;= E[I_1] + E[I_2] + ldots + E[I_n] &amp;= P(A_1) + P(A_2) + ldots + P(A_n) &amp;= n cdot frac{(n-1)!}{n!} &amp;= 1 end{aligned} $$ . Example. (Distinct Birthdays, birthday matches) In a group of $n$ people, under the usual assumptions about birthdays, what is the expected number of distinct birthdays among $n$ people, i.e. the expected number of days on which at least one of the people was born? What is the expected number of birthday matches, i.e. pairs of people with the same birthday? . Solution. . Let $X$ be the number of distinct birthdays, and we write $X = I_1 + I_2 + ldots + I_{365}$, where . $$ begin{aligned} I_j = begin{cases} 1 &amp; text{ if the }j text{th day is represented, } 0 &amp; text{ otherwise } end{cases} end{aligned} $$ . We create an indicator for each day of the year, because $X$ counts the number of days of the year that are represented. By the fundamental bridge: . $$ begin{aligned} E[I_j] &amp;= P(j text{th day is represented}) = 1 - P( text{no one is born on day }j) = 1 - left( frac{364}{365} right)^n end{aligned} $$ . for all $j$. Then, by linearity: . $$ begin{aligned} E[X] = 365 left(1 - left( frac{364}{365} right)^n right) end{aligned} $$ . Now, let $Y$ be the number of birthday matches, and we write $$ begin{aligned} Y &amp;= J_{12} + J_{13} + ldots + J_{n-1,n} end{aligned} $$ . where . $$ begin{aligned} J_{i,j} = begin{cases} 1 &amp; text{ pair }(i,j) text{ share the same birthday } 0 &amp; text{ otherwise } end{cases} end{aligned} $$ . We create an indicator for each pair of people since $Y$ counts the number of pairs of people with the same birthday. The probability of any two people having the same birthday is $1/365$, so again by the fundamental bridge and linearity, . $$ begin{aligned} E[Y] = {n choose 2} frac{1}{365} end{aligned} $$ . In addition to the fundamental bridge and linearity, the last two examples used a basic form of symmetry to simplify the calculations greatly: within each sum of indicator random variables, each indicator had the same expected value. For example, in the matching problem, the probability of the $j$th card being a match does not depend on $j$, so we can just take $n$ times the expected value of the first indicator random variable. . Example. A permutation $a_1,a_2, ldots, a_n$ of $1,2,3, ldots,n$ has a local maximum at $j$ if $a_j &gt; a_{j-1}$ and $a_j &gt; a_{j+1}$ )(for $2 leq j leq n - 1$; for $j = 1$, a local maximum at $j$ means $a_1 &gt; a_2$ while for $j = n$, it means $a_n &gt; a_{n-1}$. For example, $4,2,5,3,6,1$ has a local maxima at positions $1,3$ and $5$. The Putnam exam (a famous, hard, math competition, on which the median score is often a $0$) from 2006 posed the following question: for $n geq 2$ what is the average number of local maxima of a random permutation of $1,2, ldots,n$, with all $n!$ permutations equally likely? . Solution. . This problem can be quickly solved using indicator random variables, symmetry and the fundamental bridge. Let $I_1,I_2, ldots,I_n$ be indicator random variables where $Ij = 1$, if the there is a local maximum at position $j$ and $0$ otherwise. We are interested in the expected value of $ sum_{j=1}^{n}I_j$. For $1 &lt; j &lt; n$, $EI_j = frac{1}{3}$, since having a local maximum at $j$ is equivalent to $a_j$ being the largest of $a_{j-1},a_{j},a_{j+1}$, which has the probability $1/3$ since all orders are equally likely. For $j=1$ or $j = n$, we have $EI_j = frac{1}{2}$, since then there is only one neighbour. Thus, by linearity, . $$ begin{aligned} E left( sum_{j=1}^{n}I_j right) = 2 cdot frac{1}{2} + (n-2) cdot frac{1}{3} = frac{n+1}{3} end{aligned} $$ . The next example introduces the Negative Hypergeometric distribution, which completes the following table. The table shows the distributions for four sampling schemes: the sampling can be done with or without replacement, and the stopping rule can require a fixed number of successes. . begin{array}{ccc} hline &amp; textbf{ With replacement } &amp; textbf{ Without replacement } hline textbf{ Fixed number of trials } &amp; text{Binomial} &amp; text{Negative Binomial} textbf{ Fixed number of successes } &amp; text{Hypergeometric} &amp; text{Negative hypergeometric} hline end{array} Example. (Negative Hypergeometric distribution) An urn contains $w$ white balls and $b$ black balls. which are randomly drawn one-by-one without replacement, until $r$ white balls have been obtained. The number of black balls drawn before drawing the $r$th white ball has a Negative hypergeometric distribution with parameters $w,b,r$. We denote this distribution by $NHGeom(w,b,r)$. Of course, we assume that $r leq w$. For example, if we shuffle a deck of cards and deal them one at a time, the number of cards dealt before uncovering the first ace is NHGeom$(4,48,1)$. . As another example, suppose a college offers $g$ good courses and $b$ bad courses (for some definition of good and bad), and a student wants to find $4$ good courses to take. Not having any idea which of the courses are good, the student randomly tries out courses one at a time, stoppping when they have obtained $4$ good courses. Then, the number of bad courses the student tries out is NHGeom$(g,b,4)$ distributed. . We can obtain the PMF of $X sim$ NHGeom$(w,b,r)$ by noting that, in the urn context, $X = k$ means that the $(r+k)$th ball chosen is white and exactly $r-1$ white balls and $k$ black balls precede the $r$th white ball. This gives us, . $$ begin{aligned} P {X = k } &amp;= frac{{w choose r - 1}{b choose k}}{{w + b choose r + k - 1}} cdot frac{w - (r - 1)}{w + b - (r + k - 1)} end{aligned} $$ . Finding the expectation of a negative hypergeometric random variable directly from the definition of expectation results in complicated sums. But, let&#39;s find the expectation using indicator r.v.s. We can assume that we continue drawing balls until the urn is empty. First, consider the case $r = 1$. Label the black balls as $1,2,3, ldots,b$ and let $I_j$ be the indicator of the black ball $j$ being drawn before any white balls have been drawn. Then, $P(I_j = 1) = frac{1}{w+1}$ (this is probability of drawing a pre-assigned black ball $j$) since listing out the order in which black ball $j$ and the white balls are drawn (ignoring the other balls), all orders are equally likely by symmetry, and therefore $I_j = 1$ is equivalent to black ball $j$ being the first in the list. So, by linearity, . $$ begin{aligned} E left( sum_{j=1}^b I_j right) = sum_{j=1}^{b} E(I_j) = frac{b}{w+1} end{aligned} $$ . If we write $X = X_1 + X_2 + ldots + X_r$ where $X_j$ is the number of black balls drawn between the $(j-1)$th and $j$th white ball, then by linearity of expectations, we get that, $E[X] = frac{rb}{w+1}$. . Example. (St. Petersburg Paradox) A fair coin is tossed repeatedly. Let $T$ be the number of tosses until the first head. You are offered the following prospect, which you may accept on payment of a fee. If $T = k$, say, then you will recive $2^k$ british pounds. What would be a fair fee to ask of you? . Solution. . $T$ is a geometric random variable; $T sim $Geom$( frac{1}{2})$. This distribution of $T$ is given by, . $$ begin{aligned} P { T = k } = f_T(k) = q^k p = frac{1}{2^{k+1}} end{aligned} $$ . The expected number of tails until the first head is given by, . $$ begin{aligned} E[T] &amp;= sum_{k=0}^{ infty} frac{k}{2^{k+1}} &amp;= frac{1}{4} sum_{k=0}^{ infty}k left( frac{1}{2} right)^{k-1} &amp;= frac{1}{4} cdot frac{1}{ left(1- frac{1}{2} right)^2} &amp;= 1 end{aligned} $$ . The payoff function for this bet(prospect) is $Y = g(T) = 2^{T}$. If $T = k$, then you are rewarded $Y = 2^k$ British pounds. The distribution of the payoff is given by, . $$ begin{aligned} P {Y = y } &amp;= P {2^T = y } &amp;= P {T = log_2 y } &amp;= f_T( log_2 y) &amp;= frac{1}{2y} end{aligned} $$ . where $y=1,2,4,8,16, ldots$. . The expected value of the payoff is, . $$ begin{aligned} E[Y] &amp;= sum_{y in {2^k:k in mathbf{N} }} y f_Y(y) &amp;= sum_{y in {2^k:k in mathbf{N} }} y cdot frac{1}{2y} &amp;= sum_{y in {2^k:k in mathbf{N} }} frac{1}{2} &amp;= infty end{aligned} $$ . Thus, you would have to start with an infinite bankroll, in order to break even in this game, when it&#39;s played a large number of times, even though, heads shows up on average after one tail. Note that, $g(E[T]) = g(1) = 2$, whilst $E[g(T)]=E[Y]= infty$. . Law of the Unconscious Statistician(LOTUS) . As we saw in the St. Petersburg paradox, $E[g(X)]$ does not equal $g(E[X])$ in general, especially when $g$ is not linear. So, how do we correctly calculate $E[g(X)]$? Since, $g(X)$ is a random variable, one way is to first find the distribution of $g(X)$ and then use the definition of expectation. Perhaps, surprisingly it turns out that it is possible to find $E[g(X)]$ directly using the distribution of $X$, without first having to find the distribution of $g(X)$. This is done using the law of the unconscious statistician (LOTUS). . . Theorem. If $X$ is a discrete random variable and $g$ is a function from $ mathbf{R}$ to $ mathbf{R}$ then, . $$ begin{aligned} E[g(X)] = sum_{x} g(x) P { X = x } end{aligned} $$ . . Proof. . Let $Y = g(X)$. We have, . $$ begin{aligned} E[Y] &amp;= sum_{y} y P {Y = y } end{aligned} $$ . We can write the mathematical expectation in the ungrouped form, iterating over each sample point $ omega in Omega$ instead. . $$ begin{aligned} E[Y] &amp;= sum_{ omega in Omega} g(X( omega)) P { omega } end{aligned} $$ . Look at the set of points $ omega$ belonging to the set $ { omega in Omega:X( omega) = x }$. For all these points $X( omega) = x$, a constant. Hence, we can write the above expectation as a double sum, first holding $x$ constant, iterating over all $ omega$ such that $X( omega) = x$; and then iterating over all such points $x$. . $$ begin{aligned} E[Y] &amp;= sum_{x} sum_{ omega:X( omega)=x} g(X( omega)) P { omega } end{aligned} $$ . As $x$ is held constant in the inner sum, $g(X( omega)) = g(x)$, a constant and it can taken outside the inner-most sum. . $$ begin{aligned} E[Y] &amp;= sum_{x} g(X( omega)) sum_{ omega:X( omega)=x} P { omega } end{aligned} $$ . But, $ sum_{ omega:X( omega)=x} P { omega }$ is the probability of the event $ { X = x }$. Therefore, we should be able to write, . $$ begin{aligned} E[Y] &amp;= sum_{x} g(X( omega)) P { X = x } &amp;= sum_{x} g(x) f_X(x) end{aligned} $$ . This closes the proof. . Variance. . One important application of LOTUS is finding the variance of a random variable. Like expected-value, variance is a single-number summary of the distribution of a random variable. While expected value tells us the center of mass of the distribution, the variance tells us how spread out the distribution is. . . Definition (Variance and Standard deviation). The variance of a random variable $X$ is . $$ begin{aligned} Var[X] = E[X - EX]^2 end{aligned} $$ . The square root of the variance is called the standard deviation (SD): . $$ begin{aligned} SD(X) = sqrt{Var[X]} end{aligned} $$ . . Note that when we write $E[X - EX]^2$, we mean the expectation of the random variable $(X-EX)^2$ and not $(E(X - EX))^2$ (which is zero by linearity). . The variance of $X$ measures how far $X$ is from its mean on average, but instead of simply taking the average difference between $X$ and $EX$, we take the average squared difference. To see why, note that the average deviation from the mean, $E[X - EX]$, always equals $0$ by linearity; positive and negative deviations cancel each other out in the long run. By squaring the deviations, we ensure that both positive and negative deviations contribute to the overall variability. However, because variance is an average squared distance, it has the wrong units: if $X$ is in dollars, $Var[X]$ is in squared dollars. To get back to our original units, we take the square root; this gives us the standard deviation. . One might wonder why variance isn&#39;t defined as $E|X - EX|$, which would achieve the goal of counting both positive and negative deviations while maintaining the same units as $X$. This measure of variability isn&#39;t as popular as $E(X- EX)^2$, for a variety of reasons. Most notably, the absolute value isn&#39;t differentiable at $0$, whereas the squaring function is differentiable everywhere and is central in various fundamental mathematical results such as the Pythagorean theorem. . An equivalent expression for variance $Var(X) = E[X^2] - (E[X])^2$. This formula is often easier to work with when doing actual calculations. Since this is the variance formula we will use over and over again, we state it as it&#39;s own theorem. . . Theorem. For any random variable $X$, . $$ begin{aligned} Var(X) = E[X^2] - (E[X])^2 end{aligned} $$ . . Let $ mu = E[X]$. Expanding $(X - mu)^2$, we get: . $$ begin{aligned} E[(X - mu)^2] &amp;= E[X^2 -2 mu X + mu^2] &amp;= E[X^2] - 2 mu E[X] + mu^2 E[1] &amp;= E[X^2] - 2 mu^2 + mu^2 &amp;= E[X^2] - (E[X])^2 end{aligned} $$ . Variance has the following properties. The first two are easily from the definition, the third will be addressed in a later chapter and the last one is proven just after stating it. . $Var(X + c) = Var(X)$ for any constant $c$. From the definition, it follows that : | . $$ begin{aligned} Var(X + c) &amp;= E[X + c]^2 - (E[X+c])^2 &amp;= E[X^2 + 2cX + c^2] - (E[X] + c)^2 &amp;= E[X^2] + 2cE[X] + c^2 - (E[X])^2 - 2cE[X] - c^2 &amp;= E[X^2] - (E[X])^2 &amp;= Var(X) end{aligned} $$ . $Var(cX) = c^2 Var(X)$. We have: | . $$ begin{aligned} Var(cX) &amp;= E[cX]^2 - (E[cX])^2 &amp;= E[c^2 X^2] - (cE[X])^2 &amp;= c^2(E[X^2] - (E[X])^2) &amp;= c^2 Var(X) end{aligned} $$ . If $X$ and $Y$ are independent, then $Var(X + Y) = Var(X) + Var(Y)$. This is not true in general, if $X$ and $Y$ are dependent. We shall prove this fact shortly. | . Covariance of random variables. . Let $X$ and $Y$ be two random variables on the same sample space. Then, $X+Y$ and $XY$ are again random variables. Our aim is to now calculate the $Var(X + Y)$. For that purpose we introduce the notion of covariance, which will be analyzed in greater detail in a later section. If the joint distribution of $X$ and $Y$ is given by $f_{X,Y}(x_j,y_k)$, then the expectation of $XY$ by lotus is given by . $$ begin{aligned} E[XY] = sum x_j y_k f_{X,Y}(x_j,y_k) end{aligned} $$ . provided of course, the series converges absolutely. Now, the geometric mean of two real numbers is always less than or equal to the arithmetic mean. Thus, . $$ begin{aligned} sqrt{x_j^2 y_k^2} &amp; leq frac{x_j^2 + y_k^2}{2} |x_j y_k | &amp; leq frac{x_j^2 + y_k^2}{2} end{aligned} $$ . Therefore, $E(XY)$ always exists, if $E(X^2)$ and $E(Y^2)$ exist. . Consider the expectation of $(X - mu_x)(Y - mu_y)$. . Example. Each member of a group of $N$ players rolls a die. . (a) For any pair of players, who throw the same number, the group scores $1$ point. Find the mean and variance of the total score of the group. . (b) Find the mean and variance of the total score, if any pair of players who throw the same number scores that number. . Solution. . (a) Let $I_{ij}$ be a Bernoulli random variable. Suppose . $$ begin{aligned} I_{ij} = begin{cases} 1, &amp; quad text{if the pair }(i,j) text{ throw the same number}, j &gt; i 0, &amp; quad text{ otherwise } end{cases} end{aligned} $$ . The unconditional probability that the pair $(i,j)$ throw the same number; the probability of success is $p = frac{1}{6}$, and the probability of failure is $q = frac{5}{6}$. . Let $X$ be the total score of the group. Then, we have: . $$ begin{aligned} X &amp;= sum_{j &gt; i} I_{ij} E[X] &amp;= E left[ sum_{j &gt; i} I_{ij} right] &amp;= sum_{j &gt; i}E[I_{ij}] quad { text{ Linearity of expectations } } &amp;= {n choose 2} left( frac{1}{6} right) end{aligned} $$ . Also, the variance of $X$ is, . $$ begin{aligned} X &amp;= sum_{j &gt; i} I_{ij} Var[X] &amp;= Var left[ sum_{j &gt; i} I_{ij} right] &amp;= sum_{j &gt; i}Var[I_{ij}] quad { I_{ij} text{&#39;s are unconditionally independent } } &amp;= {n choose 2} left( frac{1}{6} right) left( frac{5}{6} right) end{aligned} $$ .",
            "url": "https://quantophile.github.io/mathematical-finance/probability-theory/2022/03/06/Discrete-Random-Variables-(summary).html",
            "relUrl": "/probability-theory/2022/03/06/Discrete-Random-Variables-(summary).html",
            "date": " • Mar 6, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Continuous Random Variables",
            "content": "Introduction. . Suppose, we have a thin straight metallic bar of length $(b-a)$, placed on the $x$-axis in the Cartesian plane with end-points $x=a$ and $x=b$. Assume that the density $f$ (how compactly or sparsely the atoms in the metal are packed) of the metal is variable and is a function of the position $x$. Assume that density per unit length at the point $x$ is $f(x)$ kg/m. We plot the density function $y=f(x)$ in the Cartesian plane below. . . We are interested to compute the mass of the metallic bar. We divide the entire interval $[a,b]$ into $n$ subintervals: . $$a = x_0 &lt; x_1 &lt; x_2 &lt; ldots &lt; x_n = b$$ . and let $ Delta x_i = x_i - x_{i-1}$ be the length of the $i$th subinterval. . The mass contribution of the $i$th subinterval is given by, $ text{Mass} = text{Density} times text{Length}$. Let $x_i^{*}$ be a test-point chosen at random in the $i$th subinterval i.e. $x_{i-1} leq x_i^{*} leq x_i$. We sample the density function $f(x)$ at this point. So, the mass of this teeny weeny element is: . $$m_i = f(x_i^{*}) Delta x_i$$ . and the total mass is approximately given by adding up the mass contributions of all the small elements: . $$M_n = sum_{i=1}^{n} f(x_i^{*}) Delta x_i$$. . Let us increase the number of sub-intervals $n$. As $n to infty$, the sum $M_n$ approaches true mass $M$. . $$M = lim_{ text{all } Delta x_i to 0} sum_{i=1}^{n} f(x_i^{*}) Delta x_i$$ . The limit of the weighted sum is the definite integral . $$M = int_{a}^{b}f(x) dx$$ . Let $X$ be a random variable, that can take on an uncountable number of values in $[a,b]$. Suppose that the probability that $X$ falls in a small infinitesimal interval $(x,x+dx)$ is variable and is a function of $x$, $f(x)dx$. $f(x)$ is said to be the probability density function of $X$. Then the total probability mass that $X$ falls between $a$ and $b$ is given by: . $$P(a leq X leq b) = int_{a}^{b} f(x) dx$$ . The numerical value $f(x)$ is not a probability. It is a density. We can think of $f(x)dx$ as an element of probability $P(x &lt; X leq x + dx)$. . Continuous Random Variables. . A random variable $X$ is continuous if its cumulative distribution function $F(x) = P(X leq x)$ is differentiable. . . Definition 1. A random variable $X$ is said to be continuous if its cumulative distribution function (CDF) $F(x)$ is differentiable and can be expressed as: . $$F_X(x) = int_{- infty}^{x}f_X(x) dx tag{1}$$ . . . Definition 2. The function $f_X$ is called the probability density function (PDF) of the continuous random variable $X$. . . Continuous random variables contrast starkly with discrete random variables in that $P(X=x)=0$ for all $x in mathbf{R}$; this may seem paradoxical since $X$ necessarily takes some value. Very roughly speaking the resolution of this paradox lies in the observation that there uncountably many possible values for $X$, this number is so large, that the probability of taking any particular value cannot exceed zero. . The probability that $X$ takes a value in the interval $[a,b]$ from (1) is: . $$P(a leq x leq b)=P(X leq b) - P(X leq a) = F_X(b) - F_X(a) = int_{a}^{b}f_X(x) dx tag{2}$$ . . Theorem 1. The PDF $f(x)$ of a continuous random variable must satisfy the following two criteria: . Non-negative: $f(x) leq 0$ | Integrates to $1$ : $ int_{ infty}^{ infty} f(x) dx = 1$ | . The first criterion is true, because probability is non-negative; if $f(x_0)$ were non-negative, then we could integrate over a tiny region around $x_0$ and get a negative probability. . The second criterion is true, since $ int_{ infty}^{ infty}f(x)dx$ is the probability of $X$ falling somewhere on the real line, which is $1$. . Conversely any such function $f$ is the PDF of some random variable. This is because if $f$ satisfies these properties, we can integrate is to get a function $F$ that satisfies the properties of a CDF. Then, a version of Universality of the uniform, the main concept in the next section can be used to create a random variable with CDF $F$. . . Example 1. (Logistic Distribution). The logistic distribution has the CDF . $$F_X(x) = frac{e^x}{1 + e^x}, quad x in mathbf{R}$$ . . To get the PDF, we differentiate the CDF, which gives: . $$ begin{aligned} f(x) &amp;= frac{(1+e^x)e^x - e^x cdot e^x}{(1+e^x)^2} &amp;= frac{e^x + e^{2x} -e^{2x}}{(1+e^x)^2} &amp;= frac{e^x}{(1+e^x)^2} end{aligned} $$ . Let $X$ be a logistic random variable. To find $P(-2 &lt; X &lt; 2)$, we need to integrate the PDF from $-2$ to $2$. . $$P(-2 &lt; X &lt; 2) = int_{-2}^{2} frac{e^x}{(1 + e^x)^2} dx = F(2) - F(-2) approx 0.76$$ . The integral was easy to evaluate since we already knew that $F$ was an antiderivative for $f$ and we had a nice expression for $F$. Otherwise, we could have made the substitution $u = 1 + e^x$, so that $e^x dx = du$. This gives: . { raw } $$ begin{aligned} P(-2 &lt; X &lt; 2) &amp;= int_{1+e^{-2}}^{1+e^2} frac{du}{u^2} &amp;= - left[ frac{1}{u} right]_{1+e^{-2}}^{1+e^2} &amp;= frac{e^2}{1 + e^2} - frac{1}{1+e^2} &amp;= frac{e^2 - 1}{e^2 + 1} end{aligned} $$ { endraw } . import numpy as np import matplotlib.pyplot as plt import math %matplotlib inline def f(x): return math.exp(x)/(1+math.exp(x))**2 def F(x): return math.exp(x)/(1+math.exp(x)) x = np.linspace(-4,4,100) y = list(map(f, x)) plt.xlabel(r&#39;$x$&#39;) plt.ylabel(r&#39;$f(x)$&#39;) plt.title(&#39;PDF of a logistic random variable&#39;) plt.plot(x,y) plt.show() . x = np.linspace(-4,4,100) y = list(map(F, x)) plt.xlabel(r&#39;$x$&#39;) plt.ylabel(r&#39;$F(x)$&#39;) plt.title(&#39;CDF of a logistic random variable&#39;) plt.plot(x,y) plt.show() . . Example 2. (Rayleigh Distribution) The Rayleigh distribution has the CDF . $$F(x) = 1 - e^{-x^2/2}, quad x &gt; 0$$ . . To get the PDF, we differentiate the CDF, which gives: . $$f(x) = xe^{-x^2/2}$$ . For $x leq 0$, both the CDF and the PDF are equal to $0$. . Let $X sim text{Rayleigh}$. To find $P(X &gt; 2)$, we need to integrate the PDF from $2$ to $ infty$. We can do that by making the substitution $u = -x^2/2$, but since we already have the CDF in a nice form, we know the integral is $F( infty) - F(2) = 1 - (1 - e^{-2}) = 1/e^2$. . The Rayleigh PDF and CDF are plotted in the figure. Again, the probability represented by a shaded area on the PDF and a vertical height on the CDF. . def f(x): return x * math.exp((-x**2)/2) def F(x): return 1 - math.exp((-x**2)/2) . x = np.linspace(0,5,100) y = list(map(f, x)) plt.xlabel(r&#39;$x$&#39;) plt.ylabel(r&#39;$f(x)$&#39;) plt.title(&#39;PDF of a Rayleigh random variable&#39;) plt.plot(x,y) plt.show() . x = np.linspace(0,5,100) y = list(map(F, x)) plt.xlabel(r&#39;$x$&#39;) plt.ylabel(r&#39;$F(x)$&#39;) plt.title(&#39;CDF of a Rayleigh random variable&#39;) plt.plot(x,y) plt.show() . . Definition 3. (Expectation of a continuous random variable). The expected value (also called expectation) of a continuous random variable with PDF $f$ is: . $$E(X) = int_{- infty}^{ infty}x f(x) dx$$ . . With this definition, the expected value retains its interpretaion as a center of mass. . . Theorem 2. (LOTUS, Continuous) If $X$ is a continuous random variable with PDF $f$ and $g$ is function from $ mathbf{R}$ to $ mathbf{R}$, then: . $$E(g(X)) = int_{- infty}^{ infty}g(x)f(x)dx$$ . . We now have all the tools we need to tackle the named distributions in this section, starting with the uniform distribution. . Uniform Distribution. . Intuitively, a uniform random variable on the interval $(a,b)$ is a complete random number between $a$ and $b$. We formalize the notion of &quot;completely random&quot; on an interval by specifying that the the PDF should be constant over the interval. . . Definition 4. (Uniform Distribution). A continuous random variable $U$ is said to have the Uniform Distribution on the interval $(a,b)$ if its PDF is: . $$ f(x) = begin{cases} frac{1}{b - a} &amp; text{ if } a &lt; x &lt; b 0 &amp; text{ otherwise } end{cases} $$We denote this by $U sim Uniform(a,b)$. . . This is a valid PDF because the area under the curve is just the area of a rectangle of width $b - a$ and height $ frac{1}{b -a}$. The CDF is the accumulated area under the PDF: . $$ F(x) = begin{cases} 0 &amp; text{ if } x leq a frac{x - a}{b - a} &amp; text{ if } a &lt; x &lt; b 1 &amp; text{ if } x geq b end{cases}$$ def f(x,a,b): return 1/(b-a) def F(x,a,b): return (x-a)/(b-a) . a = 0; b = 1 y = [f(x,a,b) for x in np.linspace(0,1,100)] plt.xlabel(r&#39;$x$&#39;) plt.ylabel(r&#39;$f(x)$&#39;) plt.title(&#39;PDF of a Uniform(0,1) random variable&#39;) plt.plot(x,y) plt.show() . a = 0; b = 1 y = [F(x,a,b) for x in np.linspace(0,1,100)] plt.xlabel(r&#39;$x$&#39;) plt.ylabel(r&#39;$f(x)$&#39;) plt.title(&#39;CDF of a Uniform(0,1) random variable&#39;) plt.plot(x,y) plt.show() . The Uniform distribution that we will most frequently use is the Uniform(0,1) distribution, also called the standard uniform. The Uniform(0,1) PDF and CDF are particularly simpy: $f(x) = 1$ and $F(x) = x$ for $0&lt;x&lt;1$. . For a general Uniform(a,b) distribution, the PDF is constant on $(a,b)$ and the CDF is ramp-shaped, increasing linearly from $0$ to $1$ as $x$ ranges from $a$ to $b$. . For the Uniform distribution, the probability is proportional to the length. . . Proposition 1. Let $U sim Uniform(a,b)$ and $(c,d)$ be a subinterval of $(a,b)$ of length $l$ (so that $l = d - c$). Then, the probability of $U$ being in $(c,d)$ is proportional to $l$. For example, a subinterval that is twice as long has twice the probability of containing $U$, and a subinterval of the same length has the same probability. . . Proof. Since the PDF of $U$ is the constant $ frac{1}{b-a}$ on $(a,b)$, the area under the PDF from $c$ to $d$ is $ frac{l}{b-a}$, which is a constant times $l$. . The above proposition is a very special property of the uniform: for any other distribution, there are intervals of the same length, that have different probabilities. Even after conditioning on a uniform random variable being in a certain subinterval, we still have a uniform distribution and thus still have probability proportional to the length (within that subinterval). We how this below. . . Proposition 2. Let $U sim Uniform(a,b)$ and let $(c,d)$ be a subinterval of $(a,b)$. Then, the conditional distribution of $U$ given $U in (c,d)$ is $Uniform(c,d)$. . . Proof. For $u$ in $(c,d)$, the conditional CDF at the point $U=u$ is : . $$ begin{aligned} P(U leq u|U in (c,d)) &amp;= frac{P(U leq u, c &lt; U &lt; d)}{P(U in (c,d))} &amp;= frac{P(U in (c,u))}{P(U in (c,d))} &amp;= frac{ left( frac{u-c}{b-a} right)}{ left( frac{d-c}{b-a} right)} &amp;= frac{u-c}{d-c} end{aligned} $$ . The conditional CDF is $0$ for $u leq c$ and $1$ for $u geq d$. So, the conditional distribution of $U$ is as claimed. . . Example 3. Let&#39;s illustrate the above propositions for $U sim Uniform(0,1)$. In this special case, the support has length $1$, so the probability is the length: the probability of $U$ falling into the interval $(0,0.3)$ is $0.3$, as is the probability of falling into $(0.3,0.6)$, $(0.4,0.7)$ or an other interval of length $0.3$ within $(0,1)$. . Now suppose, that we learn that $U in (0.4,0.7)$. Given this information, the conditional distribution of $U$ is $Uniform(0.4,0.7)$. Then, the conditional probability of $U in(0.4,0.6)=2/3$, since $(0.4,0.6)$ provides $2/3$ of the length of $(0.4,0.7)$. The conditional probability of $U in (0,0.6)$ is also $2/3$, since we discard the points to the left of $0.4$ when conditioning on $U in (0.4,0.7)$. . . Next, let&#39;s derive the mean and variace of $U sim Uniform(a,b)$. The expectation is extremely intuitive: the PDF is constant, so it&#39;s balancing point should be the midpoint of $(a,b)$. This is exactly, what we find by using the definition of expectation for continuous random variables. . $$ begin{aligned} E(X) &amp;= int_{a}^{b}x f(x) dx &amp;= int_{a}^{b} x cdot frac{1}{b-a} dx &amp;= frac{1}{b-a} left[ frac{x^2}{2} right]_{a}^{b} &amp;= frac{1}{b-a} cdot frac{b^2 - a^2}{2} &amp;= frac{a+b}{2} end{aligned} $$ . For the variance, we first find $E(U^2)$ using the continuous version of LOTUS. . $$ begin{aligned} E(U^2) &amp;= int_{a}^{b}u^2 f(u) du &amp;= int_{a}^{b} u^2 cdot frac{1}{b-a} du &amp;= frac{1}{b-a} left[ frac{u^3}{3} right]_{a}^{b} &amp;= frac{1}{b-a} cdot frac{b^3 - a^3}{3} &amp;= frac{b^2 + ab + a^2}{3} end{aligned} $$ . Then, . $$ begin{aligned} Var(U) &amp;= E(U^2) - (E(U))^2 &amp;= frac{b^2 + ab + a^2}{3} - left( frac{a + b}{2} right)^2 &amp;= frac{b^2 + ab + a^2}{3} - frac{a^2 + 2ab + b^2}{4} &amp;= frac{4b^2 + 4ab + 4a^2 - 3a^2 -6ab - 3b^2}{12} &amp;= frac{b^2 - 2ab + a^2}{12} &amp;= frac{(b-a)^2}{12} end{aligned} $$ . The above derivation isn&#39;t terribly painful, but there is an easier path, using a technique that is often useful for continuous distributions. This technique is called the location-scale transformation, and it relies on the observation that shifting and scaling a uniform random variable produces another uniform random variable. Shifting is considered a change of location, and scaling is a change of scale, hence the term location-scale. For example, if $X$ is unifrom on the interval $(1,2)$, then $X+5$ is uniform on the interval $(6,7)$, $2X$ is uniform on the interval $(2,4)$, and $2X + 5$ is uniform on $(7,9)$. . . Definition 5. (Location-Scale Transformation). Let $X$ be a random variable and $Y = sigma X + mu$, where $ sigma$ and $ mu$ are constants with $ sigma &gt; 0$. Then, we say that $Y$ has been obtained as a location-scale transformation of $X$. Here, $ mu$ controls how the location is changed and $ sigma$ control how the scale is changed. . . In a location-scale transformation, starting with $X sim Uniform(a,b)$ and transforming it to $Y = cX + d$ where $c$ and $d$ are constants with $c &gt; 0$, $Y$ is linear function of $X$ and uniformity is preserved: $Y sim Uniform(ca + d, cb + d)$. But, if $Y$ is a non-linear transformation of $X$, then $Y$ will not be linear in general. For example, for $X sim Uniform(a,b)$ with $0 leq a &lt; b$, the transformed random variable $Y = X^2$ has support $(a^2,b^2)$ but is not uniform on that interval. . Let&#39;s see how this works for finding the expectation and variance of the $Uniform(a,b)$ random variable. The location-scale strategy says to start with $U sim Uniform(0,1)$. Since the PDF of $U$ is just $1$ on the interval $(0,1)$, it is easy to see that: . $$ begin{aligned} E(U) = int_{0}^{1} x dx = frac{1}{2} E(U^2) = int_{0}^{1} x^2 dx = frac{1}{3} Var(U) = frac{1}{3} - frac{1}{4} = frac{1}{12} end{aligned} $$ Now, that we know the answers for $U$, transforming $U$ into a general $Uniform(a,b)$ random variable takes just two steps. First we change the domain from an interval of length $1$ to an interval of length $b-a$, so we multiply $U$ by the scaling factor $(b-a)$ to obtain a $Uniform(0,b-a)$ random variable. Then we shift everythin until the left endpoint of the domain is at $a$. Thus, if $U sim Uniform(0,1)$, the random variable : . $$ tilde{U} = a + (b-a)U$$ . is distributed $Uniform(a,b)$. Now the mean and variance of $ tilde{U}$ follows directly from the properties of expectation and variance. By linearity of expectations, . $$E( tilde{U}) = a + (b-a)E(U) = a + frac{b-a}{2} = frac{a + b}{2}$$ . By the fact that additive constants don&#39;t affect the variance while multiplicative constants come out squared, . $$Var( tilde{U}) = frac{(b-a)^2}{12}$$ . These agree with our previous answers. . The technique of location-scale transformation will work for any family of distributions such that shifting and scaling a random variable whose distribution in the family produces another random variable whose distribution is in the family. This technique does not apply to families of discrete distributions with a fixed support since, for example, shifting or scaling $X sim Binomial(k;n,p)$ changes the support and produces a random variable that is no longer Binomial. A Binomial random variable must be able to take on all integer values between $0$ and some upper bound, but $X + 4$ can&#39;t take values in $ {0,1,2,3 }$ and $2X$ can only take even values, so neither of these random variables has a binomial distribution. . Universality of the Uniform. . In this section, we will discuss a remarkable property of the Uniform distribution: given a $Uniform(0,1)$ random variable, we can construct a random variable with any continuous distribution we want. Conversely, given a random variable with an arbitrary continuous distribution, we can create a uniform random variable. We call this the universality of the Uniform, because it tells us the Uniform is a universal starting point for building random variables with other distributions. Universality of the Uniform also goes by many other names, such as the probability integral transform, inverse transform sampling, the quantile trasnformation, and even the fundamental theorem of simulation. . To keep proofs simple, we will state the universality of the Uniform for a case where we know that the inverse of the desired CDF exists. More generally, similar ideas can be used to simulate a random draw from any desired CDF. . . Theorem 3. (Universality of the Uniform) Let $F$ be a CDF which is a continuous function and strictly increasing on the support of the distribution. This ensures that the inverse function $F^{-1}$ exists as a function from $(0,1)$ to $ mathbf{R}$. We then have the following results: . Let $U sim Uniform(0,1)$ and $X = F^{-1}(U)$. Then, $X$ is a random variable with CDF $F$. . | Let $X$ be a random variable with CDF $F$. Then, $F(X) sim Uniform(0,1)$. . | . Let&#39;s make sure we understand what each part of the theorem is saying. The first part of the theorem says that if we start with $U sim Uniform(0,1)$ random variable and a CDF $F$, then we can create a random variable whose CDF is $F$ by plugging $U$ into the inverse CDF $F^{-1}$. Since $F^{-1}$ is a function (known as the quantile function), $U$ is a random variable, and a function of a random variable is a random variable, $F^{-1}(U)$ is a random variable; universality of the uniform says its CDF is $F$. . The second part of the theorem goes in the reverse direction, starting from a random variable $X$ whose CDF is $F$ and then creating a $Uniform(0,1)$ random variable again. Again, $F$ is a function, $X$ is a random variable, and a function of a random variable is a random varaible, so $F(X)$ is a random variable. Since, any CDF is between $0$ and $1$ everywhere, $F(X)$ must take values between $0$ and $1$. Universality of the uniform says that the distribution of $F(X)$ is Uniform on $(0,1)$. . . Note: The second part of the universality of the Uniform involves plugging a random variable $X$ into its own CDF $F$. This may seem strangely self-referential, but it makes sense because $F$ is a just a function (that satisfies the properties of a valid CDF). There is a potential notational confusion, however : $F(x) = P(X leq x)$ by definition, but it would be incorrect to say $F(X) = P(X leq X) = 1$. Rather, we should first find an expression for the CDF as a function of $x$, then replace $x$ with $X$ to obtain a random variable. For example, if the CDF of $X$ is $F(x) = 1 - e^{-x}$ for $x &gt; 0$, then $F(X) = 1 - e^{-X}$. . Understanding the statement of the theorem is the most difficult part; the proof is just a couple of lines for each direction. . Proof. . Let $U sim Uniform(0,1)$ and $X = F^{-1}(U)$. For all real $x$, | $$P(X leq x) = P(F^{-1}(U) leq x) = P(U leq F(x)) = F(x)$$ . so the CDF of $X$ is $F$, as claimed. For the last equality, we used the fact that $P(U leq u) = u$ of $u in (0,1)$. . Let $X$ have the CDF $F_X$, and find the CDF of $Y = F_X(X)$. Since $Y$ takes values in $(0,1)$, $P(Y leq y) = 0$ for $y leq 0$ and equals $1$ for $y geq 1$. For $y in (0,1)$, we have: | $$F_Y(y)=P(Y leq y) = P(F_X(X) leq y) = P(X leq F_X^{-1}(y)) = F_X(F_X^{-1}(y))=y$$ . Thus, $Y$ has $Uniform(0,1)$ CDF. . To gain more insight into what the quantile function $F^{-1}$ and the universality of the uniform mean, let&#39;s consider an example that is familiar to millions of students: percentiles on an exam. . . Example 4. (Percentiles) A large number of stuents take a certain exam, graded on a scale from $0$ to $100$. Let $X$ be the score of a random student. Continuous distributions are easier to deal with here, so let&#39;s approximate the discrete distribution of scores using a continuous distribution. Suppose that $X$ is continuous, with a CDF $F$ that is strictly increasing on $(0,100)$. In reality there are only finitely many students and only finitely many possible scores, but a continuous distribution may be a good approximation. . Suppose that the median score on the exam is $60$, i.e. half o f the students score above $60$ and the other half score below $60$ (a convenient aspect of assuming a continuous distribution is that we don&#39;t need to worry above how many students had score equal to $60$). That is, $F(60) = 1/2$ or equivalently $F^{-1}(1/2) = 60$. . If Fred scores a $72$ on the exam, then his percentile is the fraction of students who score below $72$, which is some number in $(1/2,1)$, since $72$ is above the median. This is $F(72)$. In general, a student with score $x$ has percentile $F(x)$. Going the other way, if we start with a percentile, say $0.95$, then $F^{-1}(0.95)$ is the score that that percentile. A percentile is also called a quantile, which is why $F^{-1}$ is called the quantile function. The function $F$ converts the scores to percentiles, and the function $F^{-1}$ converts the $p$th percentile to a score. . . The strange operation of plugging $X$ into its own CDF now has a natural interpretation : $F(X)$ is the percentile attained by a random student. It often happens that the distribution of scores on an exam is very non-Uniform. For example, there is no reason to thing that $10 %$ of the scores are between $70$ and $80$, even though $(70,80)$ covers $10 %$ of the possible range of scores. . On the other hand, the distribution of percentiles of the students is Uniform: the universality property shows that $F(X) sim Uniform(0,1)$. For example, $50 %$ of the students have a percentile of at least $0.50$. Universality of the uniform is expressing the fact that $10 %$ of the students have a percentile between $0$ and $0.1$, $10 %$ have a percentile between $0.2$ and $0.3, and so on - a fact that is clear from the definition of percentile. . . Example 5. (Universality with Logistic.) The Logistic CDF is: . $$F(x)= frac{e^x}{1+e^x}$$ . Suppose we have $U sim Uniform (0,1)$ and wish to generate a logistic random variable. Part 1 of the universality property says that $F^{-1}(U) sim Logistic$, so we first invert the CDF to get $F^{-1}$ : . $$ begin{aligned} y &amp;= frac{e^x}{1 + e^x} (1+e^x)y &amp;= e^x y &amp;= e^x(1 - y) e^x &amp;= frac{y}{1-y} x &amp;= log left( frac{y}{1-y} right) end{aligned} $$ . So the inverse CDF is given by: . $$F^{-1}(u) = log left( frac{u}{1-u} right)$$ . Therefore, $ log left( frac{U}{1-U} right) sim Logistic$. . . We can verify directly that $ log left( frac{U}{1-U} right)$ has the required CDF: start from the definition of the CDF, do some algebra to isolate $U$ on one side of the inequality, and then use the CDF of the uniform distribution. Let&#39;s work through these calculations once for practice: . $$ begin{aligned} P left( log left( frac{U}{1-U} right) leq x right) &amp;= P left( left( frac{U}{1-U} right) leq e^x right) &amp;= P left(U leq e^x(1-U) right) &amp;= P left(U(1+e^x) leq e^x right) &amp;= P left(U leq frac{e^x}{(1+e^x)} right) &amp;= frac{e^x}{1+e^x} end{aligned} $$ . which is indeed the logistic CDF. . We can also simulation to visualize how universality of the Uniform works. To this end, we generated $1$ million $Uniform(0,1)$ random variables. We then trasformed each of these values $u$ into $ log left( frac{u}{1-u} right)$. If the universality of the Uniform is correct, the transformed numbers should follow a Logistic distribution. . import numpy as np import matplotlib.pyplot as plt u = np.random.rand(1000000) plt.title(&#39;Histogram of u&#39;) plt.hist(u,bins=50) plt.show() . x = np.log(np.divide(u,1-u)) plt.hist(x,bins=50) plt.title(&#39;Histogram of log(u/1-u)&#39;) plt.show() . Thus, by applying $F^{-1}$, we were able to transform our Uniform draws into Logistic draws, exactly as claimed by the universality of the Uniform. . Conversely, part 2 of the universality property states that if $X sim Logistic$, then . $$F(X) = frac{e^X}{1+e^X} sim Uniform(0,1)$$ . . Example 6. (Universality with Rayleigh.) . The Rayleigh CDF is . $$F(x) = 1 - e^{-x^2/2}, quad x &gt; 0$$ . The quantile function (the inverse CDF) is given by: . $$ begin{aligned} y &amp;= 1 - e^{- frac{x^2}{2}} e^{- frac{x^2}{2}} &amp;= 1 - y - frac{x^2}{2} &amp;= log (1 - y) x^2 &amp;= -2 log (1 - y) x &amp;= sqrt{-2 log (1 - y)} end{aligned} $$ . so the inverse CDF is: . $$F^{-1}(u) = sqrt{-2 log (1 - u)}$$ . . so if $U sim Uniform(0,1)$, then $F^{-1}(U) = sqrt{-2 log(1-U)} sim Rayleigh$. . def inverse_transform(u): return np.sqrt(-2*np.log(1-u)) x = list(map(inverse_transform, u)) plt.hist(x,bins=50) plt.title(&#39;Histogram of sqrt(-2*log(1-u))&#39;) plt.show() . We again started with $1$ million realizations of $U sim Uniform(0,1)$ and transformed them to produce $1$ million realizations of $ sqrt{-2 log(1-U)}$. As the above figure shows, the realizations of $ sqrt{-2 log(1-U)}$ looks very similar to the Rayleigh PDF, as predicted by the universality of the Uniform. . Conversely, if $X sim Rayleigh$, then $F(X) = 1 - e^{X^2/2} sim Uniform(0,1)$. . Now, let us consider the extent to which universality of the Uniform holds for discrete random variables. The CDF $F$ of a discrete random variable has jumps and flat regions, it is a step-function, so $F^{-1}$ does not exist (in the usual sense). But, part 1 still holds in the sense that given a uniform random variable, we can construct a random varaible with any discrete distribution we want. The difference is instead of working with the CDF, which is not invertible, it more straightforward to work with the PMF. . Our approach is best illustrated with a picture. Suppose we want to use $U sim Uniform(0,1)$ to construct a discrete random variable with PMF $p_j = P(X = j)$ for $j=0,1,2, ldots,n$. As illustrated in the figure below, we can chop up the interval $(0,1)$ into pieces of lengths $p_0$, $p_1$, $ ldots$, $p_n$. By the properties of a valid PMF, the sum of all the $p_j$&#39;s is $1$, so this perfectly divides up the interval, without overshooting or undershooting. . . Now, define $X$ to be the random variable which equals $0$ if $U$ falls in the $p_0$ interval, $1$ if $U$ falls into the $p_1$ interval, $2$ if $U$ falls in the $p_2$ interval, and so on. Then, $X$ is a discrete random variable taking on values $0$ through $n$. The probability that $X=j$ is the probability that $U$ falls into the interval of length $p_j$. But, for a uniform random variable, the probability is the length, so $P(X=j)$ is precisely $p_j$, as desired! . The same trick will work for any discrete random varaible that can take on infinitely many possible values such as Poisson; we&#39;ll need to chop $(0,1)$ into infinitely many pieces, but the length of the pieces is still $1$. . Part 2 of the universality of the uniform, on the other hand, fails for discrete random variables. A function of discrete random variable is still discrete, then $F(X)$ is still discrete. So, $F(X)$ doesn&#39;t have a uniform distribution. For example, if $X sim Bernoulli(p)$, then $F(X)$ has only two possible values: $F(0)=1 - p$ and $F(1)=1$. . The takeaway from this section is that we can use a uniform random variable $U$ to generate random variables from both continuous and discrete distributions: in the continuous case, we can plug $U$ into the inverse CDF, and in the discrete case, we can chop up the unit interval according to the desired PMF. Universality of the uniform is often useful in practice when running simulations (since the software being used may know how to generate uniform random variables, but not know how to generate random variables with the distribution of interest), though the extent to which it is useful depends partly on how tractable it is to compute the inverse CDF. .",
            "url": "https://quantophile.github.io/mathematical-finance/probability-theory/2022/03/06/Continuous-Random-Variables.html",
            "relUrl": "/probability-theory/2022/03/06/Continuous-Random-Variables.html",
            "date": " • Mar 6, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "The Binomial and Poisson Distributions - Exercise Problems.",
            "content": ". 1.Assuming all sex distributions to be equally probable, what proportion of families with exactly six children should be expected to have three boys and three girls? . . Solution. . Let us identify success with the child being a girl, and failure as the child being a boy. Let the r.v. $X$ be the number of successes in $n=6$ trials. The required probability is, . begin{align*} P(X=3) &amp;= {6 choose 3} left( frac{1}{2} right)^3 left(1- frac{1}{2} right)^3 &amp;={6 choose 3} left( frac{1}{2} right)^6 &amp;=0.3125 end{align*} using Distributions binomial(6,3)*(0.5)^6 . 0.3125 . . 2.A bridge player had no ace in three consecutive hands. Did he have reason to complain of ill luck? . . Solution. . Let us identify success with getting one or more aces in drawing a bridge card hand. The probability $q$ of no ace in a bridge card hand is given by, . begin{align*} q &amp;= frac4852 &amp;= 0.3038175 end{align*}If the $3$ bridge card hands are drawn with replacement, then these are independent Bernoulli trials. So, the probability of no ace in three consecutive hands is $q^3 = 0.028043$. . q = binomial(48,13)/binomial(52,13) . 0.30381752701080433 . q^3 . 0.028043904088925094 . . 3.How long has a series of random digits to be in order for the probability of the digit $7$ appearing to be atleast $ frac{9}{10}$? . . Solution. . Let us identify success with the appearance of the digit $7$. The probability $p$ of success is $ frac{1}{10}$. Since each digit is randomly drawn from $ {0,1,2,3,4,5,6,7,8,9 }$ with replacement, these are independent Bernoulli trials. Let $X$ be the number of successes in $n$ Bernoulli trials. $X$ follows the binomial distribution. . begin{align*} P(X=k) = {n choose k}p^k {q^{n-k}} end{align*}We are interested to find $k$ such that, . begin{align*} P(X=0) &amp; leq left(1- frac{9}{10} right) q^n &amp; leq 0.1 left( frac{9}{10} right)^n &amp; leq 0.1 n log(0.9) &amp; leq log 0.1 n &amp; geq frac{ log 0.1}{ log 0.9} &amp;= 21.85434 end{align*}So, $n geq 22$ trials. . log(0.1)/log(0.9) . 21.854345326782834 . . 4.How many independent bridge dealings are required in order for the probability of a preassigned player having four aces at least once to be $ frac{1}{2}$ or better? Solve again for some player instead of a given one. . . Solution. . (a) Let us identify success as a pre-assigned player (e.g. North) getting four aces in a bridge card hand. The probability $p$ of success is ${48 choose 9}/{52 choose 13}$. Since each dealing is independent, these are independent Bernoulli trials. Let $X$ be the number of successes in $n$ independent dealings. . We are interested to have, . begin{align*} P(X geq 1) &amp; geq frac{1}{2} 1 - P(X geq 1) &amp; leq frac{1}{2} P(X = 0) &amp; leq 0.50 left(1- frac{48 choose 9}{52 choose 13} right)^n &amp; leq 0.50 0.9973589^n &amp; leq 0.50 n &amp; geq frac{ log 0.50}{ log 0.9973589} n &amp; geq 262.10 end{align*} 1-binomial(48,9)/binomial(52,13) . 0.9973589435774309 . log(0.50)/ log(0.9973589435774309) . 262.10400156654214 . (b) Let us identify success as some player getting four aces in a bridge card hand. The probability $p$ of success is . Repeating the calculations as above, . $ begin{align*} (1 - 0.01056422)^n &amp; leq 0.50 (0.98943578)^n &amp; leq 0.50 n &amp; geq frac{ log 0.50}{ log 0.98943578} n &amp; geq 66 end{align*}$ . binomial(4,1) * binomial(48,9) / binomial(52,13) . 0.01056422569027611 . 1-0.01056422 . 0.98943578 . log(0.50)/log(0.98943578) . 65.26553027090138 . . 5.If the probability of hitting a target is $ frac{1}{5}$ and ten shots are fired independently, what is the probability of the target&#39;s being hit at least twice? . . Solution. . Let us identify success as the target being hit by a shot. Let $X$ be the number of shots hitting the target. $X$ follows the binomial distribution. We have, . begin{align*} P(X geq 2) &amp;= 1 - P(X=0) - P(X=1) &amp;= 1 - {10 choose 0}(0.8)^{10} - {10 choose 1}(0.2)^{1}(0.8)^{9} &amp;= 0.624190 end{align*} p₀ = 0.8^10 p₁ = binomial(10,1)*0.2*0.8^9 1 - p₀ - p₁ . 0.6241903615999997 . . 6.In problem 5, find the conditional probability of the target&#39;s being hit atleast twice, assuming that atleast one hit is scored. . . Solution. begin{align*} P(X geq 2 | X geq 1) &amp;= frac{P(X geq 2, X geq 1)}{P(X geq 1)} &amp;= frac{P(X geq 2)}{P(X geq 1)} &amp;= frac{1 - P(X=0) - P(X=1)}{1 - P(X=0)} &amp;= frac{0.6241903}{0.8926258} &amp;= 0.6992743 end{align*} . 1 - p₀ . 0.8926258175999999 . (1 - p₀ - p₁)/(1-p₀) . 0.6992743759958213 . . 7.Find the probability that a hand of thirteen bridge cards selected at random contains exactly two red cards. Compare it with the corresponding probability in Bernoulli trials with $p= frac{1}{2}$. . . Solution. When drawing a bridge hand, the cards are sampled without replacement. So, they are not independent Bernoulli trials. Actually the number of red cards follows a hypergeometric distribution. The probability that a hand of thirteen bridge cards selected at random contains exactly two red cards is given by, . begin{align*} P(X_R = 2) &amp;= frac{{26 choose 2} {26 choose 11}} {52 choose 13} &amp;= 0.0039542 end{align*} . In comparison, if these cards are drawn with replacement, i.e. these were independent Bernoulli trials, then . begin{align*} P(X_R = 2) &amp;= {13 choose 2}2^{-13} &amp;= 0.0095214 end{align*} p₂ = (binomial(26,2) * binomial(26,11))/binomial(52,13) . 0.003954249420408755 . p_binom = binomial(13,2)*2^(-13) . 0.009521484375 . . 8.What is the probability that the birthdays of six people fall in two calendar months leaving exactly ten calendar months free? (Assume independence and equal probabilities for all months.) . . Solution. We think of the six people as balls and the 12 calendar months as cells. We are placing six balls into twelve cells. We can choose the two calendar months in ${12 choose 2}$ distinguishable ways. . Six people can fall into 2 calendar months leaving exactly ten calendar months free in $2^6$ distinguishable ways. . And six people can be assigned to the twelve calendar months in $12^6$ distinguishable ways. . The required probability should be, . begin{align*} frac{{12 choose 2}2^6}{12^6} end{align*} . (binomial(12,2) * 2^6)/12^6 . 0.0014146090534979425 . However, we would like to leave exactly ten calendar months free, so we must exclude the possibility that six people&#39;s birthdays fall only in the first calendar month or only in the second calendar month. Thus, the desired probability is, . begin{align*} {12 choose 2} left( frac{2^6}{12^6} - 2 cdot frac{1}{12^6} right) approx 0.001370 end{align*} binomial(12,2)*((2^6/12^6)-2*(1/12^6)) . 0.0013704025205761316 . . 9.In rolling six true dice, find the probability of obtaining (a)at least one(b)exactly one (c)exactly two, aces. Compare with poisson approximations. . . Solution. . Let us identify success as getting an ace in a dice roll. Since each dice roll is independent, we have a sequence of $6$ independent Bernoulli trials. Let $X$ be the number of aces in the $6$ Bernoulli trials. $X$ follows the binomial distribution. The probability of success $p=1/6$. . (a) $ begin{align*} P(X geq 1) &amp;= 1 - P(X=0) &amp;= 1 - q^6 &amp; approx 0.665102 end{align*}$ . (b) $ begin{align*} P(X = 1) &amp;= {6 choose 1}pq^5 &amp; approx 0.401877 end{align*}$ . (c) $ begin{align*} P(X = 2) &amp;= {6 choose 2}p^2 q^4 &amp; approx 0.200938 end{align*}$ . n,p,q = 6,1/6,5/6 1 - q^6, binomial(6,1)*p*(q^5), binomial(6,2)*(p^2)*(q^4) . (0.6651020233196159, 0.401877572016461, 0.2009387860082305) . λ = n*p 1-(ℯ^(-λ)),(ℯ^(-λ))*(λ/1), (ℯ^(-λ))*(λ^2/2) . (0.6321205588285577, 0.36787944117144233, 0.18393972058572117) . . 10.If there are on the average 1 per cent left-handers, estimate the chances of having at least four left-handers among 200 people. . . Solution. We have, $ lambda = np = 0.01*200 = 2$. Here $n$ is large, $p$ is small. Let $X$ be the number of left handers among the 200 people. $X$ is a poisson random variable. . begin{align*} P(X geq 4) &amp;= 1 - P(X=0) - P(X=1) - P(X=2) - P(X=3) &amp;= 1 - e^{- lambda} left[1+ frac{ lambda}{1}+ frac{ lambda^2}{2!} + frac{ lambda^3}{3!} right] &amp; approx 0.1428765 end{align*} poisson(k,λ) = (ℯ^(-λ))*(λ^k/factorial(k)) poisson(0,2), poisson(1,2), poisson(2,2), poisson(3,2) . (0.1353352832366127, 0.2706705664732254, 0.2706705664732254, 0.18044704431548358) . 1 - poisson(0,2) - poisson(1,2) - poisson(2,2) - poisson(3,2) . 0.1428765395014529 . . 11.A book of 500 pages contains 500 misprints. Estimate the chances that a given page contains atleast three misprints. . . Solution. . Here, the total number of misprints $T = 500$, and the experiment is performed $N=500$ times. On an average, the number of misprints per page $ lambda t = T/N = 500/500 = 1$. . begin{align*} P(X geq 3) &amp;= 1 - P(X leq 2) &amp;=1 - (P(X=0) + P(X=1) + P(X=3)) &amp; approx 0.080301 end{align*} poisson(0,1),poisson(1,1),poisson(2,1) . (0.36787944117144233, 0.36787944117144233, 0.18393972058572117) . 1 - poisson(0,1) - poisson(1,1) - poisson(2,1) . 0.08030139707139416 . . 12.Colorblindness appears in 1 per cent of the people in a certain population. How large must a random sample (with replacements) be if the probability of its containing a colorblind person is to be 0.95 or more? . . Solution. . We are given that $P(X ge 1) geq 0.95$. Consequently, . begin{align*} P(X = 0) &amp; leq 0.05 e^{- lambda} &amp; leq 0.05 - lambda &amp; leq log 0.05 lambda &amp; geq 2.9957322 n(0.01) &amp; geq 2.9957322 n &amp; geq 300 end{align*} -log(0.05) . 2.995732273553991 . . 13.In the preceding exercise, what is the probability that a sample of $100$ people will contain (a)no (b) two or more, colorblind people? . . Solution. We have, $ lambda = np = 100 cdot 0.01 = 1$. . (a) $ begin{align*} P(X=0) &amp;= e^{-1} = 0.36787 end{align*}$ . (b) $ begin{align*} P(X geq 2) &amp;= 1 - P(X &lt; 2) &amp;= 1 - P(X=0) - P(X=1) &amp;= 1 - e^{-1} - e^{-1}/2 &amp; approx 0.44818 end{align*}$ . ℯ^(-1), ℯ^(-1)/2 . (0.36787944117144233, 0.18393972058572117) . 1 - ℯ^(-1) - ℯ^(-1)/2 . 0.4481808382428365 . . 14.Estimate the number of raisins which a cookie should contain on the average if it is desired that the probability of a cookie to contain atleast one raisin be 0.99 or more. . . Solution. Let $X$ be the number of raisins in a cookie. $X sim Poisson(k; lambda)$. . begin{align*} P(X geq 1 ) &amp; geq 0.99 1 - P(X geq 1) &amp; leq 1 - 0.99 P(X=0) &amp; leq 0.01 e^{- lambda} &amp; leq 0.01 - lambda &amp; leq log 0.01 lambda &amp; geq 4.60517 end{align*} log(0.01) . -4.605170185988091 . . 15.The probability of a royal flush in poker is $p=1/649740$. How large has $n$ to be to render the probability of no royal flush in $n$ hands smaller than $1/e approx frac{1}{3}$? (Note: No calculations are necessary for the solution). . . Solution. . A royal flush is an ace, king, queen, jack and ten of a suit. . The probability of a royal flush is $p = frac{{4 choose 1}}{52 choose 5} = frac{1}{649740}$ . . binomial(4,1)/binomial(52,5), 1/649740 . (1.5390771693292702e-6, 1.5390771693292702e-6) . Let $X$ be the number of royal flushes in $n$ hands. $X sim Poisson(k; lambda)$. . begin{align*} P(X = 0) &amp; leq frac{1}{e} e^{- lambda} &amp; leq e^{-1} lambda &amp; geq 1 n cdot frac{1}{649470} &amp; geq 1 n &amp; geq 649470 end{align*} . 16.A book of $n$ pages contains on the average $ lambda$ misprints per page. Estimate the probability that at least one page will contain more than $k$ misprints. . . Solution. Let $X$ be the number of misprints per page. $X sim Poisson(k; lambda)$. . begin{align*} P(X &gt; k) &amp;= e^{- lambda} left[ frac{ lambda^{k+1}}{(k+1)!} + ldots right] end{align*}This gives us the probability that a page will contain more than $k$ misprints. . Let us identify success as a page containing more than $k$ misprints. Let $Y$ be the number of pages containing more than $k$ misprints. Then, $Y sim Binomial(m;n,p)$ where $p = e^{- lambda} left[ frac{ lambda^{k+1}}{(k+1)!} + ldots right]$. The probability of atleast one success equals one minus the probability that no page contains more than $k$ misprints. . begin{align*} P(Y geq 1) &amp;= 1 - P(Y=0) &amp;= 1 - (1-p)^n end{align*} . 17.Suppose that there exist two kinds of stars(or raisins in a cake, or flaws in a material). The probability that a given volume contains $j$ stars of the first kind is $p(j;a)$ and the probability that it contains $k$ stars of the second kind is $p(k;b)$; the two events are assumed to be independent. Prove that the probability that the volume contains a total of $n$ stars is $p(n;a + b)$. (Interprete the assertion and the assumptions abstractly.) . . Solution. . Let $X$ be the number of stars of the first kind in a unit volume. $X sim Poisson(j;a)$. $a$ is the average density of the stars of the first kind in a unit volume. . Let $Y$ be the number of stars of the second kind in a unit volume. $X sim Poisson(k;b)$. $b$ is the average density of the stars of the second kind in a unit volume. . $X$ and $Y$ are independent Poisson r.v.&#39;s, the occurrence of the stars of one kind, reveals no additional information about the occurrence of the other kind. . We are actually interested to find the probability distribution of the total number of stars $X + Y$, in a unit volume. . begin{align*} P(X + Y = n) &amp;= sum_{j=0}^{n} P(X + Y = n|X=j) cdot P(X=j) &amp;= sum_{j=0}^{n} P(Y=n-j|X=j) cdot P(X=j) &amp;= sum_{j=0}^{n} P(Y=n-j) cdot P(X=j) quad { X text{ and } Y text{ are independent } } &amp;= sum_{j=0}^{n} e^{-a} cdot frac{a^{n-j}}{(n-j)!} cdot e^{-b} cdot frac{b^j}{j!} &amp;= frac{e^{-(a+b)}}{n!} sum_{j=0}^{n} {n choose j} a^{n-j} b^j &amp;= e^{-(a+b)} cdot frac{(a+b)^n}{n!} end{align*}Thus, $X + Y sim Poisson(n,a+b)$. . . 18.A traffic problem. The flow of traffic at a certain street crossing is described by saying that the probability of a car&#39;s passing during any given second is a constant $p$; and that there is no interaction between the passing of cars at different seconds. Treating seconds as indivisible time unit, the model of Bernoulli trials applies. Suppose that a pedestrian can cross the street only if no car is to pass during the next three seconds. Find the probability that the pedestrian has to wait for exactly $k=0,1,2,3,4$ seconds. (The corresponding general formulas are not obvious and will be derived in connection with the theory of success runs in a later topic.) . . Solution. . . 19.Two people toss a true coin $n$ times each. Find the probability that they will score the same number of heads. . . Solution. Let $X$ be the number of heads scored by the first person. And let $Y$ be the number of heads scored by the second person. We are interested to find the probability of the event $X=Y$. . begin{align*} P(X=Y) &amp;= P(X=Y|X=0) cdot P(X=0) + P(X=Y|X=1) cdot P(X = 1) + P(X = Y|X = 2) cdot P(X=2) + ldots &amp;= P(Y=0) cdot P(X = 0) + P(Y=1) cdot P(X=1) + P(Y=2) cdot P(X=2) &amp;= left( frac{1}{2} right)^{2n} left[{n choose 0}^2 + {n choose 1}^2 + {n choose 2}^2 + ldots right] end{align*}But we know using the story proof that, if we have $n$ white balls and $n$ black balls, then the number of ways of constructing a sample of size $n$ from a population of size $2n$ is, . begin{align*} {n choose 0}{n choose n} + {n choose 1}{n choose {n-1}} + {n choose 2}{n choose n-2} + ldots &amp;= {2n choose n} sum_k {n choose k}{n choose n - k} &amp;= {2n choose n} sum_k {n choose k}^2 = {2n choose n} end{align*} . 20.In a sequence of Bernoulli trials with probability $p$ for success, find the probability that $a$ successes will occur before $b$ failures. (Note: The issue is decided after at most $a+b-1$ trials. This probem playerd a role in the classical theory of games in connection with the question of how to divide the pot when the game is interrupted at a moment when one player lacks $a$ points to victory, the other $b$ points.) . . Solution. The probability that $a$ successes occur before $b$ failures is given by, . . 21.In Banach&#39;s match box problem find the probability that at the moment when the first box is emptied (not found empty) the other contains exactly $r$ matches (where $r = 1,2,3, ldots,N$) . . Solution. Let us identify success with drawing a match from the first box. We are interested to find the probability that the $N$th success occurs at the $(2N-r)$th trial. That, is $N-1$ successes must occur in the preceding $2N - r - 1$ trials. Therefore the desired probability is: . begin{align*} {{2N - r - 1} choose {N - 1}} left( frac{1}{2} right)^{N-1} left( frac{1}{2} right)^{N-r} end{align*} . We would like the $N$th success to occur at the $(2N-r)$th trial. That gives us, . begin{align*} {{2N - r - 1} choose {N - 1}} left( frac{1}{2} right)^{N} left( frac{1}{2} right)^{N-r} end{align*} . Because of symmetry, we can relabel the left box as the second box and right box as the first box. Hence, the desired probability is, . begin{align*} &amp;2{{2N - r - 1} choose {N - 1}} left( frac{1}{2} right)^{2N-r} =&amp;{{2N - r - 1} choose {N - 1}} left( frac{1}{2} right)^{2N-r-1} end{align*} . . 22.(Continuation) Using the preceding result, find the probability $x$ that the box first emptied is not the one first found to be empty. Show that the expression thus obtained reduces to $x = {2N choose N}2^{-2N-1}$ or $ frac{1}{2}(N pi)^{- frac{1}{2}}$, approximately. . . Solution. Let us identify success with selecting the first box. Let us label the match box in the left pocket as 1 and the right pocket as 2. We would like to ensure that there are no more $N$ successes in the first $2N-1$ trials. Thus, the first box is emptied (but not found empty). The $2N$th trial must be a failure, so the second box is emptied after the first. Further, the $(2N+1)$st trial must result in a failure. So, the box that was emptied first is not the first one to be found empty. Thus the required probability is, . begin{align*} {2N-1 choose N} left( frac{1}{2} right)^{2N-1} cdot left( frac{1}{2} right) cdot left( frac{1}{2} right) end{align*} . By symmetry, the same applies if we relabel the match box in the left pocket as 2 and the right pocket as 1. Thus, we have: . begin{align*} {2N-1 choose N} left( frac{1}{2} right)^{2N-1} cdot left( frac{1}{2} right) cdot left( frac{1}{2} right) end{align*} . The total probability is: . begin{align*} &amp;{2N-1 choose N} left( frac{1}{2} right)^{2N-1} cdot left( frac{1}{2} right) =&amp; frac{(2N-1)!}{N!(N-1)!} left( frac{1}{2} right)^{2N} =&amp; frac{1}{2} cdot frac{2N(2N-1)!}{N!N(N-1)!} left( frac{1}{2} right)^{2N} =&amp;{2N choose N} left( frac{1}{2} right)^{2N+1} =&amp;{2N choose N} 2^{-2N-1} end{align*} . . 23.Proofs in a certain book were read independenty by two proofreaders who found, respectively, $k_1$ and $k_2$ misprints; $k_{12}$ misprints were found by both. Give a reasonable estimate of the unknown number, $n$ of misprints in the proof. (Assume that proofreading corresponds to Bernoulli trials in which the two proof readers have, respectively, probabilities $p_1$ and $p_2$ of catching a misprint. Use the law of large numbers). . . Solution. . By the law of large numbers, if a large number of identical and independent Bernoulli trials are performed, the fraction of the number of successes $S_n$ in $n$ trials, $ frac{S_n}{n}$ nears $p$. Let $I_1$ be the indicator rv that the first proof reader catches a mispint and $I_2$ be the indicator rv that the second proof-reader catches a misprint. We have, . begin{align*} P(I_1 = 1) &amp;= frac{k_1}{n} P(I_2 = 1) &amp;= frac{k_2}{n} P(I_1 = 1, I_2 = 1) &amp;= frac{k_{12}}{n} end{align*}Since $ {I_1 = 1 }$ and $ {I_2 = 1 }$ are independent events, $P {I_1 = 1, I_2 = 1 } = P {I_1 = 1 } cdot P {I_2 = 1 }$. That gives us, . begin{align*} frac{k_{12}}{n} &amp;= frac{k_1 k_2}{n^2} n &amp;= frac{k_1 k_2}{k_{12}} end{align*} . 24.To estimate the size of an animal population by trapping, traps are set $r$ times in succession. Assuming that each animal has the same probability $q$ of being trapped, that originally there were $n$ animals in all, and that the only changes in the situation between successive settings of traps are that animals have been trapped (and thus removed); find the probability that the $r$ trappings yield, repectively $n_1,n_2 ldots,n_r$ animals. . . If $n_1$ animals are trapped by the first trial, they are removed and the size of the animal population for the second trial is $n - n_1$. So, the $r$ trappings are not independent. We are sampling without replacement. . Let . $X_1$ be the number of animals trapped in the first trial, . $X_2$ be the number of animals trapped in the second trial, . $ vdots$ . $X_r$ be the number of animals trapped in the $r$th trial. . begin{align*} P {X_1 = n_1, X_2 = n_2, ldots, X_r= n_r } &amp;= P {X_1 = n_1 } cdot P {X_2 = n_2 | X_1 = n_1 } cdots P {X_r = n_r | X_1 = n_1, X_2 =n_2 ldots X_{r-1} = n_{r-1} } &amp;= {n choose n_1} q^{n_1}(1-q)^{n-n_1} {n-n_1 choose n_2} q^{n_2}(1-q)^{n-n_1-n_2} cdots &amp;= frac{n!}{n_1!n_2! dots n_r!} q^{n_1 + n_2 + ldots + n_r} (1-q)^{n-(n_1 + n_2 + ldots + n_r)} end{align*} . . 25.People are arriving at a party one at a time. While waiting for more people to arrive they entertain themselves by comparing their birthdays. Let $X$ be the number people needed to obtain a birthday match i.e. before person $X$ arrives, no two people have the same birthday, but when person $X$ arrives there is a match. Find the PMF of $X$. . . Solution. . We are interested to form an ordered $k$-tuple from the population $ {1,2,3, ldots,365 }$. There&#39;s a total of $365^{k-1}$ birthday-tuples of length $(k-1)$. Each sample is equally likely. There are $(365)_{k-1}$ distinguishable birthday-tuples that can be formed, if we sample without replacement, that is, no birthday is repeated. Therefore, the probability that no two people share the same birthday amongst a group of $(k-1)$ people is, . begin{align*} frac{(365)_{k-1}}{365^{k-1}} end{align*}The $k$-th person can have his birthday fall on $k-1$ out of $365$ days, so that when he arrives there is a match. So, the PMF of $X$ is, . begin{align*} P(X = k) = frac{(k-1)(365)_{k-1}}{365^k} end{align*} . 26.(a)Independent Bernoulli trials are performed, with probability $1/2$ of success, until there has been atleast one success. Find the PMF of the number of trials performed. . (b)Independent Bernoulli trials are performed, with probability $1/2$ of success, until there has been at least one success and at least one failure. Find the PMF of the number of trials performed. . . Solution. . Let $X$ be the waiting time until the first success. We would have to wait for $r$ trials, if there is a string of $(r-1)$ failures and the first success occurs at the $r$th trial; that is, we are looking at the sequence $ underbrace{FFF ldots F}_{r-1 text{ trials }}S$. Therefore, the probability that we need to wait till time $r$ to observe the first success is, . begin{align*} P(X=r) &amp;= left( frac{1}{2} right)^{r-1} left( frac{1}{2} right) = 2^{-r} end{align*} Let $Y$ be the waiting time until atleast one success and atleast one failure. We would have to wait for $r$ trials, if there is a string of $(r-1)$ failures followed by a success at the $r$th trial, or there is a string of $(r-1)$ successes followed by a failure at the $r$th trial. Therefore, the probability that we must wait until time $r$ is, . begin{align*} P(Y=r) &amp;= 2 cdot 2^{-r} = 2^{-r+1} end{align*} . 27.Let $X$ be an r.v. with CDF $F$, and $Y = mu + sigma X$, where $ mu$ and $ sigma$ are real numbers with $ sigma &gt; 0$. (Then, $Y$ is called the location-scale transformation of $X$; we will encounter this concept many times in chapter 5 and beyond). Find the CDF of $Y$ in terms of $F$. . . Solution. . begin{align*} F_Y(y) &amp;= P(Y leq y) &amp;= P( mu + sigma X leq y) &amp;= P( sigma X leq y - mu) &amp;= P left {X leq frac{y - mu}{ sigma} right } &amp;= F_X left( frac{y - mu}{ sigma} right) end{align*} . . 28.Let $n$ be a positive integer and begin{align*} F(x) &amp;= frac{[[x]]}{n} end{align*} . for $0 leq x leq n$, $F(x) = 0$ for $x &lt; 0$ and $F(x) = 1$ for $x &gt; n$, where $[[x]]$ is greater integer less than or equal to $x$. Show that $F$ is a CDF and find the PMF that it corresponds to. . . Solution. . We assume that $F$ is the CDF of a discrete random variable. . (i) Non-decreasing. If $x_1 &gt; x_2$, then $ frac{[[x_1]]}{n} geq frac{[[x_2]]}{n} $, so $F_X(x_1) geq F_X(x_2)$. . (ii) Right continuous. . Let $k in mathbf{N}$. Let&#39;s prove that the function $f(x)=[[x]]$ is right-continuous at $k$. . Consider an arbitrary sequence $(x_n) to k$, such that $x_n &gt; k$. Since, $(x_n) to k$, for all $ delta &gt; 0$, there exists $N in mathbf{N}$, such that $|x_n - k|&lt; delta$ for all $n geq N$. Pick $ delta &gt; 1$. Then, there exists $N$, such that $x_n in (k,k+1)$ for all $n geq N$. . Let $ epsilon &gt; 0$ be arbitrary. Then, for all $x_n in (k,k+1)$, $|f(x_n) - f(k)| = |[[x_n]]-[[k]]|=k - k = 0 &lt; epsilon$. So, for all $ epsilon &gt; 0$, there exists $ delta &gt; 0$, such that $|f(x_n) - f(k)| &lt; epsilon$. Thus, $f(x_n) to f(k)$. Since, $(x_n)$ was arbitrary, this is true for all sequences $(x_n) to k$. Thus, $f(x) =[[x]]$ is right-continuous at $k in mathbf{N}$. . Moreover, $f(x) = [[x]]$ is continuous at any point $x=c$ where $c in [k+ xi,k+1)$, $k in mathbf{N}$, $0&lt; xi&lt;1$. . Therefore, $ lim_{x to a} F_X(x) = lim_{x to a} frac{[[x]]}{n} = frac{[[a]]}{n} = F_X(a)$. . (iii) $ lim_{x to - infty} F_X(x) = 0$, since $F_X(x) = 0$ for $x &lt; 0$. Also, $ lim_{x to infty} F_X(x) = 1$, since $F_X(x) = 1$ for $x &gt; n$. . Thus, $F$ is a CDF. . Let $k in mathbf{N}$. The PMF of $X$ is given by : . begin{align*} P(X = k) &amp;= F_X(k + 1) - F_X(k) &amp;= frac{k+1}{n} - frac{k}{n} &amp;= frac{1}{n} end{align*} . 29.(a) Show that $p(n) = left( frac{1}{2} right)^{n+1}$ for $n=0,1,2, ldots$ is a valid PMF for a discrete r.v. . (b) Find the CDF of a random variable with the PMF from (a). . . Solution. . (a) Clearly, $p(n) geq 0$ for all $n=0,1,2, ldots$. So, the probabilities are non-negative. . Moreover, $ sum_{n=0}^{ infty}p(n) = sum_{n=0}^{ infty} left( frac{1}{2} right)^{n+1} = frac{1}{2} sum_{n=0}^{ infty} left( frac{1}{2} right)^n = frac{1}{2} cdot frac{1}{1- frac{1}{2}}=1$. . Hence, $p(n)$ is a valid PMF. . (b) The CDF of $X$ is given by . begin{align*} P(X leq x) &amp;= left( frac{1}{2} right) sum_{n=0}^{x} left( frac{1}{2} right)^x &amp;= frac{1}{2} cdot frac{1- left( frac{1}{2} right)^{x+1}}{ left(1 - frac{1}{2} right)} F_X(x)&amp;=1- left( frac{1}{2} right)^{x+1} end{align*} . 30.Benford&#39;s law states that in a very large variety of real-life datasets, the first digit approximately follows a particular distribution with about a $30$ % chance of a $1$, an $18$ % chance of a $2$, and in general . begin{align*} P(D=j)= log_{10} left( frac{j+1}{j} right), quad text{ for } j in {1,2,3, ldots,9 } end{align*}where $D$ is the first digit of a randomly chosen element. Check that this is indeed a valid PMF(using properties of logs). . . Solution. . begin{align*} log_{10}x geq 0 quad text{ for all } x geq 1 end{align*}so the PMF takes non-negative values. . Moreover, . begin{align*} sum_{j=1}^{9}P(D=j) &amp;= sum_{j=1}^{9} log_{10} left( frac{j+1}{j} right) &amp;= log_{10} left( frac{2}{1} right) + log_{10} left( frac{3}{2} right) + log_{10} left( frac{4}{3} right) + ldots + log_{10} left( frac{10}{9} right) &amp;= log_{10} left( frac{2}{1} cdot frac{3}{2} cdot frac{4}{3} cdots frac{10}{9} right) &amp;= log_{10} 10 &amp;= 1 end{align*} . 31.Bob is playing a video-game that has $7$ levels. He starts at level $1$ and has the probability $p_1$ of reaching level $2$. In general, given that he reaches level $j$, he has the probability $p_j$ of reaching level $j+1$, for $1 leq j leq 6$. Let $X$ be the highest level that he reaches. Find the PMF of $X$ (in terms of $p_1,p_2, ldots,p_6$). . . Solution. begin{align*} P(X = j) = prod_{i=1,i leq j}p_i end{align*} . . 32.There are $100$ prizes with one worth $1$ dollar, one worth $2$ dollars, $ ldots$ and one worth $100$ dollars. There are $100$ boxes, each of which contains one of the prizes. You get $5$ prizes by picking random boxes one at a time, without replacement. Find the PMF of how much your most valuable prize is worth (as a simple expression in terms of binomial coefficients). . . Solution. . If the value of the highest prize is $x$, the remaining $4$ prizes must be chosen from $(x-1)$ random boxes. . begin{align*} P(X =x) &amp;= frac{{x-1} choose 4}{100 choose 5} end{align*} .",
            "url": "https://quantophile.github.io/mathematical-finance/jupyter/2022/03/06/Binomial-And-Poisson-Exercise-Set.html",
            "relUrl": "/jupyter/2022/03/06/Binomial-And-Poisson-Exercise-Set.html",
            "date": " • Mar 6, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://quantophile.github.io/mathematical-finance/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://quantophile.github.io/mathematical-finance/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://quantophile.github.io/mathematical-finance/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://quantophile.github.io/mathematical-finance/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}