<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>The Binomial and Poisson Distribution. | Summary notes on Mathematical Finance</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="The Binomial and Poisson Distribution." />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Summary notes on the Binomial and Poisson Distribution." />
<meta property="og:description" content="Summary notes on the Binomial and Poisson Distribution." />
<link rel="canonical" href="https://quantophile.github.io/mathematical-finance/probability-theory/2022/03/06/The-Binomial-and-Poisson-Distributions-(summary).html" />
<meta property="og:url" content="https://quantophile.github.io/mathematical-finance/probability-theory/2022/03/06/The-Binomial-and-Poisson-Distributions-(summary).html" />
<meta property="og:site_name" content="Summary notes on Mathematical Finance" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-03-06T00:00:00-06:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="The Binomial and Poisson Distribution." />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-03-06T00:00:00-06:00","datePublished":"2022-03-06T00:00:00-06:00","description":"Summary notes on the Binomial and Poisson Distribution.","headline":"The Binomial and Poisson Distribution.","mainEntityOfPage":{"@type":"WebPage","@id":"https://quantophile.github.io/mathematical-finance/probability-theory/2022/03/06/The-Binomial-and-Poisson-Distributions-(summary).html"},"url":"https://quantophile.github.io/mathematical-finance/probability-theory/2022/03/06/The-Binomial-and-Poisson-Distributions-(summary).html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/mathematical-finance/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://quantophile.github.io/mathematical-finance/feed.xml" title="Summary notes on Mathematical Finance" /><link rel="shortcut icon" type="image/x-icon" href="/mathematical-finance/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/mathematical-finance/">Summary notes on Mathematical Finance</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/mathematical-finance/about/">About Me</a><a class="page-link" href="/mathematical-finance/search/">Search</a><a class="page-link" href="/mathematical-finance/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">The Binomial and Poisson Distribution.</h1><p class="page-description">Summary notes on the Binomial and Poisson Distribution.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-03-06T00:00:00-06:00" itemprop="datePublished">
        Mar 6, 2022
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      37 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/mathematical-finance/categories/#probability-theory">probability-theory</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/quantophile/mathematical-finance/tree/master/_notebooks/2022-03-06-The-Binomial-and-Poisson-Distributions-(summary).ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/mathematical-finance/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/quantophile/mathematical-finance/master?filepath=_notebooks%2F2022-03-06-The-Binomial-and-Poisson-Distributions-%28summary%29.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/mathematical-finance/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/quantophile/mathematical-finance/blob/master/_notebooks/2022-03-06-The-Binomial-and-Poisson-Distributions-(summary).ipynb" target="_blank">
        <img class="notebook-badge-image" src="/mathematical-finance/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fquantophile%2Fmathematical-finance%2Fblob%2Fmaster%2F_notebooks%2F2022-03-06-The-Binomial-and-Poisson-Distributions-%28summary%29.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/mathematical-finance/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#Bernoulli-Trials">Bernoulli Trials </a></li>
<li class="toc-entry toc-h2"><a href="#The-Binomial-Distribution">The Binomial Distribution </a></li>
<li class="toc-entry toc-h2"><a href="#The-Central-Term-and-the-tails">The Central Term and the tails </a></li>
<li class="toc-entry toc-h2"><a href="#The-Law-Of-Large-Numbers">The Law Of Large Numbers </a></li>
<li class="toc-entry toc-h2"><a href="#The-Poisson-Approximation">The Poisson Approximation </a></li>
<li class="toc-entry toc-h2"><a href="#The-Poisson-Distribution">The Poisson Distribution </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Spatial-Distributions.">Spatial Distributions. </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Observations-Fitting-the-Poisson-Distribution.">Observations Fitting the Poisson Distribution. </a></li>
<li class="toc-entry toc-h2"><a href="#Waiting-times.-The-negative-Binomial-Distribution.">Waiting times. The negative Binomial Distribution. </a></li>
<li class="toc-entry toc-h2"><a href="#The-Hypergeometric-distribution.">The Hypergeometric distribution. </a></li>
<li class="toc-entry toc-h2"><a href="#Discrete-Uniform-distribution.">Discrete Uniform distribution. </a></li>
<li class="toc-entry toc-h2"><a href="#Cumulative-Distribution-Functions.">Cumulative Distribution Functions. </a></li>
</ul><body>
<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-03-06-The-Binomial-and-Poisson-Distributions-(summary).ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Bernoulli-Trials">
<a class="anchor" href="#Bernoulli-Trials" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bernoulli Trials<a class="anchor-link" href="#Bernoulli-Trials"> </a>
</h2>
<p>Repeated independent trials are called Bernoulli trials if there are only two possible outcomes for each trial and their probabilities remain the same throughout the trials. It is usual to denote the two probabilities by $p$ and $q$, and to refer to the outcome with probability $p$ as success, $S$, and to the other as failure, $F$. Clearly, $p$ and $q$ must be non-negative and</p>
\begin{align*}
p + q = 1
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The sample space of each individual trial is formed by the two points $S$ and $F$. The sample space of $n$ Bernoulli trials contains $2^n$ points or successions of $n$ symbols $S$ and $F$, each point representing one possible outcome of the compound experiment. Since the trials are independent, the probabilities multiply. In other words, <em>the probability of any specified sequence is the product obtained on replacing the symbols $S$ and $F$ by $p$ and $q$</em> respectively.  Thus, $P\{SSFSF \ldots FFS\} = ppqpq\ldots qqp$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Binomial-Distribution">
<a class="anchor" href="#The-Binomial-Distribution" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Binomial Distribution<a class="anchor-link" href="#The-Binomial-Distribution"> </a>
</h2>
<p>Frequently, we are interested only in the total number of successes produced in a succession of $n$ Bernoulli trials but not in their order. The number of successes can be $0,1,2\ldots,n$ and our first problem is to determine the corresponding probabilities. Now, the event $n$ trials result in $k$ successes and $n-k$ failures can happen in as many ways as $k$ letters $S$ can be distributed among $n$ places. In other words, our event contains ${n \choose k}$ points, and, by definition, each point has the probability $p^k q^{n-k}$. This proves the</p>
<hr>
<p><strong>Theorem.</strong> Let $b(k;n,p)$ be the probability that $n$ Bernoulli trails with probabilities $p$ for success and $q = 1-p$ for failure result in $k$ successes and $n-k$ failures. Then,</p>
<p>
\begin{equation*}
b(k;n,p) = {n \choose k} p^k q^{n - k} \tag{1}
\end{equation*}
</p>
<hr>
<p>In particular, the probability of no success is $q^n$ and the probability of atleast one success is $1-q^n$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We shall treat $p$ as a constant and denote the number of successes in $n$ trilas by $S_n$; then $b(k;n,p) = P\{S_n = k\}$. In the general terminology, $S_n$ is a random variable, and the function $b(l;n,p)$ is the PMF of this random variable; we shall refer to it as the binomial PMF. The attribute binomial refers to the fact that equation (1) represents the $k$th term of the binomial expansion of $(q+p)^n$. This remark also shows that</p>
\begin{align*}
b(0;n,p) + b(1;n,p) + \ldots + b(n;n,p) = (q+p)^n = 1
\end{align*}<p>as is required by the notion of probability.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Central-Term-and-the-tails">
<a class="anchor" href="#The-Central-Term-and-the-tails" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Central Term and the tails<a class="anchor-link" href="#The-Central-Term-and-the-tails"> </a>
</h2>
<p>From equation (1), we see that</p>
<p>
\begin{align*}
\frac{b(k;n,p)}{b(k-1;n,p)} &amp;= \frac{{n \choose k}p^k q^{n-k}}{{n \choose {k-1}}p^{k-1} q^{n-k+1}} = \frac{\frac{n!}{k!(n-k)!}\cdot p}{\frac{n!}{(k-1)!(n-k+1)!}\cdot q}\\
&amp;= \frac{p}{q} \left(\frac{n - k + 1}{k}\right) = \frac{(n+1)p - pk + qk - qk}{qk}\\
&amp;= 1 + \frac{(n+1)p - k}{qk}
\end{align*}
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Accordingly, the term $b(k;n,p)$ is greater than the preceding one for $(n+1)p &gt; k$, that is $k &lt; (n+1)p$ and is small for $k &gt; (n+1)p$. If $(n+1)p = m$ happens to be an integer, then $b(m;n,p) = b(m-1;n,p)$. Thus, there exists exactly one integer $m$ such that,</p>
\begin{equation*}
(n+1)p - 1 &lt; m \le (n+1)p \tag{2}
\end{equation*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Theorem.</strong> As $k$ goes from $0$ to $n$, the terms $b(k;n,p)$ first inrease monotonically, then decrease monotonically, reaching their greatest value when $k = m$, except that $b(m-1;n,p) = b(m;n,p)$ when $m = (n+1)p$.</p>
<hr>
<p>We shall call $b(m;n,p)$ the <em>central</em> term. Often $m$ is called the most probable number of successes, but it must be understood that for large values of $n$, all terms $b(k;n,p)$ are small. In 100 tossings of a true coin, the most probable number of heads is 50, but its probability is less than $0.09$. In the next chapter we shall find that $b(m;n,p)$ is approximately $1/\sqrt{2\pi npq}$.</p>
<p>Consider the sequence $(a_k)$ whose terms are ratio of the binomial coefficients $\frac{b(k;n,p)}{b(k-1;n,p)}$, we saw above. We have, $a_k = \frac{(n+1)p}{kq}-\frac{p}{q}$. Clearly therefore, as $k$ increases, the ratio $a_k$ decreases monotonically.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If $k \geq (r+1)$, then, $(n+1-k)p \leq (n+1-(r+1))p = (n-r)p$. And $qk \geq q(r+1)$. So, $\frac{1}{qk} \leq \frac{1}{(r+1)q}$. So, we have an upper bound on $\frac{b(k;n,p)}{b(k-1;n,p)}$.</p>
<p>When $k \geq (r+1)$,</p>
\begin{equation*}
\frac{b(k;n,p)}{b(k-1;n,p)} \leq \frac{(n-r)p}{(r+1)q} \tag{3}
\end{equation*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Set herein $k=r+1,\ldots,r+\nu$ and multiply the $\nu$ inequalities to obtain</p>
<p>
\begin{align*}
\frac{b(r+\nu;n,p)}{b(r+\nu-1;n,p)} \cdot \frac{b(r+\nu-1;n,p)}{b(r+\nu-2;n,p)} \cdots \frac{b(r+1;n,p)}{b(r;n,p)} \le \left\{\frac{(n-r)p}{(r+1)q}\right\}^{\nu}
\end{align*}
</p>
<p>Consequently,</p>
<p>
\begin{equation*}
\frac{b(r+\nu;n,p)}{b(r;n,p)} \leq \left\{\frac{(n-r)p}{(r+1)q}\right\}^{\nu} \tag{4}
\end{equation*}
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For $r \geq np$, let's find an upper bound for the fraction $\frac{(n-r)p}{(r+1)q}$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Clearly, if $r \geq np$, $(n-r)p \leq (n-np)p = npq$. So,</p>
\begin{align*}
\frac{(n-r)p}{(r+1)q} \leq \frac{npq}{(r+1)q} = \frac{np}{r+1}
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Moreover, we want a lower bound on the denominator. $(r+1) \geq (np + 1)$. So,</p>
\begin{align*}
\frac{(n-r)p}{(r+1)q} \leq \frac{np}{np+1} = 1 - \frac{1}{np + 1}
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So, for $r \geq np$, the fraction $\frac{(n-r)p}{(r+1)q}$ is strictly less than unity, and the summation over $\nu$ leads to a finite geometric series with the ratio $\frac{(n-r)p}{(r+1)q}$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>
\begin{align*}
\sum_{\nu = 0}^{(n-r)} \left(\frac{(n-r)p}{(r+1)q}\right)^{\nu} &amp;\leq \sum_{\nu = 0}^{\infty} \left(\frac{(n-r)p}{(r+1)q}\right)^{\nu} = \frac{1}{1-\frac{(n-r)p}{(r+1)q}} \\
&amp;= \frac{(r+1)q}{(r+1)q-(n-r)p}  \\
&amp;= \frac{(r+1)q}{(r+1)q-((n+1)-(r+1))p} = \frac{(r+1)q}{(r+1)q-(n+1)p+(r+1)p}\\
&amp;= \frac{(r+1)q}{(r+1)-(n+1)p}
\end{align*}
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We conclude that for $r \geq np$,</p>
<p>
\begin{align*}
\sum_{\nu=0}^{(n-r)} \frac{b(r+\nu;n,p)}{b(r;n,p)} \leq \frac{(r+1)q}{(r+1)-(n+1)p}
\end{align*}
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Thus,</p>
<p>
\begin{equation*}
\sum_{\nu=0}^{(n-r)}b(r+\nu;n,p) \leq b(r;n,p)\cdot\frac{(r+1)q}{(r+1)-(n+1)p} \tag{5}
\end{equation*}
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>On the left, we have the <strong>right tail</strong> of the binomial distribution, namely the probability of atleast $r$ successes. The same calculation applied to the left tail shows that for $s \leq np$</p>
<p>
\begin{equation*}
\sum_{\rho=0}^{s}b(\rho;n,p) \leq b(s;n,p)\cdot\frac{(n-s+1)p}{(n+1)p - s} \tag{6}
\end{equation*}
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Theorem.</strong> If $r \geq np$, the probability of atleast $r$ successes satisfies the inequality (5); if $s \leq np$, the probability of at most $s$ successes satisfies inequality (6).</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Law-Of-Large-Numbers">
<a class="anchor" href="#The-Law-Of-Large-Numbers" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Law Of Large Numbers<a class="anchor-link" href="#The-Law-Of-Large-Numbers"> </a>
</h2>
<p>On several occasions, we have mentioned that our intuitive notion of probability is based on the following assumption. If in $n$ identical trails, $A$ occurs $\nu$ times, and if $n$ is very large, then $\nu/n$ should be near the probability of $A$. Clearly, a formal mathematical theory can never refer directly to real life, but it should atleast provie theoretical counterparts to the phenomena, which it tries to explain. Accordingly, we require that the vague introductory remark be made precise in the form a theorem. For this purpose we translate <em>identical trials</em> as <em>Bernoulli trials</em> with probability $p$ for success. If $S_n$ is the number of successes in $n$ trials, then $S_n/n$ is the average number of successes and should be near $p$. It is now easy to give a precise meaning to this. Pick an arbitrary $\epsilon &gt; 0$. Consider for example, the probability that $\frac{S_n}{n} - p$ exceeds $\epsilon$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This probability is the same as $P\{S_n &gt; n(p + \epsilon)\}$ and equals the left side of the inequality (5), when $r \geq n(p + \epsilon)$. Then, we can find an upper bound for the fraction $\frac{(r+1)q}{(r+1)-(n+1)p}$. Since $r \leq n$, we have:</p>
<p>
\begin{align*}
\frac{(r+1)q}{(r+1)-(n+1)p} &amp;\leq \frac{(n+1)q}{(n(p+\epsilon)+1)-(n+1)p}\\
&amp;=\frac{(n+1)q}{np + n\epsilon + 1 - np - p} \\
&amp;=\frac{(n+1)q}{n\epsilon + q}
\end{align*}
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So, we proved that the $n$th term of the sequence $(p_n)$ has an upper bound:</p>
\begin{align*}
P\{S_n &gt; n(p + \epsilon)\} \leq b(r;n,p) \cdot \frac{(n+1)q}{n\epsilon + q}
\end{align*}<p>Since probabilities are non-negative,</p>
\begin{align*}
0 \leq P\{S_n &gt; n(p + \epsilon)\} \leq b(r;n,p) \cdot \frac{(n+1)q}{n\epsilon + q}
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Passing to the limit, as $n \to \infty$, we have:</p>
\begin{align*}
\lim 0 \leq \lim P\{S_n &gt; n(p + \epsilon)\} \leq \lim \left[ b(r;n,p) \cdot \frac{(n+1)q}{n\epsilon + q}\right]
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now,</p>
\begin{align*}
\lim \left[b(r;n,p) \cdot \frac{(n+1)q}{n\epsilon + q}\right] &amp;= \lim b(r;n,p) \cdot \lim \frac{\left(1+\frac{1}{n}\right)q}{\epsilon + \frac{q}{n}}\\
&amp;= \lim b(r;n,p) \cdot \frac{q}{\epsilon}
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It's easy to see, that as $n \to \infty$, $b(r;n,p) \to 0$. Note that, as $n \to \infty$, $r \to \infty$, because $r \geq n(p + \epsilon)$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
\begin{align*}
\lim b(r;n,p) &amp;= \lim \frac{n!}{(n-r)!r!} \cdot \lim p^r q^{n-r}\\
&amp;\leq \lim \frac{n!}{n!0!} \cdot \lim p^n q^n \quad \{ r \ge 0 \text{ and } r \leq n \} \\
&amp;= \lim p^n q^n \{ \text{ since } (q^n) \to 0, \text{ if } |q|&lt;1 \}\\
&amp;= 0
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By the Squeeze theorem,</p>
\begin{align*}
\lim P\{S_n &gt; n(p + \epsilon)\} = 0
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using the inequality (6), we see in the same way that $P\{S_n &lt; n(p - \epsilon)\} \to 0$, and we have thus</p>
\begin{align*}
P\left\{\left\lvert\frac{S_n}{n} - p\right\rvert &lt; \epsilon\right\} \to 1 \tag{8}
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>In words: As $n$ increases, the probability that the average number of successes deviates from $p$ by more than any preassigned $\epsilon$ tends to zero.</em> This is one form of the law of large numbers and serves as a basis for the intuitive notion of probability as a measure of relative frequencies. For practical applications it must be supplemented by a more precise estimate of the probability on the left side in (8); such an estimate is provided by the normal approximation to the binomial distribution.</p>
<p>The assertion (8) is the classical law of large numbers. It is of very limited interest and should be replaced by the more previse and more useful strong law of large numbers.</p>
<p><em>Warning.</em> It is usual to read into the law of large numbers things which it definitely does not imply. If Peter and Paul toss a perfect coin 10,000 times, it is customary to expect that Peter will lead roughly half the time. <strong>This is not true.</strong> The arc sine law states that such an equalization is least probable. The probability that Peter leads in less than $20$ trials is very much larger than the probability that the number of trials in which he leads lies between 4990 and 5010. There does not exist any tendency for the periods of lead to equalize. The law of large numbers asserts only that in a a large number of different coin tossing games, the frequency of those in which heads lead is, at any given moment, close to $\frac{1}{2}$. Nothing is said about the fluctuations of the lead within a fixed game.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Poisson-Approximation">
<a class="anchor" href="#The-Poisson-Approximation" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Poisson Approximation<a class="anchor-link" href="#The-Poisson-Approximation"> </a>
</h2>
<p>In many applications, we deal with Bernoulli trials where, comparatively speaking $n$ is large and $p$ is small, whereas the product</p>
\begin{align*}
\lambda = np \tag{9}
\end{align*}<p>is of modertate magnitude. In such cases, it is convenient to use an approximation formula to $b(k;n,p)$ which is due to Poisson and which we proceed to derive. We have $b(0;n,p) = (1 - p)^n$ or substituting from (9),</p>
\begin{equation*}
b(0;n,p) = \left(1 - \frac{\lambda}{n}\right)^n \tag{10}
\end{equation*}<p>Taking the logarithm on both sides:</p>
\begin{equation*}
\log b(0;n,p) = n \log\left(1 - \frac{\lambda}{n}\right)
\end{equation*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Taylor's series for $\log (1 + x)$ at $x = 0$ is given by:</p>
\begin{align*}
\log (1 + x) &amp;= f(0) + xf'(0) + \frac{x^2}{2!}f''(0) + \frac{x^3}{3!}f'''(0) + \ldots\\
&amp;= x + \frac{x^2}{2!}\cdot (-1) + \frac{x^3}{3!} \cdot 2 + \frac{x^4}{4!}\cdot (-3 \cdot 2 \cdot 1) \\
&amp;= x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \ldots
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So,</p>
\begin{align*}
\log b(0;n,p) &amp;= n \log\left(1- \frac{\lambda}{n}\right) = n\left(-\frac{\lambda}{n}-\frac{\lambda^2}{2n^2}-\ldots\right)\\
&amp;= -\lambda - \frac{\lambda^2}{2n} - \frac{\lambda^3}{3n^2} - \ldots \tag{11}
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As $n \to \infty$, $\lim \log b(0;n,p) = - \lambda$, so that for large $n$,</p>
\begin{align*}
b(0;n,p) \approx e^{-\lambda} \tag{12}
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>where the sign $\approx$ is used to indicate approximate equality. Furthermore, from the expression for $b(k;n,p)/b(k-1;n,p)$, it is seen that for any fixed $k$ and sufficiently large $n$ we have,</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>
\begin{align*}
\frac{b(k;n,p)}{b(k-1;n,p)} &amp;= \frac{(n-k+1)p}{kq}\\
&amp;=\frac{np-(k-1)p}{kq}\\
&amp;= \frac{\lambda - (k-1)p}{kq}
\end{align*}
</p>
<p>For very small values of $p$, we can write $p \approx 0$ and $q \approx 1$. Thus,</p>
<p>
\begin{align*}
\frac{b(k;n,p)}{b(k-1;n,p)} \approx \frac{\lambda}{k} \tag{13}
\end{align*}
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For $k=1$, we get from this and (12), $b(1;n,p)\approx \lambda e^{-\lambda}$. For $k=2$, we get $b(2;n,p) \approx \frac{\lambda^2}{2}e^{-\lambda}$. Generally, we see by induction that,</p>
\begin{equation*}
b(k;n,p) \approx \frac{\lambda^k}{k!} e^{-\lambda} \tag{14}
\end{equation*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is the famous <em>Poisson approximation to the binomial distribution</em>. (See problems 30-34 for an estimate of the error and a proof that the approximation in (14) is uniform when $n \to \infty$ and $p \to 0$ in such a way that $\lambda = np$ remains bounded.) It is convenient to have a symbol for the right-hand member in (14) and we shall put</p>
\begin{align*}
p(k;\lambda) = \frac{\lambda^k}{k!}e^{-\lambda} \tag{15}
\end{align*}<p>With this notation $p(k;\lambda)$ should be an approximation to $b(k;n,\lambda/n)$ when $n$ is sufficiently large.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="k">function</span> <span class="n">p</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">Î»</span><span class="p">)</span>
    <span class="p">((</span><span class="n">Î»</span><span class="o">^</span><span class="n">k</span><span class="p">)</span><span class="o">/</span><span class="n">factorial</span><span class="p">(</span><span class="n">k</span><span class="p">))</span> <span class="o">*</span> <span class="n">â„¯</span><span class="o">^</span><span class="p">(</span><span class="o">-</span><span class="n">Î»</span><span class="p">)</span>
<span class="k">end</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>p (generic function with 1 method)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Birthdays</em>. What is the probability, $p_k$, that in a company of $500$ people exactly $k$ will have birthdays on New Year's day?</p>
<p>If the $500$ people are cosen at random, we may apply the scheme of 500 Bernoulli trials with the probability of success $p = \frac{1}{365}$. Then, $p_0 = \left(\frac{364}{365}\right)^{500} \approx 0.2537$. For the Poisson approximation, we put $\lambda = \frac{500}{365}$. Then,</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="k">using</span> <span class="n">Printf</span>

<span class="n">Î»</span> <span class="o">=</span> <span class="mi">500</span><span class="o">/</span><span class="mi">365</span>

<span class="k">for</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="o">:</span><span class="mi">6</span>
    <span class="nd">@printf</span><span class="p">(</span><span class="s">"p(</span><span class="si">%d</span><span class="s">,</span><span class="si">%f</span><span class="s">) = </span><span class="si">%f</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="n">Î»</span><span class="p">,</span><span class="n">p</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">Î»</span><span class="p">))</span>
<span class="k">end</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>p(0,1.369863) = 0.254142
p(1,1.369863) = 0.348139
p(2,1.369863) = 0.238452
p(3,1.369863) = 0.108882
p(4,1.369863) = 0.037288
p(5,1.369863) = 0.010216
p(6,1.369863) = 0.002332
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Centenarians.</em> At birth any particular person has a small chance of living 100 years, and in a large community the number of yearly births are large. Owing to wars, epidemics etc. different lives are not stochastically independent, but as a first approximation we may compare $n$ births to $n$ Bernoulli trials with death after $100$ years as success. In a stable community, where neither size nor mortality rate appreciably, it is reasonable to expect that the frequency of years in which exactly $k$ centenarians die is approximately $p(k;\lambda)$, with $\lambda$ depending on the size and health of the community.</p>
<p>Intuitively, there will be years in which exactly $1$ centenarian dies, there will be years in which $2$ centenarians die and there will be years in which exactly $k$ centenarians die. $p(k,\lambda)$ is on an average the count(number) of 365-day years in which $k$ centenarians die.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Misprints, Raisins</em>. If in printing a book there is a constant probability of any letter's being misprinted, and if the conditions of printing remain unchanged, then we have as many Bernoulli trials as there are letters. The frequency of the pages containing exactly $k$ misprints will then be approximately $p(k;\lambda)$ where $\lambda$ is the characteristic of the printer. Thus, the Poisson formula may be used to discover radical departures from uniformity or from the state of statistical control. A similar agrument applies in many cases. For example, if many raisins are distributed in the dough, we should expect that thorough mixing will result in the frequency of loaves with exactly $k$ raisin to be approximately $p(k;\lambda)$ with $\lambda$ a measure of the density of raisins in the dough.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Poisson-Distribution">
<a class="anchor" href="#The-Poisson-Distribution" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Poisson Distribution<a class="anchor-link" href="#The-Poisson-Distribution"> </a>
</h2>
<p>In the preceding section, we have used the Poisson expression merely as a convenient approximation to the binomial distribution in the case of large $n$ and small $p$. In connection with the matching and occupany problems of chapter IV, we have studied different probability distributions, which have also led to the Poisson expressions $p(k;\lambda)$ as a limiting form. We have here a special case of the remarkable fact that there exist a few distributions of great universaility that occur in a surprisingly large variety of problems. The three principal distributions, with ramifications throughout probability theory are the binomial distribution, the normal distribution (to be introduced in the following chapter), and the Poisson distribution</p>
\begin{align*}
p(k;\lambda) = \frac{\lambda^k}{k!}e^{-\lambda} \tag{16}
\end{align*}<p>which we shall now consider on its own merits.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We note first that on adding the equations (16) for $k = 0,1,2,\ldots$ we get on the right side $e^{-\lambda}$ times the Taylor's series for $e^\lambda$. Hence, for any fixed $\lambda$, the quantities $p(k;\lambda)$ add to unity, and therefore it is possible to conceive of an ideal experiment in which $p(k;\lambda)$ is the probability of exactly $k$ successes. We shall now indicate by many physical experiments and statistical observations actually lead to such an interpretation of (16). The examples of the next section will illustrate the wide range and the importance of the various applications of (16). The true nature of the Poisson distribution will become apparent only in connection with the theory of stochastic processes.</p>
<p>Consider a sequence of random events occurring in time, such as, radioactive disintegrations or incoming calls at a telephone exchange. Each event is represented by a point on the time axis, and we are concerned with chance distributions of points. There exists many different types of such distributions, but their study belongs to the domain of continuous probabilities which we have postponed to the second volume. Here, we shall be content to show that the simplest physical assumptions lead to $p(k;\lambda)$ as the probability of finding exactly $k$ points (events) within a fixed interval of specified length. Our methods are necessarily crude, and we shall return to the same problem with more adequate methods in chapter XVII.</p>
<p>The physical assumptions which we want to express mathematically are that the conditions of the experiment remain constant in time, and that non-overlapping time intervals are stochastically independent in the sense that information concerning the number of events in one interval reveals nothing about the other. The theory of probabilities in a continuum makes it possible to express these statements directly, but being restricted to discrete probabilities, we have to use an approximate finite model and pass to the limit.</p>
<p>Imagine that the unit time interval divided into a great number $n$ of intervals, each of length $1/n$. Either a particular subinterval is empty or it contains atleast one of our random points(or events) and we agree to call the two possibilities failure and success, respectively. The probability $p_n$ of success must be the same for all $n$ subintervals, since they have the same length.</p>
<p>Think of the intervals as bins and the events such as an emission of an $\alpha$-particle by a radioactive element as a ball. Then, a ball is equally likely to be placed in any of the $n$ bins, as they are intervals of equal length.</p>
<p>The assumed independence of non-overlapping intervals then implies that we have $n$ Bernoulli trials and the probability of exactly $k$ successes is given by $b(k;n,p)$. Now the number of successes is not necessarily the same as the number of random points, since a subinterval may contain several random points. Because, when a bin is assigned its first ball, that counts as a success, any subsequent placement of balls into this bin are technically failures.</p>
<p>However, it is natural to introduce the additional assumption that the probability of two or more random points($\alpha$-particls being emitted) during a very short teeny-weeny time interval is negligible. So, if you divvy up the unit time interval in a very large number $n$ of subintervals, it is unlikely two have two or more random points in the same sub-interval.</p>
<p>In this limiting case, the probability of finding exactly $k$ random points in the unit time interval is given by the limit of $(b,k;n,p_n)$ as $n \to \infty$. When we divide each subinterval into two parts of equal length, we find that, the probability that one or more balls falls into this sub-interval equals, $p_n = p_{2n} + p_{2n} - p_{2n}^2 = 2p_n - p_{2n}^2$ by inclusion-exclusion.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This equation states that success in the left half, or success in the right half, or in both. It follows that $p_n &lt; 2p_{2n}$. So, $p_n &lt; 2p_{2n} &lt; 4p_{4n} &lt; 8p_{8n} &lt; \ldots$. Thus, $np_n$ increases monotonically. If $np_n \to \lambda$, then $b(k;n,p_n) \approx b(k;n,\frac{\lambda}{n}) \to p(k;\lambda)$ and we find that (16) as the the probability that there is a total of $k$ random points cotained in our unit interval.</p>
<p>Note, that by the weak LLN formulation, the average number of successes; or the average number of random points($\alpha$-particles emitted) in a unit interval of time is $np_n$. The assumption, $np_n \to \infty$ leads to no sensible result, as it would imply infinitely many random points even in the smallest interval.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If instead of the unit interval, we take an arbitrary interval of length $t$ and again use a subdivision into intervals of length $1/n$, then we have Bernoulli trials with the same probability $p_n$ of success, but the number of trials is the integer nearest to $nt$ rather than $n$. The passage to the limit is the same, but we get $nt \cdot p_n \to \lambda t$ instead of $\lambda$.</p>
<hr>
<p>This leads us to consider,</p>
\begin{align*}
p(k;\lambda t) = e^{-\lambda t} \frac{(\lambda t)^k}{k!} \tag{17}
\end{align*}<p>as the probability of finding exactly $k$ points in a fixed interval of length $t$.</p>
<hr>
<p>In particular, the probability of no poinnt in an interval of length $t$ is:</p>
\begin{align*}
p(0;\lambda t) = e^{-\lambda t} \tag{18}
\end{align*}<p>and the probability of one or more points is therefore $1 - e^{-\lambda t}$.</p>
<p>The parameter $\lambda$ is a physical constant which determines the density of points on the $t$-axis. The larger $\lambda$ is, the smaller is the probability of finding no point. Suppose that a physical experiment is repeated a great number $N$ of times, and that each time we count the number of events in an interval of fixed length $t$. Let $N_k$ be the number of times that exactly $k$ events are observed. Then,</p>
\begin{align*}
N_0 + N_1 + N_2 + \ldots = N \tag{19}
\end{align*}<p>The total number of points observed in the $N$ experiments is:</p>
\begin{align*}
N_1 + 2N_2 + 3N_3 + \ldots = T \tag{20}
\end{align*}<p>and $T/N$ is average. If $N$ is large, we expect that</p>
\begin{align*}
N_k \approx N p(k;\lambda t) \tag{21}
\end{align*}<p>(this lies at the root of all applications of probability and will be justified and made more precise by the law of large numbers in chapter X). Substituting from (21) into (20), we find</p>
\begin{align*}
T &amp;\approx N\{p(1;\lambda t) + 2p(2;\lambda t) + 3p(3;\lambda t) + \ldots\} \tag{22} \\
&amp;= Ne^{-\lambda t} \sum_{k=1}^{\infty}\frac{(\lambda t)^k}{k!}\\
&amp;= Ne^{-\lambda t} \lambda t \sum_{k=0}^{\infty}\frac{(\lambda t)^k}{k!}\\
&amp;= Ne^{-\lambda t} \lambda t e^{\lambda t}\\
&amp;= N\lambda t
\end{align*}<p>and hence,</p>
\begin{align*}
\lambda t = \frac{T}{N} \tag{23}
\end{align*}<p>This relation gives us a means of estimating $\lambda$ from observations and of comparing the theory with experiments. The examples of the next section will illustrate this.</p>
<h3 id="Spatial-Distributions.">
<a class="anchor" href="#Spatial-Distributions." aria-hidden="true"><span class="octicon octicon-link"></span></a>Spatial Distributions.<a class="anchor-link" href="#Spatial-Distributions."> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We have considered the distribution of random events or points along the $t$-axis, but the same argument applies to the distribution of points in plane or space. Instead of intervals of length $t$, we have domains of length, area or volume $t$, and the fundamental assumption is that the probability of finding $k$ points in any specified domain depends only on the area or volume of the domain, but not it's shape. Otherwise, we have the same assumptions as before: (1) If $t$ is small, the probability of finding more than one point in a domain of volume $t$ is small as compared to $t$; (2) non-overlapping domains are mutually independent. To find the probability that a domain of volume $t$ contains exactly $k$ random points, we subdivide it into $n$ subdomains and approximate the required probability of $k$ successes in $n$ trials. This means neglecting the possibility of finding more than one point in the same subdomain, but our assumption (1) implies that the error tends to zero as $n \to \infty$. In the limit we again get the Poisson distribution (17). Stars in space, raisins in cake, weed seeds among grass seeds, flaws in materials, animal litters in fields are distributed in accordance with the Poisson law.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Observations-Fitting-the-Poisson-Distribution.">
<a class="anchor" href="#Observations-Fitting-the-Poisson-Distribution." aria-hidden="true"><span class="octicon octicon-link"></span></a>Observations Fitting the Poisson Distribution.<a class="anchor-link" href="#Observations-Fitting-the-Poisson-Distribution."> </a>
</h2>
<p><em>Radioactive disintegrations.</em> A radioactive substance emits $\alpha$-particles; the number of particles reaching a given portion of space during time $t$ is the best-known example of random events obeying the Poisson law. Of course, the substance continues to decay and in the long run, the density of $\alpha$-particles will decline. However, with radium it takes years before a decrease of matter can be detected; for relatively short periods, the conditions may be considered constant, and we have an ideal realization of the hypotheses which led to the Poisson distribution.</p>
<p>In a famous experiment by Ernest Rutherford, a radioactive substance was observed during $N=2608$ time intervals of $7.5$ seconds each; the number of particles reaching a counter was obtained for each period. The below table records the number $N_k$ of the periods with exactly $k$ particles. The total number of particles is $T = \sum k N_k = 10,094$, the average $T/N=3.870$. The theoretical values $Np(k;3.870)$ are seen rather close to the observed numbers $N_k$.</p>
\begin{array}{ccc}
\hline 
k &amp; N_k &amp; Np(k;3.870)\\
\hline
0 &amp; 57 &amp; 54.399\\
1 &amp; 203 &amp; 210.523 \\
2 &amp; 383 &amp; 407.361 \\
3 &amp; 525 &amp; 525.496 \\
4 &amp; 532 &amp; 508.418 \\
5 &amp; 408 &amp; 393.515 \\
6 &amp; 273 &amp; 253.817 \\
7 &amp; 139 &amp; 140.325 \\
8 &amp; 45 &amp; 67.882 \\
9 &amp; 27 &amp; 29.189 \\
k\geq 10 &amp; 16 &amp; 17.075\\
\hline
\text{ Total } &amp; 2608 &amp; 2608.00
\end{array}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Flying bomb hits on London.</em> As an example of a spatial distribution of random points consider the statistics of flying-bomb hits in the South of London during World War II. The entire area is divided into $N=576$ small areas of $t=\frac{1}{4}$ square kilometers each, and records the number $N_k$ of areas with exactly $k$ hits. The total number of hits is $T = \sum k N_k = 537$, the average $\lambda t = T/N = 0.9323$. The fit of the Poisson distribution is surprisingly good. It is interesting to note that most people believed in a tendency of the points of impact to cluster. If this were true, there would be a higher frequency of areas with either many hits or no hit and a deficiency in the intermediate classes.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Chromosome interchanges in cells.</em> Irradiation by X-rays produces certain processes in organic cells which we call chromosome interchanges. As long as radiation continues, the probability of such interchanges remains constant, and according to theory, the numbers $N_k$ of the cells with exactly $k$ interchanges should follow a Poisson distribution. The theory is also able to predict the dependence of the parameter $\lambda$ on the intensity of radiation, the temperature etc., but we shall not enter into these details.</p>
<p><em>Connections to the wrong number.</em> A total of $N=267$ telephone numbers were observed; $N_k$ indicates how many number had exactly $k$ wrong connections. The Poisson distribution $p(k;8.74)$ shows again an excellent fit.</p>
<p><em>Bacteria and blood counts</em>. Consider a photograph of a petri plate with bacterial colonies, whch are visible under the microscope as dark spots. The plate is divided into small squares. The observed number of squares with exactly $k$ dark spots was $Np(k;\lambda t)$. We have here an important practical application of the poisson distribution to spatial distributions of random points.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Waiting-times.-The-negative-Binomial-Distribution.">
<a class="anchor" href="#Waiting-times.-The-negative-Binomial-Distribution." aria-hidden="true"><span class="octicon octicon-link"></span></a>Waiting times. The negative Binomial Distribution.<a class="anchor-link" href="#Waiting-times.-The-negative-Binomial-Distribution."> </a>
</h2>
<p>Consider a succession of $n$ Bernoulli trials and let us inquire how long it will take for the $r$th success to turn up. Here $r$ is a fixed positive integer. The total number of successes in $n$ trials may, of course, fall short of $r$, but the probability that the $r$th success occurs at the trial number $\nu \leq n$ is clearly independent of $n$ and depends only on $\nu,r,p$. Since, necessarily $\nu \geq r$, it is prefereable to write $\nu = k + r$. The probability that the $r$th successes occurs at the trial number $r + k$ (where $k=0,1,2,\ldots$) will be denoted by $f(k;r,p)$. It equals the probability that exactly $k$ failures preced the $r$th success. This event occurs if, and only if, among the $r+ k - 1$ trials there are exactly $k$ failures, and the following, or $(r+k)$th trial, results in success; the corresponding probabilities are ${{r + k - 1} \choose k} \cdot p^{r-1}q^k$ and $p$, so that:</p>
<p>
\begin{align*}
f(k;r,p) = {{r + k - 1} \choose k} \cdot p^r q^k \tag{24}
\end{align*}
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since, for any $a &gt; 0$</p>
<p>
\begin{align*}
{-a \choose k} &amp;= \frac{(-a)(-a-1)\cdots(-a-(k-1))(-a-k)(-a-(k+1))\cdots (-3)(-2)(-1)}{(-a-k)!k!}\\
&amp;= \frac{(-a)(-a-1)\cdots(-a-(k-1))}{k!}\\
&amp;= (-1)^k \frac{(a+k-1)\cdots (a+1)a}{k!} \\
&amp;= (-1)^k \frac{(a+k-1)\cdots (a+1)a(a-1)!}{k!(a-1)!} \\
&amp;= (-1)^k \frac{(a+k-1)!}{(a-1)!k!}\\
&amp;= (-1)^k {a + k - 1 \choose k}
\end{align*}
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We find the alternative form:</p>
<p>
\begin{align*}
f(k;r,p) = {-r \choose k}p^r (-q)^k  \tag{25}
\end{align*}
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Suppose now that Bernoulli trials are continued as long  as necessary for $r$ successes to turn up. A typical sample point is represented by a sequence containing an arbitrary number, $k$, of letters $F$ and exactly $r$ letters $S$, the sequence terminating by  an $S$; the probabilty of such a point is, by definition $p^r q^k$. We must ask, however, whether it is possible that the trials never end, that is, whether an infinite sequence of trials may produce fewer than $r$ successes. Now, $\sum_{k=0}^{\infty}f(k;r,p)$ is the probability that the trials never end, that is, whether an infinite sequence of trials may produce fewer than $r$ successes. Now, $\sum_{k=0}^{\infty}f(k;r,p)$ is the probability that the $r$th success occurs after finitely many trials; accordingly, the possibility of an infinite sequence with fewer than $r$ successes can be discounted if, and only if,</p>
\begin{align*}
\sum_{k=0}^{\infty}f(k;r,p) = 1 \tag{26}
\end{align*}<p>To prove that (26) holds, it suffices to note that:</p>
<p>
\begin{align*}
\frac{1}{1-q} &amp;= 1 + q + q^2 + q^3 + \ldots + q^r + q^{r+1} + q^{r+2} + \ldots \\
\frac{1}{(1-q)^2} &amp;= 1 + 2q + 3q^2 + 4q^3 + \ldots + rq^{r-1} + (r+1)q^r + (r+2)q^{r+1} + \ldots \quad \{ \text{ Differentiating on both sides} \} \\
\frac{1\cdot 2}{(1-q)^3} &amp;= 2 \cdot 1 + 3 \cdot 2q + 4\cdot 3q^2 + \ldots + r(r-1)q^{r-2} + (r+1)rq^{r-1} + (r+2)(r+1)q^{r} + \ldots \quad \{ \text{ Differentiating on both sides} \} \\
\frac{(r-1)!}{(1-q)^r} &amp;= r! + \frac{(r+1)!}{2!}q^{1} + \frac{(r+2)!}{3!}q^{2} + \ldots \quad \{ \text{ Differentiating $r$ times} \} \\
\frac{1}{(1-q)^r} &amp;= \frac{r!}{(r-1)!1!} + \frac{(r+(2-1))!}{(r-1)!2!}q^{1} + \frac{(r+(3-1))!}{(r-1)!3!}q^{2} + \ldots \\
&amp; = \sum_{k=0}^{\infty}{{r + k - 1} \choose k}q^k \\
&amp;=  \sum_{k=0}^{\infty}(-1)^k{-r \choose k}q^k \\
&amp;=  \sum_{k=0}^{\infty}{-r \choose k}(-q)^k \tag{27}
\end{align*}
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>From (27), we have that</p>
<p>
\begin{align*}
\sum_{k=0}^{\infty}{-r \choose k}(-q)^k &amp;= (1-q)^{-r} = p^{-r}\\
\sum_{k=0}^{\infty}{-r \choose k}p^r(-q)^k &amp;= p^{-r} \cdot p^r = 1 \\
\sum_{k=0}^{\infty}f(k;r,p) &amp;= 1 \tag{28}
\end{align*}
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In our waiting time problem, $r$ is necessarily a positive integer, but the quantity defined by either (24) or (25) is non-negative and holds for any positive $r$. For arbitrary fixed real $r&gt;0$ and $0&lt;p&lt;1$, the sequence $\{f(k;r,p)\}$ is called a <strong>negative binomial distribution</strong>. When $r$ is a positive integer, $\{f(k;r,p)\}$ may be interpreted as the probability distribution for the waiting time to the $r$th success; as such it is aalso called the Pascal distribution. For $r = 1$, it reduces to the geometric distribution $\{pq^k\}$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Banach's Matchbox problem.</em> A certain mathematician always carries one match box in his right pocket and one in his left. When he wants a match, he selects a pocket at random, the successive choices thus constituting Bernoulli trials with $p=\frac{1}{2}$. Suppose that initially each box contained exactly $N$  matches and consider the moment when, for the first time, our mathematician discovers that a box is empty. At that moment the other box may contain $0,1,2,\ldots,N$ matches and we denote the corresponding probabilities by $u_r$ Let us identify success with the choice of the left pocket. The left pocket will be found empty at a moment when the right pocket contains exactly $r$ matches, if and only if, $N-r$ failures precede the $(N+1)$st success. The probability of this event is $f(N-r;N+1,\frac{1}{2})$. The same argument applies to the right pocket and therefore the required probability is:</p>
<p>
\begin{align*}
u_r = 2f(N-r;N+1,\frac{1}{2}) ={{2N - r} \choose {N-r}}\left(\frac{1}{2}\right)^{2N-r} = {{2N - r} \choose {N}}\left(\frac{1}{2}\right)^{2N-r}  \tag{29}
\end{align*}
</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="c"># Distribution of the waiting time to find the any of the matchboxes empty</span>

<span class="k">using</span> <span class="n">Plots</span>
<span class="k">using</span> <span class="n">Distributions</span>
<span class="n">plotlyjs</span><span class="p">()</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>

<span class="k">function</span> <span class="n">negative_binomial</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">binomial</span><span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="n">k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span><span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span><span class="o">^</span><span class="n">r</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span><span class="o">^</span><span class="n">k</span>
<span class="k">end</span>

<span class="n">plot</span><span class="p">([</span><span class="n">r</span> <span class="k">for</span> <span class="n">r</span> <span class="kp">in</span> <span class="mi">0</span><span class="o">:</span><span class="n">N</span><span class="p">],[</span><span class="mi">2</span><span class="o">*</span><span class="n">negative_binomial</span><span class="p">(</span><span class="n">N</span><span class="o">-</span><span class="n">r</span><span class="p">,</span><span class="n">N</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.50</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span> <span class="kp">in</span> <span class="mi">0</span><span class="o">:</span><span class="n">N</span><span class="p">],</span>
        <span class="n">line</span><span class="o">=:</span><span class="n">stem</span><span class="p">,</span> <span class="n">marker</span><span class="o">=:</span><span class="n">circle</span><span class="p">,</span> <span class="n">c</span><span class="o">=:</span><span class="n">red</span><span class="p">,</span>
        <span class="n">xlabel</span><span class="o">=</span><span class="s">"Number of matches left, r"</span><span class="p">,</span>
        <span class="n">ylabel</span><span class="o">=</span><span class="s">"Probability"</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s">"PMF"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
&lt;!DOCTYPE html&gt;

    
        <title>Plots.jl</title>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        
    
    
            <div id="b406f199-1076-4c14-b6ce-8cec79a788bb" style="width:600px;height:400px;"></div>
    <script>
        requirejs.config({
        paths: {
            Plotly: 'https://cdn.plot.ly/plotly-1.57.1.min'
        }
    });
    require(['Plotly'], function (Plotly) {

    PLOT = document.getElementById('b406f199-1076-4c14-b6ce-8cec79a788bb');
    Plotly.plot(PLOT, [
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            0.0,
            1.0,
            2.0,
            3.0,
            4.0,
            5.0,
            6.0,
            7.0,
            8.0,
            9.0,
            10.0,
            11.0,
            12.0,
            13.0,
            14.0,
            15.0,
            16.0,
            17.0,
            18.0,
            19.0,
            20.0
        ],
        "showlegend": true,
        "mode": "markers",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "marker": {
            "symbol": "circle",
            "color": "rgba(255, 0, 0, 1.000)",
            "line": {
                "color": "rgba(0, 0, 0, 1.000)",
                "width": 1
            },
            "size": 8
        },
        "zmax": null,
        "y": [
            0.12537068761957926,
            0.12537068761957926,
            0.12215605460369261,
            0.11572678857191931,
            0.1063435354444664,
            0.09452758706174791,
            0.08102364605292678,
            0.06672535557299852,
            0.0525714922696352,
            0.0394286192022264,
            0.02798160072416067,
            0.01865440048277378,
            0.011578593403100967,
            0.006616339087486267,
            0.003430694341659546,
            0.001583397388458252,
            0.0006333589553833008,
            0.0002111196517944336,
            5.507469177246094e-5,
            1.0013580322265625e-5,
            9.5367431640625e-7
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            0.0,
            0.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            0.12537068761957926
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            1.0,
            1.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            0.12537068761957926
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            2.0,
            2.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            0.12215605460369261
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            3.0,
            3.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            0.11572678857191931
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            4.0,
            4.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            0.1063435354444664
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            5.0,
            5.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            0.09452758706174791
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            6.0,
            6.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            0.08102364605292678
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            7.0,
            7.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            0.06672535557299852
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            8.0,
            8.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            0.0525714922696352
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            9.0,
            9.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            0.0394286192022264
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            10.0,
            10.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            0.02798160072416067
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            11.0,
            11.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            0.01865440048277378
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            12.0,
            12.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            0.011578593403100967
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            13.0,
            13.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            0.006616339087486267
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            14.0,
            14.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            0.003430694341659546
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            15.0,
            15.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            0.001583397388458252
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            16.0,
            16.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            0.0006333589553833008
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            17.0,
            17.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            0.0002111196517944336
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            18.0,
            18.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            5.507469177246094e-5
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            19.0,
            19.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            1.0013580322265625e-5
        ],
        "type": "scatter"
    },
    {
        "xaxis": "x",
        "colorbar": {
            "title": ""
        },
        "yaxis": "y",
        "x": [
            20.0,
            20.0
        ],
        "showlegend": false,
        "mode": "lines",
        "name": "PMF",
        "zmin": null,
        "legendgroup": "PMF",
        "zmax": null,
        "line": {
            "color": "rgba(255, 0, 0, 1.000)",
            "shape": "linear",
            "dash": "solid",
            "width": 1
        },
        "y": [
            0.0,
            9.5367431640625e-7
        ],
        "type": "scatter"
    }
]
, {
    "showlegend": true,
    "xaxis": {
        "showticklabels": true,
        "gridwidth": 0.5,
        "tickvals": [
            0.0,
            5.0,
            10.0,
            15.0,
            20.0
        ],
        "visible": true,
        "ticks": "inside",
        "range": [
            -0.6,
            20.6
        ],
        "domain": [
            0.09128390201224845,
            0.9934383202099738
        ],
        "tickmode": "array",
        "linecolor": "rgba(0, 0, 0, 1.000)",
        "showgrid": true,
        "title": "Number of matches left, r",
        "mirror": false,
        "tickangle": 0,
        "showline": true,
        "gridcolor": "rgba(0, 0, 0, 0.100)",
        "titlefont": {
            "color": "rgba(0, 0, 0, 1.000)",
            "family": "sans-serif",
            "size": 15
        },
        "tickcolor": "rgb(0, 0, 0)",
        "ticktext": [
            "0",
            "5",
            "10",
            "15",
            "20"
        ],
        "zeroline": false,
        "type": "-",
        "tickfont": {
            "color": "rgba(0, 0, 0, 1.000)",
            "family": "sans-serif",
            "size": 11
        },
        "zerolinecolor": "rgba(0, 0, 0, 1.000)",
        "anchor": "y"
    },
    "paper_bgcolor": "rgba(255, 255, 255, 1.000)",
    "annotations": [],
    "height": 400,
    "margin": {
        "l": 0,
        "b": 20,
        "r": 0,
        "t": 20
    },
    "plot_bgcolor": "rgba(255, 255, 255, 1.000)",
    "yaxis": {
        "showticklabels": true,
        "gridwidth": 0.5,
        "tickvals": [
            0.0,
            0.02,
            0.04,
            0.06,
            0.08,
            0.1,
            0.12
        ],
        "visible": true,
        "ticks": "inside",
        "range": [
            -0.0037611206285873776,
            0.12913180824816664
        ],
        "domain": [
            0.07581474190726165,
            0.9901574803149606
        ],
        "tickmode": "array",
        "linecolor": "rgba(0, 0, 0, 1.000)",
        "showgrid": true,
        "title": "Probability",
        "mirror": false,
        "tickangle": 0,
        "showline": true,
        "gridcolor": "rgba(0, 0, 0, 0.100)",
        "titlefont": {
            "color": "rgba(0, 0, 0, 1.000)",
            "family": "sans-serif",
            "size": 15
        },
        "tickcolor": "rgb(0, 0, 0)",
        "ticktext": [
            "0.00",
            "0.02",
            "0.04",
            "0.06",
            "0.08",
            "0.10",
            "0.12"
        ],
        "zeroline": false,
        "type": "-",
        "tickfont": {
            "color": "rgba(0, 0, 0, 1.000)",
            "family": "sans-serif",
            "size": 11
        },
        "zerolinecolor": "rgba(0, 0, 0, 1.000)",
        "anchor": "x"
    },
    "legend": {
        "borderwidth": 1,
        "tracegroupgap": 0,
        "font": {
            "color": "rgba(0, 0, 0, 1.000)",
            "family": "sans-serif",
            "size": 11
        },
        "title": {
            "font": {
                "color": "rgba(0, 0, 0, 1.000)",
                "family": "sans-serif",
                "size": 15
            },
            "text": ""
        },
        "traceorder": "normal",
        "x": 1.0,
        "yanchor": "auto",
        "xanchor": "auto",
        "bordercolor": "rgba(0, 0, 0, 1.000)",
        "bgcolor": "rgba(255, 255, 255, 1.000)",
        "y": 1.0
    },
    "width": 600
}
);
    });
    </script>

    


</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Hypergeometric-distribution.">
<a class="anchor" href="#The-Hypergeometric-distribution." aria-hidden="true"><span class="octicon octicon-link"></span></a>The Hypergeometric distribution.<a class="anchor-link" href="#The-Hypergeometric-distribution."> </a>
</h2>
<p>If we have an urn filled with $w$ white balls and $b$ black balls, then drawing $n$ balls out of the urn with replacement yields a $b(k;n,w/(w+b))$ distribution for the number of white balls obtained in $n$ trials, since the draws are independent Bernoulli trials, each with probability $w/(w+b)$ of success. If we instead sample without replacement, then the number of white balls follows a <em>hypergeometric distribution</em></p>
<p>Consider an urn with $w$ white balls and $b$ black balls. We draw $n$ balls out of the urn at random without replacement, such that all ${w + b} \choose n$ samples are equally likely. Let $X$ be the number of white balls in the sample. Then, $X$ is said to have the <em>hypergeometric distribution</em> with parameters $w$, $b$ and $n$; we denote this by $X \sim HGeom(w,b,n)$.</p>
<p>As with the binomial distribution, we can obtain the PMF of the hypergeometric distribution from the story.</p>
<hr>
<p><em>Theorem.</em> If $X \sim HGeom(w,b,n)$, then the PMF of $X$ is</p>
<p>
\begin{align*}
P(X=k) = \frac{{w \choose k}{b \choose n -k}}{{w+b} \choose n} \tag{30}
\end{align*}
</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Proof.</em></p>
<p>To get $P(X=k)$, we first count the number of possible ways to draw exactly $k$ white balls and $n-k$ black balls (without distinguishing between different orderings for getting the same set of balls.) There are ${w \choose k}{b \choose {n - k}}$ ways to draw $k$ white balls and $n - k$ black balls by the multiplication rule, and there are a total of ${w + b} \choose n$ total ways to draw $n$ balls from $w + b$ balls. Since all samples are equally likely, the naive definition of probability gives</p>
<p>
\begin{align*}
P(X = k) = \frac{{w \choose k}{b \choose {n-k}}}{{w + b} \choose n}
\end{align*}
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The hypergeometric distribution comes up in many scenarios which, on the surface, have little in common with white and black balls in an urn. The essential structure of the Hypergeometric story is that items in a population are classified using two sets of tags: in the urn story, each ball is either white or black(this is the first set of tags), and each ball is either sampled or not sampled(this is the second set of tags). Furthermore, at least one of these sets of tags is assigned completely at random (in the urn story, the balls are sampled randomly, with all sets of the correct size equally likely). Then, $X \sim HGeom(w,b,n)$ represents the number of twice-tagged items: in the urn story, balls that are both white and sampled.</p>
<p>The next two examples show seemingly dissimilar scenarios that are nonetheless isomorphic to the urn story.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Example. <em>Elk capture-recapture</em>. A forest has $N$ elk. Today, $m$ of the elks are captured, tagged and released into the wild. At a later date, $n$ elks are recaptured at random. Assume that the recaptured elks are equally likely to be any set of $n$ of the elk e.g. an elk that has been captured does not learn how to avoid being captured again.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By the story of the hypergeometric, the number of tagged elks in the recaptured sample is $HGeom(m,N-m,n)$. The $m$ tagged elks in this story correspond to the white balls and the $N-m$ untagged elks correspond to the black balls. Instead of sampling $n$ balls from the urn, we recapture $n$ elk from the forest.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Example. <em>Aces in a poker hand</em> In a five card hand drawn at random from well-shuffled deck, the number of aces in the hand $HGeom(4,48,5)$ distribution, which can be seen by thinking of the aces as the white balls and the non-aces as the black balls. Using the hypergeometric PMF, the probability that the hand has exactly three aces is</p>
<p>
\begin{align*}
P(X = 3) = \frac{{4 \choose 3}{48 \choose 2}}{52 \choose 5} \approx 0.0017
\end{align*}
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Discrete-Uniform-distribution.">
<a class="anchor" href="#Discrete-Uniform-distribution." aria-hidden="true"><span class="octicon octicon-link"></span></a>Discrete Uniform distribution.<a class="anchor-link" href="#Discrete-Uniform-distribution."> </a>
</h2>
<p>A very simple story, closely connected to the naive definition of probability describes picking a random number from some finite set of possibilities.</p>
<hr>
<p>Story. (<em>Discrete Uniform distribution</em>) Let $C$ be a finite, nonempty set of numbers. Choose one of the numbers uniformly at random (all values in $C$ are equally likely). Call the chosen number $X$. Then, $X$ is said to have the <em>discrete uniform distribution</em> with parameter $C$; we denote this by $X \sim DUnif(C)$.</p>
<p>The PMF of $X \sim DUnif(C)$ is:</p>
\begin{align*}
P(X = x) = \frac{1}{|C|} \tag{31}
\end{align*}<p>for $x \in C$ (and $0$ otherwise), since a PMF must sum to $1$.</p>
<hr>
<p>As with questions based on the naive definition of probability, questions based on a discrete uniform distribution reduce to counting problems. Specifically, for $X \sim DUnif(C)$ and any $A \subseteq C$, we have:</p>
\begin{align*}
P(X \in A) = \frac{|A|}{|C|} \tag{32}
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p>Example. <em>Random slips of paper</em>. There are 100 slips of paper in a hat, each of which has one of the numbers $1,2,\ldots,100$ written on to it, with no number appearing more than once. Five of the slips are drawn one at a time.</p>
<p>First consider random sampling with replacement (with equal probabilities).</p>
<p>(a) What is the distribution of how many of the drawn slips have a value of atleast 80 written on them?</p>
<p>(b) What is the distribution of the value of the $j$th draw (for $1 \leq j \le 5$)?</p>
<p>(c) What is the probability that the number $100$ is drawn atleast once?</p>
<p><em>Now consider random sampling without replacement (with all sets of five slips equally likely to be chosen).</em></p>
<p>(d) What is the distribution of how many of the drawn slips have a value of atleast 80 written on them?</p>
<p>(e) What is the distribution of the value of the $j$th draw (for $1 \leq j \le 5$)?</p>
<p>(f) What is the probability that the number $100$ is drawn in the sample?</p>
<hr>
<p><em>Solution.</em>
(a) Let us identify success as drawing a slip with a value of atleast $80$ written on it. We are sampling with replacement. Hence, these are independent and identical Bernoulli trials. The probability of success $p = \frac{21}{100}$, the probability of failure $q = \frac{79}{100}$. Let $X$ be the number of successes in $n=5$ trials. $X \sim Bin(k;5,0.21)$. The PMF of $X$ is given by,</p>
\begin{align*}
P(X=k) &amp;= {5 \choose k}(0.21)^k (0.79)^{5-k}
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>(b) Let $X$ be the value of the $j$th draw. $X \sim DUnif(\{1,2,3,\ldots,100\})$. The PMF of $X$ is given by,</p>
\begin{align*}
P(X = x) &amp;= \frac{1}{100}
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>(c) Let us identify success with drawing $100$. We are sampling with replacement. Hence, these are independent and identical Bernoulli trials with probability of success, $p = \frac{1}{100}$. Let $X$ be the number of times $100$ is drawn. $X \sim Bin(k;5,\frac{1}{100})$.</p>
\begin{align*}
P(X \geq 1) &amp;= 1 - P(X=0)\\
&amp;= 1 - q^5 \\
&amp;= 1 - \left(\frac{99}{100}\right)^5
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>(d) We are sampling without replacement. $X \sim HGeom(21,79,5)$. The PMF of $X$ is given by</p>
<p>
\begin{align*}
P(X=k) = \frac{{21 \choose k}{79 \choose 5-k}}{100 \choose 5}
\end{align*}
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>(e) Let $X$ be the number drawn in the $j$th trial. We are sampling without replacement, so the draws are not independent. However, still, the unconditional probability of drawing a pre-specified value in the $j$th draw, given that we don't have any knowledge about the numbers drawn in trials $1,\ldots,j-1$, is given by:</p>
\begin{align*}
P(X = k) = \frac{1}{100}
\end{align*}<p>(f) Let us identify success as including the number $100$ in the sample.  We are sampling without replacement, so the draws are not independent. Let $X$ be the number of successes in $n = 5$ trials. $X \sim HGeom(1,99,5)$. The probability of exactly one success is,</p>
<p>
\begin{align*}
P(X = 1) &amp;= \frac{{1 \choose 1}{99 \choose 4}}{100 \choose 5}
\end{align*}
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Cumulative-Distribution-Functions.">
<a class="anchor" href="#Cumulative-Distribution-Functions." aria-hidden="true"><span class="octicon octicon-link"></span></a>Cumulative Distribution Functions.<a class="anchor-link" href="#Cumulative-Distribution-Functions."> </a>
</h2>
<p>Another function that describes the distribution of a random variable is the <em>cumulative distribution function</em>(CDF). Unlike the PMF, which only discrete r.v.s possess, the CDF is defined for all r.v.s.</p>
<hr>
<p><strong>Definition</strong> (Cumulative Distribution Function). The cumulative distribution function (CDF) of an r.v. $X$ is the function $F_X$ given by $F_X(x) = P(X \leq x)$. When there is no risk of ambiguity, we sometimes drop the subscript and just write $F$ for a CDF.</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Theorem.</strong> (Valid CDFs). Any CDF $F$ has the following properties.</p>
<p>1) Increasing. If $x_1 &lt; x_2$, then $F(x_1) &lt; F(x_2)$.</p>
<p>2) Right-continuous. The CDF is right continuous, that is</p>
\begin{align*}
\lim_{x \to a^{+}} F(x) = F(a)
\end{align*}<p>3) Convergence to $0$ and $1$ in the limits:</p>
\begin{align*}
\lim_{x \to -\infty} F(x) = 0 \quad \text{ and } \quad \lim_{x \to \infty}F(x) = 1
\end{align*}<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Proof.</em>
The above criteria are true for all CDFs.</p>
<p>The first criterion is true, because the event $\{X \leq x_1\}$ is a subset of the event $\{X \leq x_2\}$. So, $P(X \leq x_1) \leq P(X \leq x_2)$. Consequently, $F_X(x_1) \leq F_X(x_2)$.</p>
<p>To prove the second criterion rigorously, we need to first show that $\mathbb{P}$ is a continuous set function. That is, if $A_1 \supseteq A_2 \supseteq A_3 \ldots$ and $A = \bigcap_{n=1}^{\infty}A_n$, then as $n \to \infty$, $P(A_n) \to P(A)$. We skip this rigorous proof for the moment, but see that it is intuitively true from the graph of a CDF.</p>
<p>For the third criterion, we have $F(x) = 0$ for $x&lt;0$ and</p>
\begin{align*}
\lim_{x \to \infty}F(x) = \lim_{x \to \infty} P(X \leq [[x]]) = \lim_{x \to \infty}\sum_{n=0}^{[[x]]}P(X =n) = \sum_{n=0}^{\infty}P(X=n) = 1
\end{align*}<p>The converse is true too: we will show that given any function $F$ that meets this criteria, we can construct a random variable $X$ whose CDF is $F$.</p>

</div>
</div>
</div>
</div>


</body>
  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="quantophile/mathematical-finance"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/mathematical-finance/probability-theory/2022/03/06/The-Binomial-and-Poisson-Distributions-(summary).html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/mathematical-finance/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/mathematical-finance/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/mathematical-finance/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>This blog is a repository of review notes, projects and solved exercise problems on various topics in Probability, Mathematical Finance, PDEs.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" target="_blank" title="fastai"><svg class="svg-icon grey"><use xlink:href="/mathematical-finance/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" target="_blank" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/mathematical-finance/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
